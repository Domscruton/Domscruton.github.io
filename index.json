[{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n  Download my resumé.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m a Graduate Data Scientist in Household Risk \u0026amp; Retail Pricing at Admiral Group PLC, a FTSE 100 Insurance firm.\nMy work involves optimizing the pricing structure through advanced and traditional Statistical Modelling and Machine Learning approaches. I am particularly interested in Stochastic Optimization, Statistical Inference and Bayesian Machine Learning (and its applications in Reinforcement Learning and Bayesian Networks).\n Download my resumé. --\r","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cc46d9ce78a2d7ec82db57f9c44aad94","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m a Graduate Data Scientist in Household Risk \u0026amp; Retail Pricing at Admiral Group PLC, a FTSE 100 Insurance firm.\nMy work involves optimizing the pricing structure through advanced and traditional Statistical Modelling and Machine Learning approaches.","tags":null,"title":"Dominic Scruton","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://domscruton.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"\rAbstract\nThis report utilizes Principal Component and Cluster Analysis to assess differences between the performance of 5872 songs from the 2000's on the basis of 13 numeric variables. Firstly, the data contains a large amount of independent information, with the first three principal components consisting of just 54.7% of the total variation in the data. The 'loudness' of a song had the highest absolute loading on the first principal component axis, suggesting it was the most important variable in explaining the variance between observations. Cluster analysis was then used to group the songs, and eight clusters were selected using K-Means. In particular, the performance of a song greatly varied between clusters. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. The best-performing clusters had higher average danceability scores, while the worst performing clusters had very high levels of instrumentalness. This suggests that songs that are more suitable for dancing and contain higher levels of vocalness (less instrumental) are more likely to be 'hits', on average. From the conclusions of the analysis, several hypotheses were generated regarding differences in the performance and composition of songs contained within different clusters of the data and further methods for analysis, such as a Principal Component Logistic Regression were discussed.\n1 Introduction\rThis report considers Principal Component and Cluster analyses to identify which variables provide the largest amount of independent information and to understand the composition of songs that are more successful. The data consists of 5872 songs from the 2000's and 16 variables, 12 of which are continuous, three are categorical and one is binary. Seven of the continuous variables are 'engineered' metrics that relate to the constitution of each song. For example, 'energy' \u0026quot;represents a perceptual measure of intensity and activity\u0026quot; and 'danceability' \u0026quot;describes how suitable a track is for dancing\u0026quot; (Ansari, 2020). The binary variable, called 'target', indicates whether the song was a 'hit' or a 'flop'. A 'hit' is defined as a song that has featured in the weekly list (issued by Billboards) of Hot-100 tracks in that decade at least once. Exactly half of the songs are labeled as 'hits' with the other half labeled as 'flops'. This analysis would enable artists to test whether their song is more likely to be a ‘hit’ or a ‘flop' based on the characteristics of the song and alter its chances of success depending on which cluster or partition of the data the song is located.\n\r2 Exploratory Data Analysis\rExploring the Data before carrying out a more rigorous analysis is important as it will enable us to assess the distributional properties of the variables and identify some of the individual patterns and relationships (Everitt, 2005). Furthermore, it can be used to consider the most appropriate methods of analysis for complex datasets that contain a range of variables whose inter-relationships are currently based on little theoretical background. We restrict the analysis of the data to the twelve continuous features that will be used for Principal Component and Cluster analyses.\n2.1 Assessing the Distribution of Variables\rHistograms are used to assess the general distribution of the variables and identify any outlying observations (Appendix A). In particular, we identify that some of the variables are expressed on widely differing scales: song duration is measured in milliseconds and ranges from 15920 to 4170227, whilst the engineered variables, such as 'danceability' range from just 0 to 1. In order for Principal Component and Cluster Analyses to work, the data therefore need to be scaled. This standardization may be adversely affected by strongly outlying observations. The histograms in Appendix A identify that the variables 'duration_ms' and 'sections', corresponding to the duration and number of sections of each song have a few strongly outlying observations. The 'Plot of Outliers' (Appendix A) identifies three clearly outlying observations and it is hard to fathom that they have come from the same underlying distribution as the rest of the data and may have been incorrectly imputed. Therefore, these observations are dropped from the analysis.\nSome of the univariate distributions for the variables are quite skewed and this may affect the quality of the Principal Component Analysis. Scaling the data, by subtracting the mean and dividing by the standard deviation will partly alleviate this issue.\n\r2.2 Exploring the Relationships Between Variables\rPrincipal Component Analysis (PCA) assumes that the covariance or correlation matrix adequately describes the relationship amongst the variables, that is the relationships are linear (Kutner et al., 2013). The matrix of scatterplots in Appendix A indicates that most variables appear to have roughly linear relationships, although there is often a large amount of scatter and 'noise'. Therefore, using the correlation matrix to describe the strength of relationships between the variables is justified and hence the conclusions of the PCA will be valid.\nHowever, whilst PCA nor Cluster analysis require multivariate normal data, they tend to work better on data that is multivariate normal (and spherical) (Everitt, 2005). In this case there appears to be some deviation from normality, especially at the tails of the distribution (Appendix A). Whilst this should not affect qualitative conclusions, it may impact on the exact specification of the principal components.\nThe complexity of the Spotify data can be illustrated by plotting a scatterplot of 'danceability' against 'acousticness', but stratified by 'energy'. For low energy, the data tend to have high levels of acousticness and lower danceability scores, whilst for high energy, the levels of acousticness fall but danceability rises. Therefore, the relationship between 'danceability' and 'acousticness' depends on the energy levels of a song. Untangling the relationships and information within the data therefore justifies the use of more rigorous multivariate analytic techniques, such as PCA and Cluster Analysis.\n\r2.3 Correlation Among the Variables\rFor PCA to be worthwhile, there needs to be some correlation between variables in the dataset (Everitt, 2005). The correlation plot below demonstrates that some variables are highly correlated, such as 'section' and 'duration', whilst others show no association. Table 1 shows the variables that have the highest correlation with song performance (target). This indicates that songs that are more danceable and louder tend to perform better, however, they need not be the variables that explain the most variation in the data as assessed by Principal Component Analysis.\nTable 1- Correlation between the Target ('Hit' or 'Flop') for each explanatory Variable\n\r\rVariable\rCorrelation with Target\r\r\r\rDanceability\r0.4585\r\rInstrumentalness\r-0.4713\r\rLoudness\r0.3473\r\r\r\r\r\r3 Methodology\r3.1 Principal Component Analysis\rThe Spotify dataset we have chosen has great potential for dimensionality reduction via Principal Component Analysis (PCA) as there is clear collinearity and correlation between some variables (see the correlation plot), suggesting that they share similar information (Zelterman, 2015). PCA was developed for continuous data, therefore the categorical variables ('key', 'mode', 'target' and 'time_signature') are dropped.\n3.1.1 The Theory of Principal Component Analysis\rConsider a multivariate random vector, \\(\\vec{x} = (x_1, ..., x_p)\\) with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Consider p different linear combinations of the the random vector \\(\\vec{x}\\):\n\\[y_i = \\vec{\\alpha_i}^T \\vec{x}\\]\nfor \\(i = 1,...,p\\), where \\(p\\) is the number of variables in the dataset and also corresponds to the total number of principal component axes. We would like to place each random vector of the data onto a new axis, where each axis explains as much variation as possible. \\(y_i\\) are the projections of the random variable, \\(\\vec{x}\\) onto each of these axes. The \\(\\alpha\\)'s are the loadings and they explain how much each variable, \\(i\\), contributes to each of the principal component axes (\\(x_i\\) is the value of the random vector for the \\(i\\)th variable). For each \\(i = 1,...,p\\), the variances of \\(y_i\\) can be expressed as:\n\\[Var(y_i) = \\vec{\\alpha}_i^T \\Sigma \\vec{\\alpha}_i\\]\nIn particular, we want to find the loadings (\\(\\alpha\\)'s) that maximize the variance along each axis (Everitt, 2005). However, one could make these variances arbitrarily large by multiply the loadings by an arbitrarily large scalar, so we restrict the loadings to have length 1. The second restriction is that the new axes, \\(y_i\\) should be mutually uncorrelated:\n\\[Cov(y_i, y_j) = \\vec{\\alpha}_i^T \\Sigma \\vec{\\alpha}_j = 0\\]\nThe solution to this constrained maximization problem can be solved via the use of a Lagrange Multiplier. The principal components are then given by the eigenvectors of the correlation (or covariance) matrix and the eigenvalues are the variances of each principal component. The principal components are mutually orthogonal (uncorrelated) and decrease in variance.\nHowever, the scaling of the variables may have a significant impact on the results of a Principal Component Analysis. If the variables are measured on different scales, we should use the correlation matrix in the PCA, which is equivalent to scaling the data by dividing each data point by the standard deviation of that variable. In this case, the 'engineered' variables, such as 'liveness' and 'speechiness' lie on a normalized scale between 0 and 1, meanwhile tempo is measured in Beats Per Minute (BPM) and loudness is measured in decibels. Furthermore, if we carry out the analysis without scaling the variables, nearly 100% of the total variation is explained by just one principal component, which consists almost entirely of the 'duration' variable (Appendix B). This is because song duration is recorded in milliseconds, with a range of 5,920 to 4,170,227 milliseconds. Clearly scaling is required for any meaningful interpretation to take place.\n\r3.1.2 Choosing an Appropriate Number of Principal Components to Retain\rIn order to reduce the dimensionality of the problem, we need to reduce the number of principal components, whilst ensuring information loss is minimized. These reduced components could then be used in a logistic regression in order to make predictions regarding the popularity of a song based on its characteristics. The two methods considered are Kaiser's criteria and the Scree plot. Under Kaiser's Criterion, we keep only those Principal components that have eigenvalues (variances) larger than the average. Since the PCA is calculated on the correlation matrix, this average will always be one. However, this cut-off point tends to retain too few components when the number of variables is small (Everitt, 2005). Another scheme for analyzing how many components to keep is the Scree Plot. This plots the variance of each principal component and the idea is to stop retaining components after the largest significant drop in the variance, thus removing the 'scree' from the analysis.\n\r3.1.2 Analysing the Principal Components and Interpreting the Loadings\rThe principal component loadings represent the relative importance of each of the original variables in determining the direction of the new principal component axes (Zelterman, 2015). The scores of the principal components are the orthogonal projections of each data point onto the principal component axes and represent the new space created under PCA. Mardia's Criterion can be used to select those variables that have significant influence on the positions of each principal component. Under this \u0026quot;rule of thumb\u0026quot;, a variable is said to have a high influence if the value of its loading on the principal component axis is larger than \\(\\frac{1}{\\sqrt{p}}\\), where \\(p\\) is the number of variables used in the PCA. The variables with high loadings on the most important axes are the dominant variables that contribute most to the variation in the dataset (Zelterman, 2015).\n\r\r3.2 Cluster Analysis\rClustering the data will allow homogeneous subgroups of songs that display similar attributes to be identified. This information can then be used to generate hypotheses about differences in the observations between groups, such as whether those songs in groups that performed better, on average, exhibit certain attributes.\n3.2.1 Performing a Cluster Analysis\rThe two general methods in which to perform a cluster analysis are partitioning and hierarchical methods. The K-Means algorithm first assigns the data into K clusters. The means or centroids of the clusters are then calculated. Observations are then iteratively reassigned to the nearest cluster (according to Euclidean distances) and for each iteration the centroid of each cluster will change as different observations become part of the grouping. This process continues until no more observations are reallocated. The data will need to be standardized and outlying observations removed in order to effectively carry out the K-means algorithm appropriately (Everitt, 2005). To deal with the sensitivity of the clusters under different starting location, the K-means approach can be first applied to a subset of the data to generate sensible initial centroids and then those centroids used as the starting point for the algorithm to be applied on the whole dataset.\nHierarchical methods assume that groupings in the data cloud have a hierarchical structure, where smaller groups are nested in larger groups. However, such methods are difficult to justify for real datasets. For complex structures, partitioning the data using K-Means is a better approach (Zelterman, 2015). Non-hierarchical methods, such as K-means are also more appropriate if the data consist of mainly continuous or ordered variables. In this case, all of the variables used in the analysis are continuous. Therefore, K-means is used to partition the data cloud into songs with similar characteristics, as opposed to identifying natural clusters under a hierarchical method. In this case, applying Ward's method with 8 clusters provides similar qualitative results regarding cluster location (Appendix C).\n\r3.2.2 Selecting the Number of Clusters\rThe 'elbow' method can be used to select an appropriate number of clusters for the K-means algorithm (Geron, 2019). The within group sum of squares is plotted against the number of clusters. Naturally, as the number of clusters increases, the within group sum of squares will decline, as observations in the cluster become 'closer', on average. In a similar manner to the scree plot, the number of clusters is chosen at the 'elbow', where the last significant drop in the within group sum of squares occurs (Everitt, 2005).\n\r\r\r4 Results\r4.1 Principal Component Analysis\rThe results of the Principal Component analysis demonstrate that variables in the dataset contain a fairly large amount of independent information. In particular, the first three principal components contain only 54.7% of the total variation, whilst to retain 90% of the total variation in the data, we would need to keep the first eight principal components.\nTable 2- Standard Deviation and Cumulative Variance of the Principal Components\n\r\r\rPrincipal Component:\rPC1\rPC2\rPC3\rPC4\rPC5\rPC6\rPC7\rPC8\rPC9\rPC10\rPC11\rPC12\r\r\r\rStandard Deviation\r1.801\r1.314\r1.263\r1.055\r1.015\r0.924\r0.885\r0.841\r0.646\r0.530\r0.380\r0.323\r\rCumulative Proportion\r0.270\r0.414\r0.547\r0.640\r0.726\r0.797\r0.862\r0.921\r0.956\r0.979\r0.991\r1.000\r\r\r\rThe Scree plot suggests the use of just two principal components, however, there is a large amount of scree (remaining variation) left over. Under Kaiser's Criterion, the first five components have standard deviations greater than one and hence we would choose to retain these. Yet, the appropriate number of components to keep is context-specific. If the principal components were to be used as variables in a PC logistic regression, we may want to retain more components, to reduce information loss. However, the purpose of this report is to visualize and explain the most important components and variables, hence only the first two or three principal components are required for further analysis.\nLarge Loadings for a Principal Component (PC) axis indicate that a variable is important in explaining variation along that axis. The main loadings on the first two PC axes as assessed by Mardia's Criterion are shown in Table 3. The loudness of a song influences the direction of the first axis the most. Songs that are louder will on average have lower scores on this first axis. Similarly, energy and acousticness also have high influence. Therefore the first PC identifies a trend in the characteristics of songs; songs that have high levels of acousticness tend to have low energy and are less loud. Furthermore, given that the first PC contains the highest information of all of the components, this suggests that 'loudness', 'acousticness' and 'energy' provide more information regarding variation between songs than the other variables.\nTable 3- Loadings of each Variable for the Two Main Principal Component Axes\n\r\rVariable\rLoading on First PC Axis\r\r\r\rLoudness\r-0.4761\r\rEnergy\r-0.4423\r\rAcousticness\r0.4194\r\r\r\r\r\rVariable\rLoading on Second PC Axis\r\r\r\rSections\r0.6664\r\rDuration / ms\r0.6541\r\r\r\rIt appears that the first principal component is partly splitting the data between songs that were 'hits' and those that were 'flops'. As discussed, high values of this principle component are associated with high acousticness and low energy/loudness. Therefore it appears that songs with high acousticness perform worse, in general. These poorly performing songs also generally have larger number of sections.\n\r4.2 Cluster Analysis\rThe K-means clustering algorithm is applied to the standardized data, in order to partition the data cloud by songs with similar characteristics. The plot of the within sum of squares for different numbers of clusters suggests that eight clusters should suffice in the analysis (at the 'elbow' of the curve).\nAs this dataset is relatively large, a subsample of the songs is initially clustered and then the estimated cluster centroids are used as seeds for analysis on the full dataset. Further, the clusters can be depicted on the principal component axes. These plots show that the first and second PC axes partly split the clusters. For example, clusters 6 and 7 are well separated from most other clusters, with large values on the first principal component. Since the first principal component has a large positive loading for acousticness and negative loadings for energy and loudness, this suggests that songs in these clusters are similarly characterized by higher levels of acousticness and lower levels of energy and loudness. This is consistent with the results in Table 4, where clusters 6 and 7 have large (scaled) acousticness values of 2.26 and 1.76, respectively.\nTable 4- Centroids of Each Cluster\n\r\rCluster\r1\r2\r3\r4\r5\r6\r7\r8\r\r\r\rDanceability\r-0.564\r-0.415\r0.805\r0.748\r-0.511\r-1.229\r-0.874\r0.055\r\rEnergy\r0.608\r0.629\r0.077\r0.283\r0.412\r-2.230\r-1.312\r-0.972\r\rLoudness\r0.456\r0.378\r0.229\r0.361\r-0.024\r-2.737\r-1.505\r-0.283\r\rSpeechiness\r-0.162\r0.124\r2.308\r-0.298\r-0.193\r-0.442\r0.344\r-0.512\r\rAcousticness\r-0.547\r-0.375\r-0.062\r-0.337\r-0.449\r2.263\r1.762\r0.817\r\rInstrumentalness\r-0.412\r-0.276\r-0.470\r-0.431\r2.008\r1.644\r0.898\r-0.388\r\rLiveness\r-0.163\r2.437\r0.041\r-0.236\r-0.109\r-0.348\r-0.065\r-0.336\r\rValence\r0.273\r-0.224\r0.642\r0.794\r-0.559\r-1.206\r-0.619\r-0.468\r\rTempo\r0.903\r-0.066\r-0.296\r-0.363\r0.146\r-0.568\r-0.273\r-0.198\r\rduration_ms\r-0.145\r-0.107\r-0.105\r-0.199\r0.302\r-0.028\r6.044\r0.031\r\rchorus_hit\r-0.155\r0.410\r-0.082\r-0.084\r0.261\r0.196\r0.224\r-0.061\r\rsections\r-0.101\r-0.232\r-0.034\r-0.172\r0.121\r0.008\r5.755\r0.062\r\r\r\rIn order to assess which variables play the largest role in separating out the clusters, a separate one-way ANOVA can be performed to test for a difference in the mean of each variable for each cluster. By ranking the F-values, we can identify which variables are most effective at separating the groups. Table 5 shows the three variables with the largest F-values. This suggests energy, acousticness and then loudness are the variables that are most responsible for the differences between the clusters. This is consistent with the conclusion of the Principal Component Analysis, where these three variables had the highest loadings on the first principal component.\nTable 5- F-Values for Variables across the Clusters\n\r\rVariable\rF-Value\r\r\r\rEnergy\r3345.34\r\rAcousticness\r2371.59\r\rLoudness\r1346.10\r\r\r\rThe number of songs in each cluster reveals that the seventh cluster contains relatively few songs. In particular, this cluster is characterized by long song duration and a large number of sections in each song (Table 4) and indicates songs with quite different characteristics to those in the rest of the dataset.\nSome of the clusters of songs have far higher proportions of 'hits' than other clusters. Songs in clusters 3 and 4 display a 77% and 73% chance of being hits, respectively (Table 6). These clusters have higher average danceability scores than the other clusters (Table 4). Hence, songs that are better for dancing are more likely to be popular. This is consistent with the results from Table 1, where it was shown that danceability was positively correlated with song popularity.\nTable 6- Percentage of Songs in each Cluster that are Hits\n\r\rCluster\r1\r2\r3\r4\r5\r6\r7\r8\r\r\r\rPercentage of Hits (%)\r47.74\r37.76\r77.02\r73.88\r2.48\r0.73\r1.41\r56.14\r\r\r\r\r\r5 Discussion\rPrior research has concluded that successful songs tend to be \u0026quot;‘happier’, more ‘party-like’, less ‘relaxed’ and more ‘female’ than most\u0026quot; (Interiano et al., 2018, p.1). The evidence presented here is fairly consistent with these results, as songs with higher danceability (more 'party-like') and more energy (less 'relaxed') tend to perform better. On a more general level, this information could be used to inform music artists about how to increase the likelihood that a song will be a 'hit' by altering the characteristics of that song to be similar to songs in clusters with a larger proportion of 'hits'.\nOne can also see that between clusters there is a great deal of heterogeneity. The third cluster is the only cluster to have a large value for 'speechiness', in particular containing songs with a large amount of spoken words, possibly of the rap genre of music (Table 4). Further, the results of the Exploratory, Principal Component and Cluster Analyses are consistent. Energy, instrumentalness and loudness are the three variables that play the largest role in separating out the clusters (Table 3). These three variables also have the highest loadings on the first principal component, suggesting that they explain more of the variation in the data than the other variables. Furthermore, clusters of songs with high danceability and low instrumentalness were associated with greater popularity. This is consistent with the results in Table 1.\nCluster analysis is a useful tool for generating hypotheses. For example, consider clusters 2 and 5. The centroids of these clusters are fairly similar, with roughly equal levels of danceability. The distinguishing difference between songs in each cluster are the levels of instrumentalness and liveness (Table 4). One could then formulate and test the hypothesis that the difference in the performance of songs between these two clusters is driven by differences in instrumentalness. If this is true, this suggests that an artist could increase the popularity of their song if it has similar characteristics to songs in cluster 5 by reducing the level of instrumentalness (i.e. by adding more vocal elements to their song).\nWe may also consider different hypotheses about the performance of songs contained within a given cluster. For example, are there differences in the characteristics of songs in cluster 1 that are 'hits' and 'flops' and which variables are the most important in explaining the within cluster popularity? This would enable an artist within a given genre (songs that have certain characteristics) to alter their music to increase the likelihood that the songs would be successful. The characteristics that increase song success may also differ between clusters.\nIn order to make predictions regarding the popularity of a new song based on its attributes, a Principal Component Logistic Regression model which utilizes the principal components as regressors could be fitted. This is useful in overcoming issues of collinearity between variables, by excluding some of the low variance principal components from the model. Hence, a new song based on the discussed variables that has maximal probability of being a hit could be created.\n\r6 Conclusion\rThis report has uncovered several insights through the use of Principal Component and Cluster Analyses, some of which were not obvious when first exploring the data. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. Clusters of songs with higher rates of success were associated with higher levels of danceability and lower instrumentalness (i.e. more vocal elements). Energy, acousticness and loudness were the variables that had the greatest influence on separating out the clusters and these variables also had the highest absolute loadings on the first principal component axes. From these conclusions, several hypotheses concerning differences in the characteristics and performance of songs across different clusters and within the same cluster were generated. These hypotheses included \u0026quot;Is the difference in the performance between songs in clusters 2 and 5 driven by differences in instrumentalness?\u0026quot; or \u0026quot;Do songs in cluster 1 that are 'hits' exhibit different characteristics to those songs that are 'flops'?\u0026quot; and suggest areas for future research.\n\rReferences\rBrian S. Everitt. 2005. \u0026quot;An R and S-Plus Companion to Multivariate Analysis\u0026quot;. Springer.\nDaniel Zelterman. 2015. \u0026quot;Applied Multivariate Statistics with R\u0026quot;. Springer.\nAurelien Geron. 2019. \u0026quot;Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow\u0026quot;. O'Reilly Media. pp 213-274.\nMichael H. Kutner, Christopher J. Nachtsheim, John Neter, William Li. 2013. \u0026quot;Applied Linear Statistical Models.\u0026quot; McGraw Hill Education.\nGarrett Grolemund, Hadley Wickham. 2017. \u0026quot;R for Data Science.\u0026quot; O'Reilly Media. doi: https://r4ds.had.co.nz/.\nMyra Interiano, Kamyar Kazemi, Lijia Wang, Jienian Yang, Zhaoxia Yu and Natalia L. Komarova. 2018. \u0026quot;Musical Trends and Predictability of Success in Contemporary Songs in and out of the Top Charts.\u0026quot; Royal Society Open Science. doi: https://royalsocietypublishing.org/doi/10.1098/rsos.171274.\nEric A. Strobl, Clive Tucker. 2000. \u0026quot;The Dynamics of Chart Success in the U.K. Pre-Recorded Popular Music Industry.\u0026quot; Journal of Cultural Economics 24. doi: https://doi.org/10.1023/A:1007601402245.\nFarooq Ansari. 2020. Spotify Hit Predictor Dataset. kaggle.com doi: https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?fbclid=IwAR1kE9neO0sdKb3pv6g-Z-SOvPXii9Ubqx0PTRIDkZYdqaBGEhtLGTrFkLA.\nRStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.\n\rAppendix\rA) Exploratory Data Analysis\rHistograms Histograms for each continuous variable in the dataset.\n#histograms for each variable to assess the distribution and look for outliers.\rpar(mfrow = c(1, 1))\rfor (i in numeric_variables) {\rhist(spotify_data[,i], main = c(\u0026quot;Histogram of \u0026quot;, i), xlab = i, breaks = 20)\r}\rprint(\u0026#39;Range of Song Duration / milliseconds:\u0026#39;)\r## [1] \u0026quot;Range of Song Duration / milliseconds:\u0026quot;\rprint(c(range(spotify_data$duration_ms)))\r## [1] 15920 4170227\rPlot of Outliers Scatterplot of Song Duration against the number of Sections in a song, to identify outlying observations.\n#Analysis of outliers in the dataset\r#List of variables with clear outliers\routliers \u0026lt;- c(\u0026quot;duration_ms\u0026quot;, \u0026quot;sections\u0026quot;)\rplot(spotify_data[, outliers], main = \u0026quot;Plot of Outliers\u0026quot;, xlab = \u0026quot;Song Duration\u0026quot;, ylab = \u0026quot;Sections\u0026quot;)\rMatrix of Scatterplots Pairwise scatterplots for each continuous variable.\npairs(spotify_no_outliers[numeric_variables][1:200,])\rTesting for Multivariate Normality A simple test of multivariate normality is conducted by plotting the (ordered) squared Mahalanobis Distance against the corresponding quantiles of a \\(\\chi^2\\)-distribution with degrees of freedom equal to the number of variables in the dataset. If the transformed data lie along a straight line then the data are multivariate normally distributed.\n\rB) Principal Component Analysis Without Scaling\rThis appendix contains the results of a Principal Component Analysis on the unscaled numerical data. In particular, the first principal contains almost all of the variation in the data and this is provided by the 'duration' variable.\n#PCA Without Scaling\rpca.spotify.unscaled \u0026lt;- prcomp(spotify_numeric)\rsummary(pca.spotify.unscaled)\r## Importance of components:\r## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8\r## Standard deviation 112174 30.23 20.11 4.966 2.136 0.2979 0.2365 0.2245\r## Proportion of Variance 1 0.00 0.00 0.000 0.000 0.0000 0.0000 0.0000\r## Cumulative Proportion 1 1.00 1.00 1.000 1.000 1.0000 1.0000 1.0000\r## PC9 PC10 PC11 PC12\r## Standard deviation 0.1688 0.1381 0.1034 0.08701\r## Proportion of Variance 0.0000 0.0000 0.0000 0.00000\r## Cumulative Proportion 1.0000 1.0000 1.0000 1.00000\rpca.spotify.unscaled$rotation\r## PC1 PC2 PC3 PC4\r## danceability -1.302208e-07 -8.650312e-04 -8.958769e-04 -1.181289e-02\r## energy -3.102277e-07 1.729092e-03 2.404089e-04 -3.530510e-02\r## loudness -7.337131e-06 2.773744e-02 -8.162803e-03 -9.964386e-01\r## speechiness 1.318074e-08 -5.034456e-05 -7.637436e-05 -2.022660e-03\r## acousticness 3.824303e-07 -1.529234e-03 -2.182361e-04 3.912435e-02\r## instrumentalness 4.572867e-07 -3.172999e-04 1.134257e-03 2.521589e-02\r## liveness -3.717663e-08 2.124559e-04 2.317251e-04 -3.759318e-03\r## valence -3.518179e-07 1.845762e-04 -8.102301e-04 -1.605659e-02\r## tempo -8.319169e-06 9.965844e-01 7.776454e-02 2.675845e-02\r## duration_ms 1.000000e+00 9.430128e-06 -1.429315e-05 -8.819774e-06\r## chorus_hit 1.652100e-05 -7.717809e-02 9.959979e-01 -8.118795e-03\r## sections 3.641884e-05 9.347518e-03 -4.325897e-02 4.999516e-02\r## PC5 PC6 PC7 PC8\r## danceability -4.964583e-03 3.733586e-01 3.111789e-01 -1.021434e-01\r## energy 3.988151e-03 -1.320574e-01 2.489982e-01 3.197516e-01\r## loudness -4.925853e-02 -2.149171e-02 -2.318400e-02 -5.028992e-02\r## speechiness -1.256685e-03 4.660923e-02 3.200967e-02 2.167349e-02\r## acousticness -4.364288e-03 1.514271e-01 -3.634591e-01 -8.138958e-01\r## instrumentalness 1.295951e-02 -7.139996e-01 5.418293e-01 -4.239904e-01\r## liveness 1.471401e-03 -5.298768e-02 -4.544922e-02 1.110710e-01\r## valence -1.037914e-02 5.519963e-01 6.417573e-01 -1.725608e-01\r## tempo 7.282709e-03 1.279704e-03 -3.406733e-05 -6.739756e-04\r## duration_ms 3.675594e-05 9.349871e-07 -2.608640e-08 7.858695e-09\r## chorus_hit -4.433113e-02 6.574808e-04 -2.330242e-05 -2.036780e-04\r## sections -9.976053e-01 -1.716226e-02 2.436889e-03 4.258439e-03\r## PC9 PC10 PC11 PC12\r## danceability -3.101105e-01 -6.427783e-01 4.055585e-01 -2.817126e-01\r## energy 3.170263e-01 4.086354e-01 6.385141e-01 -3.766503e-01\r## loudness -6.092071e-03 -6.119759e-03 -1.309125e-02 8.556876e-03\r## speechiness 1.120431e-01 -8.222999e-02 4.896812e-01 8.586177e-01\r## acousticness 2.436457e-01 1.597009e-01 2.732911e-01 -1.464008e-01\r## instrumentalness -3.280515e-02 -1.097029e-01 -3.381879e-02 4.240195e-02\r## liveness 8.186357e-01 -5.330207e-01 -1.545167e-01 -6.798993e-02\r## valence 2.452160e-01 3.023282e-01 -2.969051e-01 1.166974e-01\r## tempo -5.212203e-04 -8.387501e-04 1.209447e-04 6.572693e-06\r## duration_ms 4.495584e-08 1.004793e-07 -1.165984e-07 3.722348e-08\r## chorus_hit -2.501794e-04 -1.733233e-04 1.018916e-04 -7.168368e-05\r## sections 1.417663e-04 -8.156361e-04 1.786454e-03 -1.727792e-03\r\rC) Clustering with Hierarchical Methods\rHierarchical clustering using Ward's method performs similarly to K-means on this dataset.\n#Euclidean Distance matrix\rDEuclidean \u0026lt;- dist(spotify_scaled)\r#cluster using Ward\u0026#39;s method\rHClusters \u0026lt;- hclust(DEuclidean, \u0026quot;ward.D2\u0026quot;)\r#Dendrogram\rplot(HClusters, xlab = \u0026quot;Observation Number\u0026quot;)\rPlotting the clusters on the first two principal component axes shows that cluster locations are generally similar to that of the K-means method.\n#Cut observations into 8 clusters\rMyClusters \u0026lt;- cutree(HClusters, 8)\r#copy dataset\rspotify_trial \u0026lt;- spotify_no_outliers\r#extract clusters\rspotify_trial$cluster \u0026lt;- MyClusters\rspotify_trial$cluster \u0026lt;- as.factor(spotify_trial$cluster)\r#plot clusters on PC axes\rggplot(spotify_trial, aes(x = scores_1, y = scores_2, colour = cluster)) + geom_point(shape=1) +\rggtitle(\u0026quot;Hierarchical Clusters on the Principal Component Axes\u0026quot;) + labs(y=\u0026quot;PC2\u0026quot;, x = \u0026quot;PC1\u0026quot;)\r\rD) Code used in the Main Report\r#1. Import Data and Relevant Libraries\r#Import data\rspotify_data \u0026lt;- read.csv(\u0026quot;C:/Users/User/Documents/St Andrews/Multivariate Analysis/Data/spotify dataset.csv\u0026quot;)\r#Remove the track, artist and url for each song (irrelevant to the purposes of this general analysis)\rspotify_data \u0026lt;- spotify_data[, 4:19]\r#Load relevant libraries:\r#three-dimensional data cloud plots\rlibrary(scatterplot3d)\r#multivariate visualization\rlibrary(lattice)\r#visualization of correlation matrix\rlibrary(corrplot)\r#Dendrograms\rlibrary(ape)\r#Choose appropriate number of clusters\rlibrary(NbClust)\r#Analysis of the Composition of different clusters\rlibrary(tidyverse)\rlibrary(dplyr)\rlibrary(ggplot2)\r#2. Dropping of Outliers from the Data and Extraction of Variables used in the PCA\r#Extract continuous variables\rvariables \u0026lt;- sapply(spotify_data, is.numeric)\rnumeric_variables \u0026lt;- vars[vars == TRUE]\r#Drop outlying observations\routliers \u0026lt;- which(spotify_data$duration_ms \u0026gt; 3e6)\rspotify_no_outliers \u0026lt;- spotify_data[-c(outliers), ]\r#Extract numeric variables for later PCA and Cluster Analysis\rspotify_numeric \u0026lt;- spotify_no_outliers[numeric_variables]\r#3. CoPlot to illustrate complex relationships in the data cloud\r#CoPlot for danceability and acousticness, stratified by Energy\rxyplot(spotify_no_outliers$danceability ~ spotify_no_outliers$acousticness |\rcut(spotify_no_outliers$energy, 3), main = \u0026quot;Coplot for Danceability and Acousticness, Cut by Energy\u0026quot;, xlab = \u0026quot;Acousticness\u0026quot;, ylab = \u0026quot;Danceability\u0026quot;)\r#4. Correlation Between Variables\r#correlation matrix\rcorr_matrix \u0026lt;- cor(spotify_no_outliers)\rsort(corr_matrix[, 16], decreasing = TRUE)\r#correlation plot\rcorrplot(corr_matrix, method=\u0026quot;ellipse\u0026quot;)\r#5. Principal Component Analysis\r#Principal Component Analysis with scaling\rpca.spotify \u0026lt;- prcomp(spotify_numeric, scale. = TRUE)\rsummary(pca.spotify)\r#Scree Plot\rplot(pca.spotify$sdev^2, xlab = \u0026quot;Principal Component\u0026quot;, ylab = \u0026quot;variance\u0026quot;, main = \u0026quot;Scree Plot for Spotify Principal Component Analysis (PCA)\u0026quot;)\rlines(pca.spotify$sdev^2)\r#Principal Component loadings (rotations)\rpca.spotify$rotation\r#Mardia\u0026#39;s Criterion to select variables with high loadings on each PC axis\rfor(i in 1:8){\rwhich.pass\u0026lt;-abs(pca.spotify$rotation[,i])\u0026gt;(0.7*max(abs(pca.spotify$rotation[,i])))\rcat(\u0026quot;\\nPC\u0026quot;,i,\u0026quot;\\n\u0026quot;,sep=\u0026quot;\u0026quot;)\rprint(pca.spotify$rotation[which.pass,i])\r}\r#use sample of data to visualize scores\rscores1 \u0026lt;- pca.spotify$x[1:500,1]\rscores2 \u0026lt;- pca.spotify$x[1:500,2]\r#plot scores\rpar(mfrow = c(1,2))\rplot(scores1,scores2,ylim=range(scores2),xlab=\u0026quot;PC1\u0026quot;,ylab=\u0026quot;PC2\u0026quot;, type=\u0026quot;n\u0026quot;,lwd=2, main = \u0026quot;Hit (1) or Flop (0)\u0026quot;)\r#Add target variable in reduced spatial plot\rtext(scores1,scores2,labels=spotify_no_outliers$target[1:500],cex=0.7,lwd=2)\r#add scroes in reduced spatial plot\rplot(scores1,scores2,ylim=range(scores2),xlab=\u0026quot;PC1\u0026quot;,ylab=\u0026quot;PC2\u0026quot;, cex = 0.1 * spotify_no_outliers$sections, main = \u0026quot;Sections\u0026quot;)\r#6. Cluster Analysis\r#Scale data\rspotify_scaled \u0026lt;- scale(spotify_numeric)\rn\u0026lt;-length(spotify_scaled[,1])\r#find within group sum of squares (WSS) for different numbers of clusters\r#WSS for first cluster\rwss1 \u0026lt;- (n - 1) * sum(apply(spotify_scaled, 2, var))\rwss \u0026lt;- numeric(0)\r#calculate WSS for 2 to 20 group partitions given by k-means clustering\rset.seed(160001695)\rfor (i in 2: 20) {\rW \u0026lt;- sum(kmeans(spotify_scaled, i)$withinss)\rwss \u0026lt;-c (wss,W)\r}\rwss\u0026lt;-c(wss1, wss)\r#Plot WSS against each cluster to select number of clusters for K-Means\rplot(1:20, wss, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Number of groups\u0026quot;, ylab = \u0026quot;within groups sum of squares\u0026quot;, lwd = 2, main = \u0026quot;Selecting the Number of Clusters for K-Means\u0026quot;)\rset.seed(160001695)\r#perform initial cluster analysis on sub-sample of data\rk8_initial \u0026lt;- kmeans(spotify_scaled[1:500, ], 8)\rset.seed(160001695)\r#use initial clusters to start the full clustering\rk8 \u0026lt;- kmeans(spotify_scaled, centers = k8_initial$centers)\rscores_1 \u0026lt;- pca.spotify$x[,1]\rscores_2 \u0026lt;- pca.spotify$x[,2]\r#Add clusters as new variable, so we can test differences and generate hypotheses #between songs in different (or the same) cluster. spotify_no_outliers$cluster \u0026lt;- k8$cluster\rscores_3 \u0026lt;- pca.spotify$x[, 3]\rpar(mfrow = c(1,1))\r#Plot clusters on PC axes\rspotify_no_outliers$cluster \u0026lt;- as.factor(spotify_no_outliers$cluster)\rggplot(spotify_no_outliers, aes(x = scores_1, y = scores_2, colour = cluster)) +\rgeom_point(shape=1) + ggtitle(\u0026quot;Clusters on the Principal Component Axes\u0026quot;) + labs(y=\u0026quot;PC2\u0026quot;, x = \u0026quot;PC1\u0026quot;)\r#Centres of each of the five clusters, by variable\rround(k8$centers, 3)\r#matrix to store F Values\rF.Results \u0026lt;- matrix(NA, nrow = length(variables), ncol = 2)\rF.Results \u0026lt;- as.data.frame(F.Results)\rF.Results$Variables \u0026lt;- variables\r#aov does not enable lists to be used as variables, so we must calculate all of the F-statistics\r#individually\r#F-statistic for assessing difference in mean danceability across clusters- anova of each variable\r#for each cluster\ri \u0026lt;- 0\rfor(num_var in c(numeric_variables)){\ri \u0026lt;- i + 1\rF.Results[i, 2] \u0026lt;- summary(aov(spotify_no_outliers[[num_var]] ~\rspotify_no_outliers$cluster))[[1]][[\u0026quot;F value\u0026quot;]][1]\r}\r#Count number of songs in each cluster\rCluster \u0026lt;- table(spotify_no_outliers$cluster)\rCounts \u0026lt;- data.frame(\rNo_Clusters = factor(c(seq(1, 8)), levels=c(seq(1, 8))),\rSongs = Cluster\r)\r#Plot number of songs in each cluster\rggplot(data = Counts, aes(x = No_Clusters, y = Songs.Freq, fill = No_Clusters)) +\rgeom_bar(colour=\u0026quot;black\u0026quot;, stat=\u0026quot;identity\u0026quot;) + ggtitle(\u0026quot;Number of Songs Contained Within Each Cluster\u0026quot;) + labs(y=\u0026quot;Song Frequency\u0026quot;, x = \u0026quot;Cluster Number\u0026quot;)\r#Percentage of songs that are \u0026#39;hits\u0026#39; in each cluster\r#Number of Clusters\rN_clusters \u0026lt;- c(seq(1, 8))\r#Empty vector of Percentages\rPercentage_target \u0026lt;- c(rep(NA, 8))\rfor (i in N_clusters) {\rx \u0026lt;- filter(spotify_no_outliers, spotify_no_outliers$cluster == i)\rPercentage_target[i] \u0026lt;- (dim(filter(x, x$target == 1))[1] / dim(x)[1]) * 100\r}\r\r\r","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"07ac9ee104b1bdab9cace174d750a76f","permalink":"https://domscruton.github.io/project/spotify/spotify/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/project/spotify/spotify/","section":"project","summary":"Abstract\nThis report utilizes Principal Component and Cluster Analysis to assess differences between the performance of 5872 songs from the 2000's on the basis of 13 numeric variables. Firstly, the data contains a large amount of independent information, with the first three principal components consisting of just 54.","tags":["Other"],"title":"Spotify Multivariate Analysis Report","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://domscruton.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://domscruton.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://domscruton.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://domscruton.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]