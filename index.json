[{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n  Download my resum√©.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m a Graduate Data Scientist in Household Risk \u0026amp; Retail Pricing at Admiral Group PLC, a FTSE 100 Insurance firm.\nMy work involves optimizing the pricing structure through advanced and traditional Statistical Modelling and Machine Learning approaches. I am particularly interested in Stochastic Optimization, Statistical Inference and Bayesian Machine Learning (and its applications in Reinforcement Learning and Bayesian Networks).\n Download my resum√©. --\r","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cc46d9ce78a2d7ec82db57f9c44aad94","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I\u0026rsquo;m a Graduate Data Scientist in Household Risk \u0026amp; Retail Pricing at Admiral Group PLC, a FTSE 100 Insurance firm.\nMy work involves optimizing the pricing structure through advanced and traditional Statistical Modelling and Machine Learning approaches.","tags":null,"title":"Dominic Scruton","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://domscruton.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"Intuition behind the Link function, discussion of the various model fitting techniques and their advantages \u0026amp; disadvantages, derivation of IRLS using Newton-Raphson and Fisher Scoring. IRLS is then implemented in R\nGLMs - A Natural Extension of the Linear Model The first model we naturally learn on any statistics-based course is Simple Linear Regression (SLR). Despite placing strong (linear) assumptions on the relationship between the response and covariates, as well as the error distribution if we are interested in statistical inference, the Linear model is a surprisingly useful tool for representing many natural processes. However, when we wish to deal with non-linear random variable generating processes, such as the probability of occurrence of an event from a binary or multinomial distribution, or for the modelling of counts within a given time period, we need to generalize the Linear Model.\nGeneralized Linear Models enable us to explicitly model the mean of a distribution from the exponential family, such that predictions lie in a feasible range and the relationship between the mean and the variance is appropriate to perform reliable inference. In this brief blog we discuss the intuition behind the Generalized Linear Model and Link function and prove the method of Iteratively ReWeighted Least Squares enables us to fit a GLM, before implementing it in R.\nGeneralized Linear Models have 3 components:\n1) Random Component Error Structure\nyi‚ÄÑ‚àº‚ÄÑexponential‚ÄÖfamil**y‚ÄÖdistibution\nWe also typically assume each yi is independent and identically distributed, although this assumption can be relaxed through the use of Generalized Estimating Equations (GEE\u0026rsquo;s).\n2) Systematic Component/ Linear Predictor\n$$\\eta_i = \\beta_0 + \\sum_{i = 1}^{p}\\beta_p x_{i, p}$$\n3) Link Function\nùîº[yi|x1,‚ÄÜi,‚ÄÜ\u0026hellip;,‚ÄÜxp,‚ÄÜi]=Œºi g(Œºi)=Œ∑i\nIt should be made clear that we are not modellig the response yi explicitly, but rather modelling the mean of the distibution, Œºi. Predictions for each observation i are given by Œºi, with each yi assumed to be centred around Œºi in expectation but with an error term that has a distibution specified by the member of the exponential family used. Therefore, the link function does not transform the response yi but instead transforms the mean Œºi. Note that the linear model is a specific type of GLM, where yi‚ÄÑ‚àº‚ÄÑNorma**l and $g(\\mu_i) = \\mu_i = \\eta_i = \\beta_0 + \\sum_{i = 1}^{p}\\beta_p x_{i, p}$. For a Poisson GLM, each yi is a r.v. simulated from the Poisson distribution with mean Œºi, hence yi has a Poisson error distribution, the difference between yi and $\\hat{y_i} = \\mu_i$.\nLink Functions So Generalized Linear models are simply a natural extension of the Linear Model. They differ through the explicit introduction of a link function (the link function for the linear model is simply the identity, g(Œº)=Œº) and through the specification of a mean-variance relationship (the response belongs to a member of the exponential family). Using a link function allows us to transform values of the linear predictor to predictions of the mean, such that these predictions are always contained within the range of possible values for the mean, Œºi.\nWhen choosing a link function, there is no \u0026lsquo;correct\u0026rsquo; choice, however there are a few properties we require to be able to interpret and fit a model:\n  The link function transforms the linear predictor such that the prediction of Œºi for each yi is within the range of possible values of Œº.\n  The link function must be monotonic and therefore have a unique inverse. That is, each value on the linear predictor must be mapped to a unique value of the mean and the link function must preserve the order/ranking of predictions.\n  The link function must be differentiable, in order to estimate model coefficients.\n  For OLS, the linear predictor X**Œ≤ can take on any value in the range (‚ÄÖ‚àí‚ÄÖ‚àû,‚ÄÜ‚àû). For a Poisson model, we require the rate parameter Œº (equivalent to the commonly used Œª), to lie in the range (0,‚ÄÜ‚àû), thus we need a link function that transforms the linear predictor Œ∑ to lie in this range. The common choice of link function for a Poisson GLM is the log-link (log(Œºi)=Œ∑i). Exponentiating the linear predictor results in Œºi‚ÄÑ‚àà‚ÄÑ(0,‚ÄÜ‚àû) as required for count data modeled via a Poisson. The log-link also results in a nice interpretation, since it exponentiates the linear predictor resulting in a multiplication of exponentiated coefficients: $log(\\mu_i) = exp(\\b_0 + \\beta_1 x_{1, i}) = \\exp(\\beta_0) \\exp(\\beta_1 x_{1, i})$. Note that we can\u0026rsquo;t use the square root function as a link function since it does not have a unique inverse (i.e. $\\sqrt(4) = \\pm 2$).\nFor the Binomial, we choose a link function that maps pi, the probability of success to the interval [0,‚ÄÜ1]. The link function $g(\\mu_i) = log(\\frac{p_i}{n - p_i}) = X \\beta_i$ is one candidate. This transforms the Linear predictor: $\\frac{p_i}{1 - p_i} = \\exp(X \\beta_i)$ $\\implies p_i = (\\frac{e^{X \\beta_i}}{1 + e ^ {X \\beta_i}}) \\in [0, 1]$ to the required range.\nWhilst we are able to choose any link function that satisfies these properties, the usual choice is to select the Canonical link function, which arises from writing the distribution in its exponential form. These link functions have nice mathematical properties and simplify the derivation of the Maximum Likelihood Estimators. We could also use an Information Criteria such as AIC to choose the best-fitting link function, although there is typically little deviation in performance, so we typically choose the link function with the most intuitive interpretation (which is often the canonical link function anyway).\nSome common distributions and Canonical links are show below:\n    Family Canonical Link (Œ∑‚ÄÑ=‚ÄÑg(Œº)) Inverse Link (Œº‚ÄÑ=‚ÄÑg‚àí1(Œ∑))    Binomial Logit: $\\eta = log \\left( \\frac{\\mu}{n - \\mu} \\right)$ $\\mu = \\frac{n}{1 + e^{-n}}$  Gaussian Identity: Œ∑‚ÄÑ=‚ÄÑŒº Œº‚ÄÑ=‚ÄÑŒ∑  Poisson Log: Œ∑‚ÄÑ=‚ÄÑlog(Œº) Œº‚ÄÑ=‚ÄÑeŒ∑  Gamma Inverse: $\\eta = \\frac{1}{\\mu}$ $\\mu = \\frac{1}{\\eta}$    The Exponential Family of Distributions A random variable y has a distribution within the exponential family if its probability density function is of the form:\n$$f(y;\\theta, \\phi) = exp \\left( \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right)$$\nwhere:\n  Œ∏ is the location parameter of the distribution\n  a(œï) is the scale/dispersion parameter\n  c(y,‚ÄÜœï) is a normalization term\n  It can also be shown that ùîº[y]=b‚Ä≤(Œ∏) and Var[y]=b‚Ä≥(Œ∏)a(œï).\nLikelihood Analysis and Newton-Raphson Method The fitting of any form of Statistical Learning algorithm involves an optimization problem. Optimization refers to the task of either minimizing or maximizing some function f(Œ≤) by altering Œ≤. We usually phrase most optimization problems in terms of minimizing f(Œ≤). For the case of parametric learning models (we place distributional assumptions on the target/response yi, such that they are drawn independently from some probability distribution pmodel(y,‚ÄÜŒ∏)), we we can also use the process of Maximum Likelihood to find model coefficients. Under this approach, we have the following optimization problem:\nŒ≤*‚ÄÑ=‚ÄÑargma**x(l(yi|Œ≤))\nwhere l(yi) is the likelihood function. The likelihood can be interpreted as the probability of seeing the data in our sample given the parameters and we naturally wish to maximize this quantity to obtain a good model fit.\nWhy Maximum Likelihood?\nThere are several reasons why the process of Maximum Likelihood is preferential:\n  Simple and intuitive method to find estimates for any parametric model.\n  For large samples, MLE\u0026rsquo;s have useful properties, assuming large n and i.i.d (independent and identically distributed) samples\n  $\\mathop{\\mathbb{E}}[\\hat{\\theta}_{MLE}] = \\theta$ (Unbiased)\n  $Var(\\hat{\\theta}_{MLE}) = \\frac{1}{n I(\\theta)}$, where I(Œ∏) is the Fisher Information in the sample. That is, we can calculate the variance of model coefficients and hence perform inference\n  The MLE also achieves the Cramer Lower Bound, that is it has the smallest variance of any estimator, thus is Asymptotically Efficient.\n  However, there are several disadvantages to Maximum Likelihood, particularly if the purpose of modelling is for prediction. Under Maximum Likelihood, we fit exactly to the training dataset, resulting in overfitting and poor generalization to unseen data.\n  Consider the general form of the probability density function for a member of the exponential family of distributions:\n$$f(y;\\theta, \\phi) = exp \\left( \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right)$$\nThe likelihood is then (assuming independence of observations):\n$$L(f(y_i)) = \\prod_{i = 1}^n exp \\left( \\frac{1}{a(\\phi)} (y_i \\theta_i - b(\\theta_i) + c(y_i, \\phi) \\right)$$\nwith log-likelihood (since the log of the product of exponentials is the sum of the exponentiated terms):\n$$log(L(f(y_i))) = l(f(y_i)) = \\sum_{i = 1}^n \\frac{1}{a(\\phi)}(y_i \\theta_i - b(\\theta_i)) + c(y_i, \\phi)$$\nSince the logarithmic function is monotonically increasing, order is preserved hence finding the maximum of the log-likelihood yields the same result as finding the maximum of the likelihood. We wish to maximize this log-likelihood, hence we can differentiate, equate to zero and solve for Œ≤j (We could also ensure the second derivative evaluated at Œ≤j is negative, therefore we have maximized (and not minimized) the log-likelihood). Via the Chain Rule we have:\nEquation 1\n$$\\frac{\\partial l(f(y_i))}{\\partial \\beta_j} = \\sum_{i = 1}^n \\frac{\\partial l(f(y_i))}{\\partial \\theta_i} \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j} = 0$$\nThis is also known as the Score function, since it tells us how sensitive the model is to changes in Œ≤ at a given value of Œ≤. Since we differentiate with respect to each coefficent, Œ≤j (j‚ÄÑ‚àà‚ÄÑ[0,‚ÄÜ1,‚ÄÜ\u0026hellip;,‚ÄÜK]), we have a system of (K‚ÄÖ+‚ÄÖ1) equations to solve.\nChain Rule\nThe Chain Rule is often used in Optimization problems. Here we can utilize the chain rule by recognizing that, as seen in 3.1, the likelihood is a function of the location parameter of the distribution, Œ∏, which in turn is a function of the mean Œº. Via the link function, we have that g(Œºi)=Œ∑i and via the linear predictor, $\\eta_i = \\beta_0 + \\sum_{i = 1}^{p}\\beta_p x_{i, p}$.\nEach of these individual partial derivatives can then be identified:\n$$\\frac{\\partial l_i}{\\partial \\theta_i} = \\frac{y_i - b'(\\theta_i)}{a(\\phi)}$$\n$$\\frac{\\partial \\theta_i}{\\partial \\mu_i} = \\frac{1}{V(\\mu_i)}$$\nsince $\\frac{\\mu_i}{\\theta_i} = b''(\\theta_i) = V(\\mu_i)$ where V is the variance function of the model as it dictates the mean-variance relationship.\n$$\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\frac{1}{g'(\\mu_i)}$$\nsince $g(\\mu_i) = \\eta_i \\implies \\frac{\\partial \\eta_i}{\\partial \\mu_i} = g'(\\mu_i)$. Finally:\n$$\\frac{\\partial \\eta_i}{\\partial \\beta_j} = x_j$$\nPutting this all together yields the Maximum Likelihood Equations:\n$$\\sum_{i = 1}^n \\frac{(y_i - \\mu_i) x_{i, j}}{a(\\phi) V(\\mu_i) g'(\\mu_i)} = 0$$\nLeast Square Vs Maximum Likelihood If we assume a normal distribution, there are two methods to solve exactly for the coefficients (we could also use Gradient Descent however this does not typically yield an exact solution).\nHowever, this does not have a closed form solution (what does it mean for a linear system of equations to have a closed form solultion) (except for the normal distribution). As a sanity check to confirm our conclusions thus far, note that for the normal distribution we have the Normal Equations which have the standard closed form solution (what is it)?. The Maximum Likelihood equations are a set of equations for the regression coefficients Œ≤‚ÄÑ=‚ÄÑ(Œ≤0,‚ÄÜ\u0026hellip;,‚ÄÜŒ≤1)\nEffectively, we need to find coefficients Œ≤j (which for each observation yi affect our prediction of Œºi, g‚Ä≤(Œºi) and V(Œºi) via our distributional assumptions for the relationship between the mean and the variance), such that summing these terms over all observations gives 0. To solve this, we use the Newton - Raphson Method\nIteratively Reweighted Least Squares (IRLS) Recall the Newton - Raphson method for a single dimension. We wish to find the root of the function (in this case the value of Œ≤ such that the derivative of the log-likelihood is 0). In One-Dimension, to find the root of function f we have:\n$$x_{t+1} = x_t - \\frac{f(x_t)}{f'(x_t)}$$\nThe closer f(xt) is to 0, the closer we are to the root, hence the step change between iterations will be smaller.\n  Instead of the log-likelihood function *f*(*x*), we want to optimize its derivative. Therefore we have the following Newton-Raphson equation in One-Dimension: $$x_{t+1} = x_t - \\frac{f'(x_t)}{f''(x_t)}$$\nor for the vector of coefficient estimates at iteration t:\n$$\\beta_{t+1} = \\beta_t - \\left( \\frac{\\partial l}{\\partial \\beta_j}(\\beta_t) \\right) \\left( \\frac{\\partial^2 l}{\\partial \\beta_j \\partial \\beta_k} \\right)^{-1}$$\nThe Newton-Raphson technique is derived by considering the Taylor Expansion about the solution Œ≤* (that sets $\\frac{\\partial l}{\\partial \\beta}$ to zero).\n$$0 = \\frac{\\partial l}{\\partial \\beta}(\\beta^*) - (\\beta - \\beta^*) \\frac{\\partial^2 l}{\\partial \\beta_j \\beta_k} + \u0026hellip;$$\nIf we ignore all derivative terms higher than 2n**d order, we can derive and iterative solution. Under the Newton-Raphson approach, the function being minimized is approximated locally by a quadratic function, and this approximated function is minimized exactly. We then have:\nŒ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑŒ≤t‚ÄÖ‚àí‚ÄÖHt‚àí1Ut\nwhere UtistheScoreVecto**r evaluated at Œ≤t. Ht denotes the (p + 1) x (p + 1) Hessian matrix of second Derivatives\nGiven that $\\frac{\\partial l}{\\beta_j} = \\nabla_{\\beta} l = \\frac{(y_i - \\mu_i)}{a(\\phi)} \\frac{x_{i,j}}{V(\\mu_i)}\\frac{1}{g'(\\mu_i)}$, the Hessian is then:\nEquation 4\n$$\\nabla^{2}_{\\beta} = \\sum_{i = 1}^n \\frac{x_{i,j}}{a(\\phi)} \\left( (y_i - \\mu_i)' \\frac{1}{g'(\\mu_i)} \\frac{1}{V(\\mu_i)} + (y_i - \\mu_i) \\left( \\frac{1}{g'(\\mu_i)} \\frac{1}{V(\\mu_i)} \\right)' \\right)$$\nby Product Rule. yi is a data point so does not depend on Œ≤.\n$$\\frac{\\partial \\mu_i}{\\partial \\beta_k} = \\frac{\\mu_i}{\\eta_i} \\frac{\\eta_i}{\\beta_k} = \\frac{1}{g'(\\mu_i)} x_k$$\nhence the first term becomes:\n$$\\frac{x_{i,j}}{a(\\phi)} \\left( - \\frac{x_{i, k}}{g'(\\mu_i)} \\right) \\frac{1}{g'(\\mu_i)} \\frac{1}{V(\\mu_i)} = - \\frac{x_{i, j} x_{i, k}}{a(\\phi)(g'(\\mu_i))^2} \\frac{1}{V(\\mu_i)}$$\nOf course, if we differentiate by the same Œ≤-coefficient, we have xj2, which are values on the diagonal of the Hessian matrix, which recall is $\\begin{bmatrix} \\left( \\frac{\\partial ^2 l}{\\partial \\beta_j^2} \\right) \u0026amp; \\left( \\frac{\\partial ^2 l}{\\partial \\beta_k \\partial \\beta_j} \\right)\\\\ \\left( \\frac{\\partial ^2 l}{\\partial \\beta_j \\partial \\beta_k} \\right) \u0026amp; \\left( \\frac{\\partial ^2 l}{\\partial \\beta_k^2} \\right)\\\\ \\end{bmatrix}$ in 2-Dimensions.\nNow consider the 2n**d term in equation 4:\n$$\\left( \\frac{1}{g'(\\mu_i)} \\frac{1}{V(\\mu_i)} \\right)'$$\nIf we used Newton-Raphson, we would need to calculate this derivative. However, if we use Fisher Scoring, this term cancels out and we don\u0026rsquo;t need to calculate the derivative. Fisher Scoring is a form of Newton\u0026rsquo;s Method used in statistics to solve Maximum Likelihood equations numerically. Instead of usig the inverse of the Hessian, we use the inverse of the Fisher Information matrix:\nFisher Scoring Œ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑŒ≤t‚ÄÑ=‚ÄÑJ‚àí1‚àál where J‚ÄÑ=‚ÄÑùîº[‚àí‚àá2l], the expected value of the negative Hessian.\nTaking the negative expected value from equation 4, the first term becomes\n$$\\mathop{\\mathbb{E}} \\left( - - \\frac{x_{i,j} x_{i,k}}{a(\\phi) (g'(\\mu_i))^2} \\frac{1}{V(\\mu_i)} = \\frac{x_{i,j}x_{i,k}}{a(\\phi) (g'(\\mu_i))^2} \\frac{1}{V(\\mu_i)}$$\nsince none of the above values depende on y hence are all constant. The 2n**d term in equation 4 becomes:\n$$\\mathop{\\mathbb{E}} \\left( - \\frac{x_{i,j}}{a(\\phi)}(y_i - \\mu_i) \\left( \\frac{1}{g'(\\mu_i) V(\\mu_i)} \\right) ' \\right) = - \\frac{x_{i,j}}{a(\\phi)}(y_i - \\mu_i) \\left( \\frac{1}{g'(\\mu_i) V(\\mu_i)} \\right)' \\mathop{\\mathbb{E}}(y_i - \\mu_i)$$\nbut this expectation is equal to zero and the second term therefore vanishes. We then have:\n$$J = \\sum_{i = 1}^n \\frac{x_{i,j} x_{i,k}}{a(\\phi) (g'(\\mu_i))^2 \\frac{1}{V(\\mu_i)}} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}$$\nThis can be rewritten as:\nJ‚ÄÑ=‚ÄÑXTW****X\nwhere\n$$\\mathbf{W} = \\frac{1}{a(\\phi)} \\begin{bmatrix} \\frac{1}{V(\\mu_1)} \\frac{1}{(g'(\\mu_1))^2} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026hellip; \u0026amp; \\\\ \u0026amp; \u0026amp; \\frac{1}{V(\\mu_n)(g'(\\mu_n))^2}\\\\ \\end{bmatrix}$$\nFrom Fisher Scoring, we have:\nŒ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑŒ≤t‚ÄÑ=‚ÄÑJ‚àí1‚àáŒ≤l(Œ≤t)\nWe can rewrite $\\nabla_{\\beta}l = \\sum_{i = 1}^n \\frac{y_i - \\mu_i}{a(\\phi)} frac{1}{a(\\phi)} \\frac{x_{i,j}}{V(\\mu_i g'(\\mu_i))}$ as XTD****V‚àí1(y‚ÄÖ‚àí‚ÄÖŒº) where:\n$$\\mathbf{D} = \\begin{bmatrix} \\frac{1}{g'(\\mu_1)} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026hellip; \u0026amp; \\\\ \u0026amp; \u0026amp; \\frac{1}{g'(\\mu_n)} \\\\ \\end{bmatrix}$$\n$$\\mathbf{V}^{-1} = \\frac{1}{a(\\phi)} \\begin{bmatrix} \\frac{1}{V(\\mu_1)} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026hellip; \u0026amp; \\\\ \u0026amp; \u0026amp; \\frac{1}{V(\\mu_n)} \\\\ \\end{bmatrix}$$\nThen for the Fisher Equation we have:\nŒ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑŒ≤t‚ÄÖ+‚ÄÖ(XTW****X)‚àí1XTD****V‚àí1(y‚ÄÖ‚àí‚ÄÖŒº)\nWe can write this more generally, by noting that W is the same as D****V‚àí1, except we have $\\frac{1}{(g'(\\mu_i))^2}$.\n‚üπD****V‚àí1‚ÄÑ=‚ÄÑW****M\nwhere\n$$\\mathbf{M} = \\begin{bmatrix} \\frac{1}{g'(\\mu_1)} \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026hellip; \u0026amp; \\\\ \u0026amp; \u0026amp; \\frac{1}{g'(\\mu_1)} \\\\ \\end{bmatrix}$$\n‚üπŒ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑŒ≤t‚ÄÖ+‚ÄÖ(XTW**X)‚àí1XTW**M(y‚ÄÖ‚àí‚ÄÖŒº)\nWe can already calculate each of these terms and thus generate an iterative model-fitting algorithm. We can update the Œ≤\u0026rsquo;s until convergence of the algorithm. We can then simplify the equation:\nŒ≤t‚ÄÖ+‚ÄÖ1‚ÄÑ=‚ÄÑ(XTW**X)‚àí1(XTW**X)bet**at‚ÄÖ+‚ÄÖ(XTW**X)‚àí1XTW**M(y‚ÄÖ‚àí‚ÄÖŒº)=(XTW**X)‚àí1XTW(X**Œ≤t‚ÄÖ+‚ÄÖM(y‚ÄÖ‚àí‚ÄÖŒº))‚ÄÑ=‚ÄÑ(XTW**X)‚àí1XTW**Zt\nwhere Zt‚ÄÑ:=‚ÄÑŒ∑t‚ÄÖ+‚ÄÖM(y‚ÄÖ‚àí‚ÄÖŒº)\nThis is \u0026ldquo;iteratively Reweighted Least Squares\u0026rdquo;- at each iteration we are solving a weighted least squares problem, is iterative because we update W\u0026rsquo;s and Z\u0026rsquo;s at the same time. The amount the algorithm updates depends on two things- Zt and W. For Zt, larger deviation between y and Œº results in larger steps in the iteration procedure. Unless we have a saturated model, $y \\not \\mu$ (observed values don\u0026rsquo;t equal predicted values), there is a trade-off as we vary Œ≤, resulting in different discrepencies between yi and Œºi.\nThe Hessian is a matrix of second derivatives of the log-likelihood function, that is, derivatives of the derivative of the log-likelihood function. The second derivative therefore measures curvature, that is how much the first derivative changes as we vary the input. If the likelihood has a second derivative of zero, then the likelihood (cost function) is a flat line, so its value can be predicted using only the gradient. If the gradient is 1, a step size of œµ‚ÄÑ\u0026gt;‚ÄÑ0 then the likelihood function will increase by the value of œµ. If the 2n**d derivative is negative (the gradient of the likelihood function is becoming more negative fo an increase œµ at x), the likelihood function curves downwards, so the likelihood will decrease by more than œµ. That is, the likelihood decreases faster than the gradient predicts for small œµ.\nWhen our function has multiple input dimensions, there are many 2n**d derivatives (one for each feature and then for each feature crossed with every other feature), and can be collected in a matrix called the Hessian. If we look at the IRLS equation, we have 3 terms. Firstly, we have the original value of the function, the expected improvement due to the slope of the function and a correction we must apply to account for the curvature of the function. When this last term is too small, the likelihood step can actually move downhill.\nDerivation of Model Fitting Algorithms/Coefficients This derivation of Iteratively Reweighted Least Squares for GLMs follows a similar procedure to the derivation of any model fitting algorithm. Firstly, we identify an objective function over which to optimize. Typical Machine Learning problems involve minimizing some loss function, which gives discrepencies between the actual and true values. We then differentiate this function to find a minimum and use Newton - Raphson \u0026hellip; What other algorithms are there for fitting other models and how are their model-fitting algorithms derived?\nWhy Bother with Parametric Assumptions\nAdvantages - Large amounts of data can be modelled as random variables from the exponential family of distributions - If there are relatively few observations, providing a structure for the model generating process can improve predictive performance - Enables us to carry out inference on model covariates - Simple and intuitive to understand. In some industries (such as insurance), this has huge benefits- it is transparent and can fit into a rate structure\nDisadvantages - Validity of inference dependent on assumptions being satisfied - Places a very rigid structure - Typically has worse predictive performance than non-linear models, such as Boosted Trees and Neural Networks.\nIRLS Implementation In the following, we implement IRLS for Generalized Linear Models (GLMs). We could also write in C++ for a more efficient implementation.\n","date":1627851535,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627851535,"objectID":"e82bf63cb169d0ee4f2b2776a9c02e88","permalink":"https://domscruton.github.io/post/irls/","publishdate":"2021-08-01T21:58:55+01:00","relpermalink":"/post/irls/","section":"post","summary":"Intuition behind the Link function, discussion of the various model fitting techniques and their advantages \u0026amp; disadvantages, derivation of IRLS using Newton-Raphson and Fisher Scoring. IRLS is then implemented in R","tags":["Optimization","GLMs","Mathematical Statistics"],"title":"GLMs: Intuition behind the Link Function and Derivation of Iteratively Reweighted Least Squares","type":"post"},{"authors":[],"categories":[],"content":"1) Introduction- Framing the Problem The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year. The problem itself is one of classification, however the predicted probabilities of making a claim are of more interest, as often they are far below 0.5, suggesting that none of the models would classify any individuals as making a claim. This was due to only 3.64% of individuals in the training data making a claim and caused also by a lack of correlation between the target variable and most of the features, suggesting that the features lacked enough information to fully explain differences in the claims rate between individuals. The classification problem can be expressed as follows:\n$$y_i = f(X_i, \\theta)$$\nIn this case, the response, $y_i$, is the probability that individual i makes a claim within the next year. It could also be converted into a prediction (\u0026lsquo;claim\u0026rsquo; or \u0026lsquo;no claim\u0026rsquo;) by assigning a threshold; if the probability of claiming is greater than this threshold, the individual is predicted to make a claim, otherwise they are not predicted to make a claim. The goal of machine learning is to find and estimate a model, $f(X_i, \\theta)$, that takes as its arguments the data for the ith instance ($X_i$) and the estimated parameter values ($\\theta$), and provides the most accurate predictions on unseen data.\nIn general, one aims to find the model that makes these best predictions, either by classifying the individuals into \u0026lsquo;claim\u0026rsquo; or \u0026lsquo;no claim\u0026rsquo; as accurately as possible, or providing probabilities that each individual will claim that are as close as possible to the true, underlying probabilites (which may be unknown). However, in this situation, it may also be prudent to consider measures of performance that take into account the economic implications of incorrectly predicting an individual won\u0026rsquo;t claim when in fact they do. Equivalently, it may be more economical to overpredict the probabilities that certain individuals will claim, rather that underpredict these. Later, we use the ROC curve and the Area Under Curve (AUC) to compare the performance of models that takes these ideas into account by implicitly considering false negatives when comparing model performance.\nWe implement a Logistic Regression, Random Forest classification and Neural Network and the justification for these models is presented in Section 6. The performance of all models is assessed by 3-fold cross-validation, using the AUC score as a measure of model performance.\n2) Import and Tidy the Data The first step is to import the data, assess its structure and perform any preliminary steps to tidy the data.\n#import relevant libraries #Numpy for scientific computation import numpy as np #Pandas for data manipulation import pandas as pd #Matplotlib plotting library %matplotlib inline import matplotlib.pyplot as plt #Seaborn for statistical data visualization import seaborn as sns  #import test and training data training_data = pd.read_csv(\u0026quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/train.csv\u0026quot;) test_data = pd.read_csv(\u0026quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/test.csv\u0026quot;)  #Assess the structure of the data training_data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id target ps_ind_01 ps_ind_02_cat ps_ind_03 ps_ind_04_cat ps_ind_05_cat ps_ind_06_bin ps_ind_07_bin ps_ind_08_bin ... ps_calc_11 ps_calc_12 ps_calc_13 ps_calc_14 ps_calc_15_bin ps_calc_16_bin ps_calc_17_bin ps_calc_18_bin ps_calc_19_bin ps_calc_20_bin     0 7 0 2 2 5 1 0 0 1 0 ... 9 1 5 8 0 1 1 0 0 1   1 9 0 1 1 7 0 0 0 0 1 ... 3 1 1 9 0 1 1 0 1 0   2 13 0 5 4 9 1 0 0 0 1 ... 4 2 7 7 0 1 1 0 1 0   3 16 0 0 1 2 0 0 1 0 0 ... 2 2 4 9 0 0 0 0 0 0   4 17 0 0 2 0 1 0 1 0 0 ... 3 1 1 3 0 0 0 1 1 0    5 rows √ó 59 columns\n The data contains 57 features, which are a range of categorical, binomial and continuous variables, the column of target values and an id indicating a unique identity for each individual. The training data constitutes instances for 595212 individuals.\nprint(\u0026quot;Number of instances in the training set:\u0026quot;, len(training_data))  Number of instances in the training set: 595212  #summary of the training data training_data.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id target ps_ind_01 ps_ind_02_cat ps_ind_03 ps_ind_04_cat ps_ind_05_cat ps_ind_06_bin ps_ind_07_bin ps_ind_08_bin ... ps_calc_11 ps_calc_12 ps_calc_13 ps_calc_14 ps_calc_15_bin ps_calc_16_bin ps_calc_17_bin ps_calc_18_bin ps_calc_19_bin ps_calc_20_bin     count 5.952120e+05 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 ... 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000 595212.000000   mean 7.438036e+05 0.036448 1.900378 1.358943 4.423318 0.416794 0.405188 0.393742 0.257033 0.163921 ... 5.441382 1.441918 2.872288 7.539026 0.122427 0.627840 0.554182 0.287182 0.349024 0.153318   std 4.293678e+05 0.187401 1.983789 0.664594 2.699902 0.493311 1.350642 0.488579 0.436998 0.370205 ... 2.332871 1.202963 1.694887 2.746652 0.327779 0.483381 0.497056 0.452447 0.476662 0.360295   min 7.000000e+00 0.000000 0.000000 -1.000000 0.000000 -1.000000 -1.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   25% 3.719915e+05 0.000000 0.000000 1.000000 2.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 4.000000 1.000000 2.000000 6.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   50% 7.435475e+05 0.000000 1.000000 1.000000 4.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 5.000000 1.000000 3.000000 7.000000 0.000000 1.000000 1.000000 0.000000 0.000000 0.000000   75% 1.115549e+06 0.000000 3.000000 2.000000 6.000000 1.000000 0.000000 1.000000 1.000000 0.000000 ... 7.000000 2.000000 4.000000 9.000000 0.000000 1.000000 1.000000 1.000000 1.000000 0.000000   max 1.488027e+06 1.000000 7.000000 4.000000 11.000000 1.000000 6.000000 1.000000 1.000000 1.000000 ... 19.000000 10.000000 13.000000 23.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000    8 rows √ó 59 columns\n From the above, some of the continuous features have right-skew in their distributions, such as \u0026lsquo;ps_ind_01\u0026rsquo;, with a mean value of 1.9 that is much lower than its maximum value and upper quartile. Later some of these continuous variables will be scaled to improve the performance of the models that are fitted to the data.\n#The test data contains no target variable test_data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id ps_ind_01 ps_ind_02_cat ps_ind_03 ps_ind_04_cat ps_ind_05_cat ps_ind_06_bin ps_ind_07_bin ps_ind_08_bin ps_ind_09_bin ... ps_calc_11 ps_calc_12 ps_calc_13 ps_calc_14 ps_calc_15_bin ps_calc_16_bin ps_calc_17_bin ps_calc_18_bin ps_calc_19_bin ps_calc_20_bin     0 0 0 1 8 1 0 0 1 0 0 ... 1 1 1 12 0 1 1 0 0 1   1 1 4 2 5 1 0 0 0 0 1 ... 2 0 3 10 0 0 1 1 0 1   2 2 5 1 3 0 0 0 0 0 1 ... 4 0 2 4 0 0 0 0 0 0   3 3 0 1 6 0 0 1 0 0 0 ... 5 1 0 5 1 0 1 0 0 0   4 4 5 1 7 0 0 0 0 0 1 ... 4 0 0 4 0 1 1 0 0 1    5 rows √ó 58 columns\n print(\u0026quot;Number of instances in the test data:\u0026quot;, len(test_data))  Number of instances in the test data: 892816  test_data.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id ps_ind_01 ps_ind_02_cat ps_ind_03 ps_ind_04_cat ps_ind_05_cat ps_ind_06_bin ps_ind_07_bin ps_ind_08_bin ps_ind_09_bin ... ps_calc_11 ps_calc_12 ps_calc_13 ps_calc_14 ps_calc_15_bin ps_calc_16_bin ps_calc_17_bin ps_calc_18_bin ps_calc_19_bin ps_calc_20_bin     count 8.928160e+05 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 ... 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000 892816.000000   mean 7.441535e+05 1.902371 1.358613 4.413734 0.417361 0.408132 0.393246 0.257191 0.163659 0.185905 ... 5.438478 1.440265 2.875013 7.540367 0.123720 0.627756 0.554660 0.287796 0.349344 0.152428   std 4.296830e+05 1.986503 0.663002 2.700149 0.493453 1.355068 0.488471 0.437086 0.369966 0.389030 ... 2.330081 1.200620 1.694072 2.745882 0.329262 0.483403 0.497004 0.452736 0.476763 0.359435   min 0.000000e+00 0.000000 -1.000000 0.000000 -1.000000 -1.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   25% 3.720218e+05 0.000000 1.000000 2.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 4.000000 1.000000 2.000000 6.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000   50% 7.443070e+05 1.000000 1.000000 4.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 5.000000 1.000000 3.000000 7.000000 0.000000 1.000000 1.000000 0.000000 0.000000 0.000000   75% 1.116308e+06 3.000000 2.000000 6.000000 1.000000 0.000000 1.000000 1.000000 0.000000 0.000000 ... 7.000000 2.000000 4.000000 9.000000 0.000000 1.000000 1.000000 1.000000 1.000000 0.000000   max 1.488026e+06 7.000000 4.000000 11.000000 1.000000 6.000000 1.000000 1.000000 1.000000 1.000000 ... 20.000000 11.000000 15.000000 28.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000    8 rows √ó 58 columns\n A simple visual comparison of the mean of each feature for the test data suggests it is very similar to that of the training data.\n3) Exploratory Data Analysis Summary of the Features In order to understand the relationships between the different features and the binary response, and to assess the structure of each of the variables, the following information was gathered to provide a brief summary of the groups of features:\n Target: Binary response variable indicating a claim (1) or no claim (0) IND: these 18 variables refer to characeristics of each individual driver REG: 3 variables that refer to the region of the claim CAR: 16 variables related to the particular car of each individual on which the claim was made CALL: 10 variables, which are feature engineered variables relating to the theory behind pricing of autoinsurance quotations  Missing values for the binary variables are indicated by a -1, whilst the postfix \u0026lsquo;bin\u0026rsquo; indicates binary features and \u0026lsquo;cat\u0026rsquo; indicates categorical features. Features without a postfix are numerical. The exact meaning of each individual feature is not available (Porto Seguro\u0026rsquo;s Safe Driver Prediction- Welcome, 2017).\nHistograms show the distribution of each feature. We separate the plotting of histograms of the features by the four feature types as discussed above, which makes it easier to visualise the distributions of features within groups. These histograms also include missing values (-1 in value) for each of the features, which enables us to assess which features contain the majority of missing values and also understand how each feature should be prepared for machine learning algorithms.\n#histogram for each 'individual-related' feature in the dataset training_data.iloc[:, 2: 20].hist(bins = 50, figsize = (20, 15)) plt.show()     The attributes that relate to individual characteristics are mainly binary and contain few missing values.\n#histogram for each 'Regional-based' feature in the dataset training_data.iloc[:, 20: 23].hist(bins = 50, figsize = (20, 15)) plt.show()     The attribute \u0026lsquo;ps_reg_03\u0026rsquo; contains a large number of missing values and this is something we may need to take into consideration. All three attributes of the \u0026lsquo;region-based\u0026rsquo; features are either left or right skewed, so will be normalized to lie in the interval (0,1), after removing missing values. They also contain no clear outliers, so scaling in this way will not be adversely affected by outlying observations.\n#histogram for each 'Car-related' feature in the dataset training_data.iloc[:, 23: 39].hist(bins = 50, figsize = (20, 15)) plt.show()     The \u0026lsquo;car-related\u0026rsquo; features consist of a range of categorical, binary and numeric features. Some have a significant number of missing values, however these may in themselves provide explanatory power.\n#histogram for each 'Calculated (engineered)' feature in the dataset training_data.iloc[:, 39:].hist(bins = 50, figsize = (20, 15)) plt.show()     None of the engineered features contain missing values and the continuous features also contain no obvious outliers, so standardising these features to lie in the interval (0,1) should not be adversely affected by the presence of outlying observations.\n#Proportion of target instances that made a claim (1) and didn't make a claim (0) training_data[\u0026quot;target\u0026quot;].value_counts() / len(training_data)  0 0.963552 1 0.036448 Name: target, dtype: float64  The correlation matrix can be used to assess if there are any linear relationships between the target variable and the attributes in the training dataset:\ncorr_matrix = training_data.corr() corr_matrix[\u0026quot;target\u0026quot;].sort_values(ascending = False)  target 1.000000 ps_car_13 0.053899 ps_car_12 0.038790 ps_ind_17_bin 0.037053 ps_reg_02 0.034800 ps_ind_07_bin 0.034218 ps_car_04_cat 0.032900 ps_car_03_cat 0.032401 ps_reg_03 0.030888 ps_ind_05_cat 0.029165 ps_car_15 0.027667 ps_reg_01 0.022888 ps_car_05_cat 0.020754 ps_ind_01 0.018570 ps_car_01_cat 0.016256 ps_ind_08_bin 0.013147 ps_car_06_cat 0.011537 ps_ind_04_cat 0.009360 ps_ind_03 0.008360 ps_ind_12_bin 0.007810 ps_ind_14 0.007443 ps_car_11_cat 0.006129 ps_car_09_cat 0.005322 ps_ind_18_bin 0.004555 ps_ind_02_cat 0.004534 ps_ind_13_bin 0.002460 ps_ind_11_bin 0.002028 ps_calc_03 0.001907 ps_ind_10_bin 0.001815 ps_calc_01 0.001782 ps_calc_14 0.001362 ps_calc_02 0.001360 ps_calc_10 0.001061 ps_car_10_cat 0.001038 ps_calc_05 0.000771 ps_calc_09 0.000719 ps_calc_16_bin 0.000624 ps_calc_18_bin 0.000552 ps_calc_11 0.000371 ps_calc_06 0.000082 ps_calc_04 0.000033 ps_calc_07 -0.000103 ps_calc_17_bin -0.000170 id -0.000188 ps_calc_13 -0.000446 ps_calc_15_bin -0.000490 ps_calc_08 -0.001006 ps_calc_20_bin -0.001072 ps_calc_12 -0.001133 ps_car_11 -0.001213 ps_calc_19_bin -0.001744 ps_car_14 -0.004474 ps_ind_09_bin -0.008237 ps_car_08_cat -0.020342 ps_ind_15 -0.021506 ps_ind_16_bin -0.027778 ps_car_02_cat -0.031534 ps_ind_06_bin -0.034017 ps_car_07_cat -0.036395 Name: target, dtype: float64  The fact that many of the features show little correlation with the binary target suggests that either the features lack information to explain and predict the target binary response, or the features have non-linear relationships with the target variable and hence a more complex model, such as a Neural Network, which can work more effectively on datasets that are not linearly separable, could be used. Noticeably, some of the attributes exhibit very little correlation with the binary target variable and they may just be \u0026lsquo;noise\u0026rsquo; and have no real relationship with the response. On the other hand, several features relating to the type of car that each individual insures appear to have greater correlation with the target variable. It could also be the case that missing values provide information about the probability of a claim; those individuals who are more likely to claim may provide less information in order to try reduce their insurance quotations.\n4) Preparing the Data for Machine Learning Algorithms Missing Data Most machine learning algorithms can\u0026rsquo;t work with missing values. There are a number of strategies that could be used in order to deal with this problem (Geron, 2019):\ni) Get rid of the corresponding instances\nWe have already seen that some features have a large number of missing values. Therefore, getting rid of the corresponding instances of these attributes will lead to a greatly reduced sample size and the loss of potentially important information.\nii) Get rid of the whole attribute\nMany of the attributes with missing values have high correlation with the target. For example, \u0026lsquo;ps_reg_03\u0026rsquo; has a relatively high correlation of 0.0324 with the target variable and a large number of missing values. Getting rid of this feature may lead to reduced predictive power.\niii) Set the values to some value (e.g. the median)\nFor the continuous and ordinal categorical features with many levels, it is natural that we might want to fill their missing values with some value, such as the median of that feature. However, it does not make sense to fill the missing values of binary features with their median value. Therefore, binary features with missing values are converted to multinomial features, with a new category representing missing values.\nWe use a preparation pipeline to prepare the data for a machine learning algorithm, ensuring that all binary and continous features are correctly added to the model. We separate the features into two types, with different transformations carried out on each of the two types of feature:\na) Continuous and Categorical Features\nFor these features, missing values are inputted using the median value of that respective feature, using $SimpleImputer$. Then the data is normalized to lie in the interval (0,1) (via $MinMaxScaler$). Neural Networks generally expect an input value between 0 and 1 and as previously discussed, there are no extreme outliers so this scaling method is not unduly affected by outliers, providing justification for scaling these featues in this way. The categorical features have a natural ordering and are already encoded numerically, so there is no need to separate them into further columns using dummy indexing.\nb) Binary Features\nThe missing values are used as factor levels, which changes these features from binary to multinomial, as individuals with missing values may provide predictive power and information. The missing values are first converted to the number 2, so that they can be appropriately encoded using the $OneHotEncoder$ (this does not accept negative integers).\nGiven the low correlation between the target and some features, there may be a case for dropping some features from the models that we later fit. However, the correlation coefficient only measures linear correlations and may completely miss nonlinear relationships (Geron, 2019). Therefore, given the complex nature of the data, we retain all features when fitting each model.\n#Split the training data into the labels and features training_labels = training_data[\u0026quot;target\u0026quot;] training_features = training_data.drop([\u0026quot;target\u0026quot;, \u0026quot;id\u0026quot;], axis = 1)  We first need to fill in the missing values for the ordinal categorical and numerical features, so that we can impute the median values for these features.\n#Extract the features that we will treat as numerical num_attributes = [\u0026quot;ps_ind_01\u0026quot;, \u0026quot;ps_ind_02_cat\u0026quot;, \u0026quot;ps_ind_03\u0026quot;, \u0026quot;ps_ind_05_cat\u0026quot;, \u0026quot;ps_ind_14\u0026quot;, \u0026quot;ps_ind_15\u0026quot;, \u0026quot;ps_reg_01\u0026quot;, \u0026quot;ps_reg_02\u0026quot;, \u0026quot;ps_reg_03\u0026quot;, \u0026quot;ps_car_01_cat\u0026quot;, \u0026quot;ps_ind_04_cat\u0026quot;, \u0026quot;ps_car_04_cat\u0026quot;, \u0026quot;ps_car_06_cat\u0026quot;, \u0026quot;ps_car_09_cat\u0026quot;, \u0026quot;ps_car_10_cat\u0026quot;, \u0026quot;ps_car_11\u0026quot;, \u0026quot;ps_car_11_cat\u0026quot;, \u0026quot;ps_car_12\u0026quot;, \u0026quot;ps_car_13\u0026quot;, \u0026quot;ps_car_14\u0026quot;, \u0026quot;ps_car_15\u0026quot;, \u0026quot;ps_calc_01\u0026quot;, \u0026quot;ps_calc_02\u0026quot;, \u0026quot;ps_calc_03\u0026quot;, \u0026quot;ps_calc_04\u0026quot;, \u0026quot;ps_calc_05\u0026quot;, \u0026quot;ps_calc_06\u0026quot;, \u0026quot;ps_calc_07\u0026quot;, \u0026quot;ps_calc_08\u0026quot;, \u0026quot;ps_calc_09\u0026quot;, \u0026quot;ps_calc_10\u0026quot;, \u0026quot;ps_calc_11\u0026quot;, \u0026quot;ps_calc_12\u0026quot;, \u0026quot;ps_calc_13\u0026quot;, \u0026quot;ps_calc_14\u0026quot;]  We replace the -1 values (corresponding to missing values) for these features with NA, so that we can then impute the median of each feature in place of the missing values.\n#replace the numerical features with missing values encoded as NA for training data training_features[num_attributes] = training_features[num_attributes].replace(-1,np.NaN)  #replace the numerical features with missing values encoded as NA for test data test_data[num_attributes] = test_data[num_attributes].replace(-1,np.NaN)  #The numerical missing values have now been fillied with NA's training_features.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 595212 entries, 0 to 595211 Data columns (total 57 columns): ps_ind_01 595212 non-null int64 ps_ind_02_cat 594996 non-null float64 ps_ind_03 595212 non-null int64 ps_ind_04_cat 595129 non-null float64 ps_ind_05_cat 589403 non-null float64 ps_ind_06_bin 595212 non-null int64 ps_ind_07_bin 595212 non-null int64 ps_ind_08_bin 595212 non-null int64 ps_ind_09_bin 595212 non-null int64 ps_ind_10_bin 595212 non-null int64 ps_ind_11_bin 595212 non-null int64 ps_ind_12_bin 595212 non-null int64 ps_ind_13_bin 595212 non-null int64 ps_ind_14 595212 non-null int64 ps_ind_15 595212 non-null int64 ps_ind_16_bin 595212 non-null int64 ps_ind_17_bin 595212 non-null int64 ps_ind_18_bin 595212 non-null int64 ps_reg_01 595212 non-null float64 ps_reg_02 595212 non-null float64 ps_reg_03 487440 non-null float64 ps_car_01_cat 595105 non-null float64 ps_car_02_cat 595212 non-null int64 ps_car_03_cat 595212 non-null int64 ps_car_04_cat 595212 non-null int64 ps_car_05_cat 595212 non-null int64 ps_car_06_cat 595212 non-null int64 ps_car_07_cat 595212 non-null int64 ps_car_08_cat 595212 non-null int64 ps_car_09_cat 594643 non-null float64 ps_car_10_cat 595212 non-null int64 ps_car_11_cat 595212 non-null int64 ps_car_11 595207 non-null float64 ps_car_12 595211 non-null float64 ps_car_13 595212 non-null float64 ps_car_14 552592 non-null float64 ps_car_15 595212 non-null float64 ps_calc_01 595212 non-null float64 ps_calc_02 595212 non-null float64 ps_calc_03 595212 non-null float64 ps_calc_04 595212 non-null int64 ps_calc_05 595212 non-null int64 ps_calc_06 595212 non-null int64 ps_calc_07 595212 non-null int64 ps_calc_08 595212 non-null int64 ps_calc_09 595212 non-null int64 ps_calc_10 595212 non-null int64 ps_calc_11 595212 non-null int64 ps_calc_12 595212 non-null int64 ps_calc_13 595212 non-null int64 ps_calc_14 595212 non-null int64 ps_calc_15_bin 595212 non-null int64 ps_calc_16_bin 595212 non-null int64 ps_calc_17_bin 595212 non-null int64 ps_calc_18_bin 595212 non-null int64 ps_calc_19_bin 595212 non-null int64 ps_calc_20_bin 595212 non-null int64 dtypes: float64(16), int64(41) memory usage: 258.8 MB  #Import relevant packages #MinMaxScaler to normalize the features from sklearn.preprocessing import MinMaxScaler #SimpleImputer to impute the median values in place of missing values from sklearn.impute import SimpleImputer #Pipeline to create transformation pipelines to process the data from sklearn.pipeline import Pipeline  #Pipeline for the features that we treat as numerical num_pipeline = Pipeline([ #Impute missing values with the median ('imputer', SimpleImputer(strategy = \u0026quot;median\u0026quot;)), #Standardise values to lie in the range [0,1] ('min_max_scaler', MinMaxScaler()) ])  Now consider the binary features, which will become multinomial with three levels, once we specify the missing values as a new factor level. After converting these features to multinomial, we apply One Hot Encoding. This creates one binary attribute per category, adding two extra columns for each of these features to the feature matrix.\n#extract binary features cat_attributes = [\u0026quot;ps_ind_06_bin\u0026quot;, \u0026quot;ps_ind_07_bin\u0026quot;, \u0026quot;ps_ind_08_bin\u0026quot;, \u0026quot;ps_ind_09_bin\u0026quot;, \u0026quot;ps_ind_10_bin\u0026quot;, \u0026quot;ps_ind_11_bin\u0026quot;, \u0026quot;ps_ind_12_bin\u0026quot;, \u0026quot;ps_ind_13_bin\u0026quot;, \u0026quot;ps_ind_16_bin\u0026quot;, \u0026quot;ps_ind_17_bin\u0026quot;, \u0026quot;ps_ind_18_bin\u0026quot;, \u0026quot;ps_car_02_cat\u0026quot;, \u0026quot;ps_car_03_cat\u0026quot;, \u0026quot;ps_car_05_cat\u0026quot;, \u0026quot;ps_car_07_cat\u0026quot;, \u0026quot;ps_car_08_cat\u0026quot;, \u0026quot;ps_calc_15_bin\u0026quot;, \u0026quot;ps_calc_16_bin\u0026quot;, \u0026quot;ps_calc_17_bin\u0026quot;, \u0026quot;ps_calc_18_bin\u0026quot;, \u0026quot;ps_calc_19_bin\u0026quot;, \u0026quot;ps_calc_20_bin\u0026quot;]  #Replace -1 with 2 as the OneHotEncoder can't work for negative integers training_features[cat_attributes] = training_features[cat_attributes].replace(-1,2)  #apply OneHotEncoding again, now for the test data test_data[cat_attributes] = test_data[cat_attributes].replace(-1,2)  from sklearn.preprocessing import OneHotEncoder  from sklearn.compose import ColumnTransformer full_pipeline = ColumnTransformer([ #pipeline for numerical features (\u0026quot;num\u0026quot;, num_pipeline, num_attributes), #OneHotEncoder for (previously) binary features (\u0026quot;cat\u0026quot;, OneHotEncoder(), cat_attributes) ])  #use pipeline to prepare training data training_prepared = full_pipeline.fit_transform(training_features)  C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify \u0026quot;categories='auto'\u0026quot;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning)  #transform the test data based on the scaling estimated from the training data test_prepared = full_pipeline.transform(test_data)  5) The Confusion Matrix and ROC Curve as Measures of Model Performance It is important, before carrying out the model selection procedure, to consider an appropriate measure of model performance. This avoids biases that may occur when selecting a performance measure after different models have been fit.\nIn the given situation, using accuracy as a measure of performance is not suitable, because all the models fitted predict very low probabilities that individuals will make a claim and hence using the standard threshold probability of 0.5, always predict that individuals will not make a claim (i.e. always predict a negative). This is caused by the low rate of individuals making a claim within a year (just 3.64%) and the lack of information contained within the features relating to the binary target.\n#Proportion of positive and negative target values ('claim' Vs 'no claim') in the training data print(\u0026quot;Proportion of individuals who didn't make a claim:\u0026quot;, round((573518 / (21694 + 573518)), 4)) print(\u0026quot;Proportion of individuals who made a claim:\u0026quot;, round((21694 / (21694 + 573518)), 4))  Proportion of individuals who didn't make a claim: 0.9636 Proportion of individuals who made a claim: 0.0364  Accuracy also ignores the types of errors that are made. In the context of automotive insurance, incorrectly predicting that an individual won\u0026rsquo;t claim when they actually do (false negatives) will be more costly (financially) to Porto Seguro than incorrectly predicting that an individual will claim and then they actually don\u0026rsquo;t (false positives). Hence, more emphasis should be placed on reducing the false negative rate. The ROC curves of different models and the ROC AUC scores can be compared to assess model performance in this manner. The ROC curve plots the true positive rate against the falser postive rate. The true postive rate (recall) takes into account false negatives, as it is the ratio of positive instances that are correctly detected by the classifier:\n$$Recall = True Positive Rate = \\frac{TP}{TP + FN}$$\nThus, if the number of false negatives increases, the recall will fall. The ROC curve also allows us to assess the trade-off between the true positive rate (recall) and the false positive rate; the higher the recall, the higher the false positive rate will be, on average. That is, if the recall rises, this may be due to the fact we are always predicting a positive outcome, however conequently this will lead to an increase in the number of false positive that are predicted. Thus, this measure of performance enables us to assess this trade-off and incorporate the problems of false negative classification. The Area Under the Curve (AUC) provides a quantitative measure of performance between models, calculating the area under the ROC curve for a given model. A higher AUC score indicates a better performing model.\n5) Selection and Training of Models The No Free Lunch Theorem states that if we make absolutely no assumptions about the data, then there is no reason to prefer one model over another. The only way to know for sure is to evaluate them all (Geron, 2019). However, in practice we can make a few reasonable assumptions about the data. the Porto Seguro automotive insurance data provides a range of features; categorical, numeric and binary. We have shown that most of the variables have little correlation with the binary target variable of \u0026lsquo;claim\u0026rsquo; or \u0026lsquo;no claim\u0026rsquo;. This suggests that a more complex model, such as a neural network may be beneficial to make predictions for datasets that are not close to being linearly separable. However, given the large number of features in relation to the size of the data, we expect that neural networks may not achieve their \u0026lsquo;potential\u0026rsquo; as they may overfit the data quite quickly. This suggests that more simple methods for classification, such as logistic regression may also perform relatively well when making predictions on unseen data. Finally, we fit a Random Forest classification model to the training data and again assess its performance using cross-validation. Random Forest models are Ensemble Methods that average out the predictions of decision trees and introduce extra randomness when growing trees, resulting in extra tree diversity (different features are used to split nodes, not necessarily the optimal features) that leads to an increase in bias and a reduced variance. This may enable random forest models to generalize better to unseen data.\nCross-Validation K-Fold Cross Validation is a common way to evaluate different models (and with different hyperparameters). This process splits the training data into k folds (here k = 3), where (k-1) of the folds are used to train the model, the performance of which is then assessed by comparing its predictions to the target values of the unused fold, which acts as a validation set. This ensures the model is assessed on instances that were previously unseen when fitting the model.\nCross-Validation allows one to assess potential issues of overfitting and underfitting across models and provides an estimate of how each model will generalize to an independent sample of unseen data. Using this approach, we can train multiple models using different hyperparameters on the reduced training set and select the model that performs best on average for each of the k validation folds.\nAll models discussed are types of supervised machine learning; the training set fed into the algorithm includes the desired solutions (binary classifications) called labels.\na) Logistic Regression Logistic regression can be used to estimate the probability that a given instance belongs to one of the binary classes. In a similar manner to linear regression, logistic regression computes a weighted sum of input features, plus a bias term. However, it then uses the logistic function to output a number (probability) between 0 and 1. The weights used in the regression are estimated by minimizing the log loss function:\n$$J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[ y^i \\log(p^i) + (1 - y^i) \\log(1 - p^i)]$$\nThat is, when the true value (label) for a given instance is 1 (\u0026lsquo;claim\u0026rsquo;, i.e. $y^i = 1$), the loss for individual i becomes $\\log(p^i)$. Hence, a higher predicted probability leads to a reduced loss in this case and when $p^i = 1$, $\\log(p^i) =0$ and the loss or cost for instance i is zero. Hence, on average, the optimal solution will predict low probabilities for instances/ individuals that did not make a claim and high probabilities for those that did make a claim, using the input features, to minimize this loss. Logistic Regression can be used for binary classification by specifying a threshold probability, beyond which an instance is predicted to make a claim.\nfrom sklearn.linear_model import LogisticRegression log_reg = LogisticRegression(random_state = 160001695)  from sklearn.model_selection import cross_val_predict  We now use cross-validation to train the model on\n#extract the predicted probabilities for the k validation sets y_train_pred = cross_val_predict(log_reg, training_prepared, training_labels, cv = 3, method = \u0026quot;predict_proba\u0026quot;)  C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning)  #predicted values of a 'claim' (target = 1) logistic_predictions = y_train_pred[:,1]  Now we can look at the ROC curve to assess the performance of the model\nfrom sklearn.metrics import roc_curve #false positve rate, true positive rate, probability thresholds fpr, tpr, thresholds = roc_curve(training_labels, logistic_predictions)  #Create a function to plot the ROC curve def plot_roc_curve(fpr, tpr, label = None): #plot roc curve for fitted model plt.plot(fpr, tpr, linewidth = 2, label = label) #Add line for random guess model plt.plot([0,1], [0,1], 'k--') plt.xlabel('False Positive Rate', fontsize = 14) plt.ylabel('True Positive Rate (Recall)', fontsize = 14) plt.grid(True) plt.title('ROC Curve', fontsize = 18)  plot_roc_curve(fpr, tpr, \u0026quot;Logistic Regression\u0026quot;) plt.savefig(\u0026quot;roc_curve1.png\u0026quot;) plt.show()     Now we assess the area under the curve (AUC) For the Receiver Operating Characteristic (ROC) Curve:\nfrom sklearn.metrics import roc_auc_score print(\u0026quot;AUC Score for the Logistic Regression Model:\u0026quot;, round(roc_auc_score(training_labels, logistic_predictions), 4))  AUC Score for the Logistic Regression Model: 0.6215  b) Random Forest Model A Random Forest is an ensemble of decision trees, which makes predictions by taking a weighted average of the probabilities (in this case) of each tree. It also introduces extra randomness when growing trees; instead of always splitting a node by the best feature (as determined by the training set), it searches for the best feature among a random subset of the features. This may allow the model to improve its ability to generalize to new datasets, by raising its bias but reducing its variance in making predictions on new, unseen data.\nEach individual decision tree that makes up the Random Forest is estimated using the CART Training Algorithm. This is a \u0026lsquo;greedy\u0026rsquo; algorithm, in that it greedily searches the optimal feature and threshold to split the data by, without considering whether this split will lead to the best gini impurity several levels down. Thus, this model is often considered as reasonable but not optimal. Regularization for the tree can simply be controlled via the maximum depth of each decision tree, which here is set to 3.\nUse Scikit-Learn\u0026rsquo;s SGD classifier as a method for binary classification\nfrom sklearn.ensemble import RandomForestClassifier forest_clf = RandomForestClassifier(random_state = 160001695, max_depth = 3) #extract predicted probabilities for the k validation sets y_probas_forest = cross_val_predict(forest_clf, training_prepared, training_labels, cv = 3, method = \u0026quot;predict_proba\u0026quot;)  C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning)  #false positive rate, true positive rate, probability thresholds fpr_rf, tpr_rf, thresholds_rf = roc_curve(training_labels, y_probas_forest[:,1])  #Plot the ROC curves for logistic and random forest models plt.plot(fpr, tpr, \u0026quot;b:\u0026quot;, label = \u0026quot;Logistic Regression\u0026quot;) plot_roc_curve(fpr_rf, tpr_rf, \u0026quot;Random Forest\u0026quot;) plt.legend(loc = \u0026quot;lower right\u0026quot;) plt.show()     print(\u0026quot;AUC Score for the Random Forest Model:\u0026quot;, round(roc_auc_score(training_labels, y_probas_forest[:,1]), 4))  AUC Score for the Random Forest Model: 0.6036  We see that the performance of the Random Forest Regressor is not as good as that of the Logistic Regression model, as measured by the ROC curve and AUC score. In particular, the ROC curve for the Random Forest model lies under that of the Logistic Regression model for all thresholds. Hence, for a given false positive rate, the true positive rate for the random forest model is smaller, suggesting it is underestimating probabilities for those individuals that made a claim. This information is confirmed by the lower AUC score of 0.6036.\nc) Neural Network Multilayer perceptrons can be used to create a classification model for this task. For a binary classification problem, we require a single output neuron using the logistic activation function (Geron, 2019). The output will be a number between zero and one, which can be interpreted as the estimated probability of the positive class (making a claim).\nA perceptron with a single layer and single neuron (i.e. a threshold logic unit, TLU) is equivalent to a logistic regression model, as it computes a linear combination of feature inputs and uses the sigmoid function to transform these weighted linear combinations into a probability lying in the interval (0,1). Multilayer perceptrons are composed of an input layer, one or more layers of TLU\u0026rsquo;s, called hidden layers and the output layer. Every layer except the outer layer contains a bias neuron and is fully connected to the next layer. Hence, by adding more layers, we hope to train a model that improves upon the Logistic Regression model in terms of its predictive performance as measured by AUC.\nThe model is trained using backpropagation. This works by splitting the input data used to train the model into \u0026lsquo;mini-batches\u0026rsquo;. Each mini-batch of data is passed through the model and the network\u0026rsquo;s output error is measured by a loss function, in this case for the sigmoid function, which is the same as that of the logistic regression loss function:\n$$J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^{m}[ y^i \\log(p^i) + (1 - y^i) \\log(1 - p^i)]$$\nwhere m is the number of instances in the mini-batch fed in to the neural network. Then the contribution to this error of the outputs from each neuron is computed. This is a reverse pass, and measures the error gradient backward through the network. The connection weights for each neuron are then corrected using Gradient Descent to reduce the error. One cycle of a forward and reverse pass is known as an epoch.\n#import tensorflow and keras for neural networks import tensorflow as tf from tensorflow import keras  We first fit a Neural Network with two hidden layers and ten neurons per layer, using the default learning rate. Later these hyperparameters are tuned to search for a better model as assessed by AUC.\nNeural_Net = keras.models.Sequential([ #first hidden layer keras.layers.Dense(10, activation = \u0026quot;relu\u0026quot;, input_shape = training_prepared.shape[1:]), #second hidden layer keras.layers.Dense(10, activation = \u0026quot;relu\u0026quot;), #output neuron keras.layers.Dense(1, activation = \u0026quot;sigmoid\u0026quot;) ])  #Compile model Neural_Net.compile(loss = \u0026quot;binary_crossentropy\u0026quot;, optimizer = \u0026quot;sgd\u0026quot;, metrics = [\u0026quot;accuracy\u0026quot;])  We now create a function that carries out K-fold cross-validation on the Neural Net model. This is customized to calculate the AUC score as the measure of performance for each fold and then the AUC score is later averaged across the folds to provide a final score for the model.\n#KFold function splits data into k consecutive folds from sklearn.model_selection import KFold  Each of the three Neural Nets is trained with just 3 epochs to reduce computational time and allow for a quicker assessment of models:\n#Carry out K-Fold cross-validation with k = 3 folds by default def Neural_Net_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 3): #empty array to store predictions NN_pred = np.zeros(len(training_labels)) for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared): #split data into training and test set x_train, x_test=training_prepared[train_index], training_prepared[test_index] #split corresponding test and training labels y_train, y_test=training_labels[train_index], training_labels[test_index] #Fit model on training set Neural_Net.fit(x_train, y_train,epochs= epochs) #Make predictions on test test NN_pred = Neural_Net.predict(x_test) #Calculate AUC score for the kth fold print(\u0026quot;AUC cross-validation score: \u0026quot;, round(roc_auc_score(y_test, NN_pred), 5))  #Carry out cross-validation on the model Neural_Net_CrossVal(Neural_Net, k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 72s 180us/sample - loss: 0.1558 - accuracy: 0.9637 Epoch 2/3 396808/396808 [==============================] - 68s 172us/sample - loss: 0.1542 - accuracy: 0.9637 Epoch 3/3 396808/396808 [==============================] - 72s 182us/sample - loss: 0.1538 - accuracy: 0.9637 AUC cross-validation score: 0.60311 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 66s 167us/sample - loss: 0.1545 - accuracy: 0.9634 Epoch 2/3 396808/396808 [==============================] - 63s 160us/sample - loss: 0.1542 - accuracy: 0.9634 Epoch 3/3 396808/396808 [==============================] - 71s 180us/sample - loss: 0.1541 - accuracy: 0.9634- loss: 0.1541 - ac AUC cross-validation score: 0.61718 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1534 - accuracy: 0.9636 Epoch 2/3 396808/396808 [==============================] - 66s 166us/sample - loss: 0.1533 - accuracy: 0.9636 Epoch 3/3 396808/396808 [==============================] - 65s 164us/sample - loss: 0.1532 - accuracy: 0.9636 AUC cross-validation score: 0.61913  print(\u0026quot;Average AUC score\u0026quot;, (0.60311 + 0.61718 + 0.61913) / 3)  Average AUC score 0.6131399999999999  Therefore, we see that the average AUC score for the neural network model (0.61314) is slightly lower than that of the logistic regression model (0.6215). We can also see that the accuracy is roughly equal to the proportion of individuals in the training set who made a claim. As the predicted probabilities are so low, all individuals are predicted as not claiming and hence the accuracy is a useless measure of model performance in this case.\n7) Fine-Tune Models Given the results of the AUC scores, we now only consider hyperparameter tuning of the Neural Net as there is evidence that this model may provide the best predictions on unseen data if the hyperparameters are tuned well enough to improve model performance beyond that of the logistic regression model. Because the AUC score is not a standard metric in keras, we use the cross-validation function and manually assess models with different hyperparameters, instead of using grid search. The first neural network model that we fitted had two hidden layers, each with 10 neurons and used the default learning rate of 0.001.\nIn order to assess the performance of the neural network binary classifier, we use the custom cross validation function that calculates the AUC score for each fold for each set of hyperparameters. By testing on the unseen fold, an unbiased assessment of how each model might generalize to new data can be carried out.\nThere are several different hyperparameters for the neural network that we consider:\na) Number of Hidden Layers\nFor many problems, a single hidden layer can provide reasonable results. However, given the (relatively) complex nature of this problem, with a large number of fairly uninformative features, adding more layers may improve the predictive ability of a neural network.\nb) Number of Neurons per hidden layer\nAs the number of neurons per layer increases, we expect the predictive power of the model to increase. However, as with the number of hidden layers, overfitting may become an issue if too many neurons are added per layer, so cross-validation can be used to find an optimal trade-off of bias and variance of the probability predictions.\nc) Learning Rate\nThis hyperparameter determines the size of steps that the gradient descent takes when minimizing the sigmoid cost function. The lower the learning rate, the more iterations the gradient descent algorithm will take to converge. However, if the learning rate is too high, the algorithm may overshoot and diverge. The learning rate is one of the most influential hyperparameters in fitting a Neural network, so we consider three different values and compare their performance to the original learning rate of 1e-3.\nWe now create a function that can build simple Sequential Neural Networks for a given set of input values:\ndef build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3): #Define the keras model model = keras.models.Sequential() #Add an input layer model.add(keras.layers.InputLayer(input_shape = training_prepared.shape[1:])) for layer in range(n_hidden): #Assume each hidden layer contains the same number of neurons model.add(keras.layers.Dense(n_neurons, activation = \u0026quot;relu\u0026quot;)) #Add the output layer model.add(keras.layers.Dense(1, activation = \u0026quot;sigmoid\u0026quot;)) #Use SGD for backpropagation optimizer = keras.optimizers.SGD(lr = learning_rate) #compile model model.compile(loss = \u0026quot;binary_crossentropy\u0026quot;, optimizer = optimizer) return model  a) Number of Hidden Layers We use the customized cross-validation function to assess the performance of the original neural network model but instead with 1 and 3 hidden layers.\n#Then manually compare each model by considering cross-validation Neural_Net_CrossVal(build_model(n_hidden = 1, n_neurons = 10), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 57s 145us/sample - loss: 0.1757 Epoch 2/3 396808/396808 [==============================] - 57s 142us/sample - loss: 0.1552 Epoch 3/3 396808/396808 [==============================] - 69s 175us/sample - loss: 0.1546 AUC cross-validation score: 0.58507 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1552 Epoch 2/3 396808/396808 [==============================] - 57s 143us/sample - loss: 0.1550 Epoch 3/3 396808/396808 [==============================] - 56s 142us/sample - loss: 0.1548 AUC cross-validation score: 0.59959 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 53s 132us/sample - loss: 0.1540 Epoch 2/3 396808/396808 [==============================] - 66s 166us/sample - loss: 0.1539 Epoch 3/3 396808/396808 [==============================] - 63s 158us/sample - loss: 0.1538 AUC cross-validation score: 0.60671  print(\u0026quot;Average AUC score\u0026quot;, round(((0.58507 + 0.59959 + 0.60671) / 3), 4))  Average AUC score 0.5971  Neural_Net_CrossVal(build_model(n_hidden = 3, n_neurons = 10), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 71s 178us/sample - loss: 0.1575 Epoch 2/3 396808/396808 [==============================] - 65s 164us/sample - loss: 0.1548 Epoch 3/3 396808/396808 [==============================] - 55s 139us/sample - loss: 0.1542 AUC cross-validation score: 0.59475 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 55s 138us/sample - loss: 0.1548 Epoch 2/3 396808/396808 [==============================] - 54s 136us/sample - loss: 0.1546 Epoch 3/3 396808/396808 [==============================] - 56s 140us/sample - loss: 0.1545 AUC cross-validation score: 0.60568 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 59s 149us/sample - loss: 0.1537 Epoch 2/3 396808/396808 [==============================] - 60s 151us/sample - loss: 0.1536 Epoch 3/3 396808/396808 [==============================] - 57s 144us/sample - loss: 0.1535 AUC cross-validation score: 0.6113  print(\u0026quot;Average AUC score\u0026quot;, round(((0.59475 + 0.60568 + 0.6113) / 3),4))  Average AUC score 0.6039  We can see that the neural network might be suffering from issues of overfitting for 3 hidden layers and underfitting for 1 hidden layer, as reflected by the lower AUC scores in comparison to the original Neural Network model with just 1 hidden layer.\nb) Number of Neurons per Hidden Layer Again apply the custom cross-validation function to models with 5 and 15 neurons in each of the two hidden layers.\nNeural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 5), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 57s 145us/sample - loss: 0.1595 Epoch 2/3 396808/396808 [==============================] - 56s 141us/sample - loss: 0.1556 Epoch 3/3 396808/396808 [==============================] - 55s 140us/sample - loss: 0.1548 AUC cross-validation score: 0.57591 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1554 Epoch 2/3 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1551 Epoch 3/3 396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 56s 141us/sample - loss: 0.1549 AUC cross-validation score: 0.59693 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1542 Epoch 2/3 396808/396808 [==============================] - 60s 150us/sample - loss: 0.1540 Epoch 3/3 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1539 AUC cross-validation score: 0.60563  print(\u0026quot;Average AUC score\u0026quot;, round(((0.57591 + 0.59693 + 0.60563) / 3),4))  Average AUC score 0.5928  Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 15), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 59s 148us/sample - loss: 0.1587 Epoch 2/3 396808/396808 [==============================] - 56s 142us/sample - loss: 0.1545 Epoch 3/3 396808/396808 [==============================] - 60s 152us/sample - loss: 0.1540 AUC cross-validation score: 0.59848 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 59s 148us/sample - loss: 0.1547 Epoch 2/3 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1545 Epoch 3/3 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1543 AUC cross-validation score: 0.60942 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1536 Epoch 2/3 396808/396808 [==============================] - 61s 155us/sample - loss: 0.1535 Epoch 3/3 396808/396808 [==============================] - 62s 157us/sample - loss: 0.1534 AUC cross-validation score: 0.61443  print(\u0026quot;Average AUC score\u0026quot;, round(((0.59848 + 0.60942 + 0.61443) / 3), 4))  Average AUC score 0.6074  Similarly, the AUC scores for 5 and 15 neurons per hidden layer are lower than the AUC score for 10 hidden layers.\nc) Learning Rate Finally, we compare the sensitivity of the AUC score for the models under cross-validation for different values of the learning rate (1e-4 and 1e-2).\nNeural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-4), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 61s 154us/sample - loss: 0.2657 Epoch 2/3 396808/396808 [==============================] - 58s 145us/sample - loss: 0.1578 Epoch 3/3 396808/396808 [==============================] - 57s 145us/sample - loss: 0.1569 AUC cross-validation score: 0.53327 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1577 Epoch 2/3 396808/396808 [==============================] - 59s 148us/sample - loss: 0.1576 Epoch 3/3 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1575 AUC cross-validation score: 0.54585 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1568 Epoch 2/3 396808/396808 [==============================] - 61s 153us/sample - loss: 0.1567 Epoch 3/3 396808/396808 [==============================] - 61s 154us/sample - loss: 0.1566 AUC cross-validation score: 0.55127  print(\u0026quot;Average AUC score\u0026quot;, round(((0.53327 + 0.54585 + 0.55127) / 3), 4))  Average AUC score 0.5435  Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2), k = 3)  Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 60s 152us/sample - loss: 0.1560 - loss: Epoch 2/3 396808/396808 [==============================] - 63s 158us/sample - loss: 0.1539 Epoch 3/3 396808/396808 [==============================] - 64s 161us/sample - loss: 0.1535 AUC cross-validation score: 0.61233 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 60s 151us/sample - loss: 0.1542 Epoch 2/3 396808/396808 [==============================] - 64s 162us/sample - loss: 0.1540 Epoch 3/3 396808/396808 [==============================] - 60s 152us/sample - loss: 0.1539 AUC cross-validation score: 0.61899 Train on 396808 samples Epoch 1/3 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1533 Epoch 2/3 396808/396808 [==============================] - 72s 180us/sample - loss: 0.1532 Epoch 3/3 396808/396808 [==============================] - 71s 178us/sample - loss: 0.1532 AUC cross-validation score: 0.61907  print(\u0026quot;Average AUC score\u0026quot;, round(((0.61233 + 0.61899 + 0.61907) / 3), 4))  Average AUC score 0.6168  The results of this hyperparameter tuning support a model with two hidden layers, ten neurons per hidden layer and a learning rate of 1e-2 as the best trialled model, using the AUC score as a measure of performance, with an average AUC of 0.61689.\n8) Assessing Model Performance We now fit the updated Neural Network model with a learning rate of 1e-2, which was chosen as the optimal model with the highest AUC score but with 5 epochs to ensure convergence to the optimal weights at each layer (we would have used more epochs but that was too time consuming for this PC). The fitted model is then used to make predictions on the training set via cross-validation and the ROC curve is plotted to get a more accurate assessment of the model\u0026rsquo;s performance.\n#Final model with 2 hidden layers, 10 neurons per hidden layer and a learning rate of 0.01 NN_final = build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2)  #Compile Model NN_final.compile(loss = \u0026quot;binary_crossentropy\u0026quot;, optimizer = \u0026quot;sgd\u0026quot;, metrics = [\u0026quot;accuracy\u0026quot;]) #Fit model to training data NN_final.fit(training_prepared, training_labels, epochs = 5) #Extract predicted probabilities on the test set y_pred = NN_final.predict(test_prepared)  Train on 595212 samples Epoch 1/5 595212/595212 [==============================] - 91s 154us/sample - loss: 0.1541 - accuracy: 0.9636 Epoch 2/5 595212/595212 [==============================] - 89s 150us/sample - loss: 0.1537 - accuracy: 0.9636 Epoch 3/5 595212/595212 [==============================] - 93s 156us/sample - loss: 0.1536 - accuracy: 0.9636 Epoch 4/5 595212/595212 [==============================] - 90s 152us/sample - loss: 0.1535 - accuracy: 0.9636 Epoch 5/5 595212/595212 [==============================] - 90s 151us/sample - loss: 0.1534 - accuracy: 0.9636  We now calculate the AUC score for the final model, however this time we train each fold on 5 epochs to ensure convergence to the optimal weights at each layer. We then average the AUC score for each fold to estimate the overall AUC score.\ndef Neural_Net_Final_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 5): #empty array to store predictions NN_pred = np.zeros(len(training_labels)) for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared): #split data into training and test set x_train, x_test=training_prepared[train_index], training_prepared[test_index] #split corresponding test and training labels y_train, y_test=training_labels[train_index], training_labels[test_index] #Fit model on training set Neural_Net.fit(x_train, y_train,epochs= epochs) #Make predictions on test test NN_pred = Neural_Net.predict(x_test) #Calculate AUC score for the kth fold print(\u0026quot;AUC cross-validation score: \u0026quot;, round(roc_auc_score(y_test, NN_pred), 5))  Neural_Net_Final_CrossVal(NN_final, k = 3)  Train on 396808 samples Epoch 1/5 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637 Epoch 2/5 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637 Epoch 3/5 396808/396808 [==============================] - 59s 149us/sample - loss: 0.1529 - accuracy: 0.9637 Epoch 4/5 396808/396808 [==============================] - 58s 146us/sample - loss: 0.1529 - accuracy: 0.9637 Epoch 5/5 396808/396808 [==============================] - 60s 151us/sample - loss: 0.1529 - accuracy: 0.9637 AUC cross-validation score: 0.62041 Train on 396808 samples Epoch 1/5 396808/396808 [==============================] - 60s 150us/sample - loss: 0.1538 - accuracy: 0.9634 Epoch 2/5 396808/396808 [==============================] - 58s 147us/sample - loss: 0.1537 - accuracy: 0.9634 Epoch 3/5 396808/396808 [==============================] - 60s 152us/sample - loss: 0.1537 - accuracy: 0.9634 Epoch 4/5 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634 Epoch 5/5 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634 AUC cross-validation score: 0.6219 Train on 396808 samples Epoch 1/5 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1531 - accuracy: 0.9636 Epoch 2/5 396808/396808 [==============================] - 62s 156us/sample - loss: 0.1530 - accuracy: 0.9636 Epoch 3/5 396808/396808 [==============================] - 61s 153us/sample - loss: 0.1530 - accuracy: 0.9636 Epoch 4/5 396808/396808 [==============================] - 63s 159us/sample - loss: 0.1530 - accuracy: 0.9636 Epoch 5/5 396808/396808 [==============================] - 60s 151us/sample - loss: 0.1530 - accuracy: 0.9636 AUC cross-validation score: 0.62102  print(\u0026quot;Average AUC score\u0026quot;, round(((0.62041 + 0.62102 + 0.6219) / 3), 4))  Average AUC score 0.6211  Whilst the AUC score provides a formal metric to assess the performance of each model, we may also consider plotting the ROC curve for the fitted Neural Network model and compare it to the ROC curves under Logistic Regression and the Random Forest classifier. Unfortunately, there is no easy way to extract all predictions from the Cross-Validation carried out, so we instead split the training data randomly into a training and validation set purely for the purpose of plotting a ROC curve to roughly illustrate the differences in performance between the different models. To do this, we randomly shuffle the indices of the data and split the test and training set into 80% training set and 20% validation set. We then approximate the true ROC curve using predictions solely on the validation set.\nimport random #Set random seed for reproducibility random.seed(160001695) #shuffle indices of full training set shuffled_indices = np.random.permutation(len(training_prepared)) #validation set is 20% of full training data validation_set_size = int(len(training_prepared) * 0.2) #extract the validation indices validation_indices = shuffled_indices[:validation_set_size] #extract the training indices train_indices = shuffled_indices[validation_set_size:] #create training set train_set = training_prepared[train_indices] #create validation set validation_set = training_prepared[validation_indices]  #Train on the training data and predict for the validation data NN_fit_ROC = NN_final.fit(train_set, training_labels[train_indices], epochs = 5) NN_pred_ROC = NN_final.predict(validation_set)  Train on 476170 samples Epoch 1/5 476170/476170 [==============================] - 83s 175us/sample - loss: 0.1533 - accuracy: 0.9635 Epoch 2/5 476170/476170 [==============================] - 80s 167us/sample - loss: 0.1532 - accuracy: 0.9635 Epoch 3/5 476170/476170 [==============================] - 70s 146us/sample - loss: 0.1532 - accuracy: 0.9635 Epoch 4/5 476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635 Epoch 5/5 476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635  #Extract the false positive rate, true positive rate and thresholds for the validation set predictions fpr_NN, tpr_NN, thresholds_NN = roc_curve(training_labels[validation_indices], NN_pred_ROC[:, 0])  #Plot the ROC curve for all three different models assessed plt.plot(fpr, tpr, \u0026quot;b:\u0026quot;, label = \u0026quot;Logistic Regression\u0026quot;) plot_roc_curve(fpr_rf, tpr_rf, \u0026quot;Random Forest\u0026quot;) plot_roc_curve(fpr_NN, tpr_NN, \u0026quot;Tuned Neural Network\u0026quot;) plt.legend(loc = \u0026quot;lower right\u0026quot;) plt.savefig(\u0026quot;roc_curve.png\u0026quot;) plt.show()     The above ROC curve illustrates that the tuned Neural Network model tends to outperform the other fitted models for the majority of threshold values, when estimating its probabilities on the randomly sampled validation set. Despite having a slightly lower (but almost identical) AUC score of 0.6211 to that of the Logistic Regression model, the ROC curve seems to slightly favour the Tuned Neural Network model and hence we use this model as the final model to make predictions on the test data for upload to Kaggle (Also increasing the number of epochs to 20 for example would have likely raised the AUC Score for this model).\n[Note- both the AUC score and ROC Curves are dependent on how the data is split for each fold in the cross-validation process and this splitting is done randomly. Therefore, it is not contradictory that the ROC curve for the Tuned Neural Network is generally higher than that of the Logistic Regression model, despite the fact that the Logistic Regression model had a larger AUC score. This is because the AUC was calculated on a different set of sampled cross-validations. Essentially we have two seemingly equally performing models and in this case we choose the Neural Network. It may or may not generalize as well to the actual test data]\nWe now use the following code to upload the best model (Neural network with 2 hidden layers, 10 nodes per hidden layer and a learning rate of 1e-2) to kaggle:\n#use fitted model to predict for test data target_NN = NN_final.predict(test_prepared)  #extract probalities for 'claim' target_NN = target_NN[:, 0]  #Extract identity column identity = test_data[\u0026quot;id\u0026quot;] #Prepare the data into 2 columns for submission submission = pd.DataFrame({'id': identity, 'target': target_NN}) filename = 'Porto Seguro Predictions.csv' #Convert file to csv for Kaggle submission submission.to_csv(filename,index=False)  9) Conclusion In this project, we have assessed the performance of three models that could be used for binary classification for the Porto Seguro automotive insurance data. The three assessed models are: Logistic Regression, Random Forest Classifier and a Multi-Layer Perceptron Neural Network. The ROC curve and AUC score were used as measures of performance, as these take into account the false negative rate that could be more important for economic reasons to this insurance company. Cross-validation was then used to assess the performance of the different models, allowing an unbiased calculation of the AUC score by training a model and validating on a previously unseen kth fold of the data. The model that was chosen in the end was a Neural Network Multi-Layer Perceptron with two hidden layers, ten neurons per layer and a learning rate of 1e-2. Its AUC score was 0.6211 using three folds and five training epochs.\nThere are further methods that could also be considered beyond the analysis presented here to improve the performance of these models. Primarily, the number of features could be reduced and this may improve the tuning of neural networks as they may suffer less from overfitting, hence more layers could be added to create a model that acts on the reduced set of features which contain the most information. Principal component analysis could also be used to condense some of the numerical and categorical variables into new features containing most of the information (variation) from the original features, which would again slow the onset of overfitting and lead to potentially more powerful neural networks.\nReferences Geron, A. (2019). \u0026lsquo;Hands-On Machine Learning with Scikit-Learn, Keras \u0026amp; Tensorflow\u0026rsquo;, O\u0026rsquo;Reilly Media.\nSeverance, C. (2016). \u0026lsquo;Python for Everybody: Exploring Data in Python 3\u0026rsquo;, CreateSpace Independent Publishing Platform.\nPython Software Foundation (2016). Python Language Reference, version 2.3. URL: http://www.python.org\nRStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA. URL: http://www.rstudio.com/.\nKaggle.com (2017). Porto Seguro\u0026rsquo;s Safe Driver Prediction Data. URL: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data\nKaggle.com (2017). Porto Seguro\u0026rsquo;s Safe Driver Prediction- Welcome. URL: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222\n","date":1591028883,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591028883,"objectID":"86268d884501a98db00a5dbc5de0d02e","permalink":"https://domscruton.github.io/project/carinsurance/","publishdate":"2020-06-01T17:28:03+01:00","relpermalink":"/project/carinsurance/","section":"project","summary":"1) Introduction- Framing the Problem The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year.","tags":["Other","Supervised Learning","Python","SKLearn","Keras","Tensorflow"],"title":"Predicting Car Insurance Claims","type":"project"},{"authors":null,"categories":null,"content":"\rAbstract\nThis report utilizes Principal Component and Cluster Analysis to assess differences between the performance of 5872 songs from the 2000's on the basis of 13 numeric variables. Firstly, the data contains a large amount of independent information, with the first three principal components consisting of just 54.7% of the total variation in the data. The 'loudness' of a song had the highest absolute loading on the first principal component axis, suggesting it was the most important variable in explaining the variance between observations. Cluster analysis was then used to group the songs, and eight clusters were selected using K-Means. In particular, the performance of a song greatly varied between clusters. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. The best-performing clusters had higher average danceability scores, while the worst performing clusters had very high levels of instrumentalness. This suggests that songs that are more suitable for dancing and contain higher levels of vocalness (less instrumental) are more likely to be 'hits', on average. From the conclusions of the analysis, several hypotheses were generated regarding differences in the performance and composition of songs contained within different clusters of the data and further methods for analysis, such as a Principal Component Logistic Regression were discussed.\n1 Introduction\rThis report considers Principal Component and Cluster analyses to identify which variables provide the largest amount of independent information and to understand the composition of songs that are more successful. The data consists of 5872 songs from the 2000's and 16 variables, 12 of which are continuous, three are categorical and one is binary. Seven of the continuous variables are 'engineered' metrics that relate to the constitution of each song. For example, 'energy' \u0026quot;represents a perceptual measure of intensity and activity\u0026quot; and 'danceability' \u0026quot;describes how suitable a track is for dancing\u0026quot; (Ansari, 2020). The binary variable, called 'target', indicates whether the song was a 'hit' or a 'flop'. A 'hit' is defined as a song that has featured in the weekly list (issued by Billboards) of Hot-100 tracks in that decade at least once. Exactly half of the songs are labeled as 'hits' with the other half labeled as 'flops'. This analysis would enable artists to test whether their song is more likely to be a ‚Äòhit‚Äô or a ‚Äòflop' based on the characteristics of the song and alter its chances of success depending on which cluster or partition of the data the song is located.\n\r2 Exploratory Data Analysis\rExploring the Data before carrying out a more rigorous analysis is important as it will enable us to assess the distributional properties of the variables and identify some of the individual patterns and relationships (Everitt, 2005). Furthermore, it can be used to consider the most appropriate methods of analysis for complex datasets that contain a range of variables whose inter-relationships are currently based on little theoretical background. We restrict the analysis of the data to the twelve continuous features that will be used for Principal Component and Cluster analyses.\n2.1 Assessing the Distribution of Variables\rHistograms are used to assess the general distribution of the variables and identify any outlying observations (Appendix A). In particular, we identify that some of the variables are expressed on widely differing scales: song duration is measured in milliseconds and ranges from 15920 to 4170227, whilst the engineered variables, such as 'danceability' range from just 0 to 1. In order for Principal Component and Cluster Analyses to work, the data therefore need to be scaled. This standardization may be adversely affected by strongly outlying observations. The histograms in Appendix A identify that the variables 'duration_ms' and 'sections', corresponding to the duration and number of sections of each song have a few strongly outlying observations. The 'Plot of Outliers' (Appendix A) identifies three clearly outlying observations and it is hard to fathom that they have come from the same underlying distribution as the rest of the data and may have been incorrectly imputed. Therefore, these observations are dropped from the analysis.\nSome of the univariate distributions for the variables are quite skewed and this may affect the quality of the Principal Component Analysis. Scaling the data, by subtracting the mean and dividing by the standard deviation will partly alleviate this issue.\n\r2.2 Exploring the Relationships Between Variables\rPrincipal Component Analysis (PCA) assumes that the covariance or correlation matrix adequately describes the relationship amongst the variables, that is the relationships are linear (Kutner et al., 2013). The matrix of scatterplots in Appendix A indicates that most variables appear to have roughly linear relationships, although there is often a large amount of scatter and 'noise'. Therefore, using the correlation matrix to describe the strength of relationships between the variables is justified and hence the conclusions of the PCA will be valid.\nHowever, whilst PCA nor Cluster analysis require multivariate normal data, they tend to work better on data that is multivariate normal (and spherical) (Everitt, 2005). In this case there appears to be some deviation from normality, especially at the tails of the distribution (Appendix A). Whilst this should not affect qualitative conclusions, it may impact on the exact specification of the principal components.\nThe complexity of the Spotify data can be illustrated by plotting a scatterplot of 'danceability' against 'acousticness', but stratified by 'energy'. For low energy, the data tend to have high levels of acousticness and lower danceability scores, whilst for high energy, the levels of acousticness fall but danceability rises. Therefore, the relationship between 'danceability' and 'acousticness' depends on the energy levels of a song. Untangling the relationships and information within the data therefore justifies the use of more rigorous multivariate analytic techniques, such as PCA and Cluster Analysis.\n\r2.3 Correlation Among the Variables\rFor PCA to be worthwhile, there needs to be some correlation between variables in the dataset (Everitt, 2005). The correlation plot below demonstrates that some variables are highly correlated, such as 'section' and 'duration', whilst others show no association. Table 1 shows the variables that have the highest correlation with song performance (target). This indicates that songs that are more danceable and louder tend to perform better, however, they need not be the variables that explain the most variation in the data as assessed by Principal Component Analysis.\nTable 1- Correlation between the Target ('Hit' or 'Flop') for each explanatory Variable\n\r\rVariable\rCorrelation with Target\r\r\r\rDanceability\r0.4585\r\rInstrumentalness\r-0.4713\r\rLoudness\r0.3473\r\r\r\r\r\r3 Methodology\r3.1 Principal Component Analysis\rThe Spotify dataset we have chosen has great potential for dimensionality reduction via Principal Component Analysis (PCA) as there is clear collinearity and correlation between some variables (see the correlation plot), suggesting that they share similar information (Zelterman, 2015). PCA was developed for continuous data, therefore the categorical variables ('key', 'mode', 'target' and 'time_signature') are dropped.\n3.1.1 The Theory of Principal Component Analysis\rConsider a multivariate random vector, \\(\\vec{x} = (x_1, ..., x_p)\\) with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Consider p different linear combinations of the the random vector \\(\\vec{x}\\):\n\\[y_i = \\vec{\\alpha_i}^T \\vec{x}\\]\nfor \\(i = 1,...,p\\), where \\(p\\) is the number of variables in the dataset and also corresponds to the total number of principal component axes. We would like to place each random vector of the data onto a new axis, where each axis explains as much variation as possible. \\(y_i\\) are the projections of the random variable, \\(\\vec{x}\\) onto each of these axes. The \\(\\alpha\\)'s are the loadings and they explain how much each variable, \\(i\\), contributes to each of the principal component axes (\\(x_i\\) is the value of the random vector for the \\(i\\)th variable). For each \\(i = 1,...,p\\), the variances of \\(y_i\\) can be expressed as:\n\\[Var(y_i) = \\vec{\\alpha}_i^T \\Sigma \\vec{\\alpha}_i\\]\nIn particular, we want to find the loadings (\\(\\alpha\\)'s) that maximize the variance along each axis (Everitt, 2005). However, one could make these variances arbitrarily large by multiply the loadings by an arbitrarily large scalar, so we restrict the loadings to have length 1. The second restriction is that the new axes, \\(y_i\\) should be mutually uncorrelated:\n\\[Cov(y_i, y_j) = \\vec{\\alpha}_i^T \\Sigma \\vec{\\alpha}_j = 0\\]\nThe solution to this constrained maximization problem can be solved via the use of a Lagrange Multiplier. The principal components are then given by the eigenvectors of the correlation (or covariance) matrix and the eigenvalues are the variances of each principal component. The principal components are mutually orthogonal (uncorrelated) and decrease in variance.\nHowever, the scaling of the variables may have a significant impact on the results of a Principal Component Analysis. If the variables are measured on different scales, we should use the correlation matrix in the PCA, which is equivalent to scaling the data by dividing each data point by the standard deviation of that variable. In this case, the 'engineered' variables, such as 'liveness' and 'speechiness' lie on a normalized scale between 0 and 1, meanwhile tempo is measured in Beats Per Minute (BPM) and loudness is measured in decibels. Furthermore, if we carry out the analysis without scaling the variables, nearly 100% of the total variation is explained by just one principal component, which consists almost entirely of the 'duration' variable (Appendix B). This is because song duration is recorded in milliseconds, with a range of 5,920 to 4,170,227 milliseconds. Clearly scaling is required for any meaningful interpretation to take place.\n\r3.1.2 Choosing an Appropriate Number of Principal Components to Retain\rIn order to reduce the dimensionality of the problem, we need to reduce the number of principal components, whilst ensuring information loss is minimized. These reduced components could then be used in a logistic regression in order to make predictions regarding the popularity of a song based on its characteristics. The two methods considered are Kaiser's criteria and the Scree plot. Under Kaiser's Criterion, we keep only those Principal components that have eigenvalues (variances) larger than the average. Since the PCA is calculated on the correlation matrix, this average will always be one. However, this cut-off point tends to retain too few components when the number of variables is small (Everitt, 2005). Another scheme for analyzing how many components to keep is the Scree Plot. This plots the variance of each principal component and the idea is to stop retaining components after the largest significant drop in the variance, thus removing the 'scree' from the analysis.\n\r3.1.2 Analysing the Principal Components and Interpreting the Loadings\rThe principal component loadings represent the relative importance of each of the original variables in determining the direction of the new principal component axes (Zelterman, 2015). The scores of the principal components are the orthogonal projections of each data point onto the principal component axes and represent the new space created under PCA. Mardia's Criterion can be used to select those variables that have significant influence on the positions of each principal component. Under this \u0026quot;rule of thumb\u0026quot;, a variable is said to have a high influence if the value of its loading on the principal component axis is larger than \\(\\frac{1}{\\sqrt{p}}\\), where \\(p\\) is the number of variables used in the PCA. The variables with high loadings on the most important axes are the dominant variables that contribute most to the variation in the dataset (Zelterman, 2015).\n\r\r3.2 Cluster Analysis\rClustering the data will allow homogeneous subgroups of songs that display similar attributes to be identified. This information can then be used to generate hypotheses about differences in the observations between groups, such as whether those songs in groups that performed better, on average, exhibit certain attributes.\n3.2.1 Performing a Cluster Analysis\rThe two general methods in which to perform a cluster analysis are partitioning and hierarchical methods. The K-Means algorithm first assigns the data into K clusters. The means or centroids of the clusters are then calculated. Observations are then iteratively reassigned to the nearest cluster (according to Euclidean distances) and for each iteration the centroid of each cluster will change as different observations become part of the grouping. This process continues until no more observations are reallocated. The data will need to be standardized and outlying observations removed in order to effectively carry out the K-means algorithm appropriately (Everitt, 2005). To deal with the sensitivity of the clusters under different starting location, the K-means approach can be first applied to a subset of the data to generate sensible initial centroids and then those centroids used as the starting point for the algorithm to be applied on the whole dataset.\nHierarchical methods assume that groupings in the data cloud have a hierarchical structure, where smaller groups are nested in larger groups. However, such methods are difficult to justify for real datasets. For complex structures, partitioning the data using K-Means is a better approach (Zelterman, 2015). Non-hierarchical methods, such as K-means are also more appropriate if the data consist of mainly continuous or ordered variables. In this case, all of the variables used in the analysis are continuous. Therefore, K-means is used to partition the data cloud into songs with similar characteristics, as opposed to identifying natural clusters under a hierarchical method. In this case, applying Ward's method with 8 clusters provides similar qualitative results regarding cluster location (Appendix C).\n\r3.2.2 Selecting the Number of Clusters\rThe 'elbow' method can be used to select an appropriate number of clusters for the K-means algorithm (Geron, 2019). The within group sum of squares is plotted against the number of clusters. Naturally, as the number of clusters increases, the within group sum of squares will decline, as observations in the cluster become 'closer', on average. In a similar manner to the scree plot, the number of clusters is chosen at the 'elbow', where the last significant drop in the within group sum of squares occurs (Everitt, 2005).\n\r\r\r4 Results\r4.1 Principal Component Analysis\rThe results of the Principal Component analysis demonstrate that variables in the dataset contain a fairly large amount of independent information. In particular, the first three principal components contain only 54.7% of the total variation, whilst to retain 90% of the total variation in the data, we would need to keep the first eight principal components.\nTable 2- Standard Deviation and Cumulative Variance of the Principal Components\n\r\r\rPrincipal Component:\rPC1\rPC2\rPC3\rPC4\rPC5\rPC6\rPC7\rPC8\rPC9\rPC10\rPC11\rPC12\r\r\r\rStandard Deviation\r1.801\r1.314\r1.263\r1.055\r1.015\r0.924\r0.885\r0.841\r0.646\r0.530\r0.380\r0.323\r\rCumulative Proportion\r0.270\r0.414\r0.547\r0.640\r0.726\r0.797\r0.862\r0.921\r0.956\r0.979\r0.991\r1.000\r\r\r\rThe Scree plot suggests the use of just two principal components, however, there is a large amount of scree (remaining variation) left over. Under Kaiser's Criterion, the first five components have standard deviations greater than one and hence we would choose to retain these. Yet, the appropriate number of components to keep is context-specific. If the principal components were to be used as variables in a PC logistic regression, we may want to retain more components, to reduce information loss. However, the purpose of this report is to visualize and explain the most important components and variables, hence only the first two or three principal components are required for further analysis.\nLarge Loadings for a Principal Component (PC) axis indicate that a variable is important in explaining variation along that axis. The main loadings on the first two PC axes as assessed by Mardia's Criterion are shown in Table 3. The loudness of a song influences the direction of the first axis the most. Songs that are louder will on average have lower scores on this first axis. Similarly, energy and acousticness also have high influence. Therefore the first PC identifies a trend in the characteristics of songs; songs that have high levels of acousticness tend to have low energy and are less loud. Furthermore, given that the first PC contains the highest information of all of the components, this suggests that 'loudness', 'acousticness' and 'energy' provide more information regarding variation between songs than the other variables.\nTable 3- Loadings of each Variable for the Two Main Principal Component Axes\n\r\rVariable\rLoading on First PC Axis\r\r\r\rLoudness\r-0.4761\r\rEnergy\r-0.4423\r\rAcousticness\r0.4194\r\r\r\r\r\rVariable\rLoading on Second PC Axis\r\r\r\rSections\r0.6664\r\rDuration / ms\r0.6541\r\r\r\rIt appears that the first principal component is partly splitting the data between songs that were 'hits' and those that were 'flops'. As discussed, high values of this principle component are associated with high acousticness and low energy/loudness. Therefore it appears that songs with high acousticness perform worse, in general. These poorly performing songs also generally have larger number of sections.\n\r4.2 Cluster Analysis\rThe K-means clustering algorithm is applied to the standardized data, in order to partition the data cloud by songs with similar characteristics. The plot of the within sum of squares for different numbers of clusters suggests that eight clusters should suffice in the analysis (at the 'elbow' of the curve).\nAs this dataset is relatively large, a subsample of the songs is initially clustered and then the estimated cluster centroids are used as seeds for analysis on the full dataset. Further, the clusters can be depicted on the principal component axes. These plots show that the first and second PC axes partly split the clusters. For example, clusters 6 and 7 are well separated from most other clusters, with large values on the first principal component. Since the first principal component has a large positive loading for acousticness and negative loadings for energy and loudness, this suggests that songs in these clusters are similarly characterized by higher levels of acousticness and lower levels of energy and loudness. This is consistent with the results in Table 4, where clusters 6 and 7 have large (scaled) acousticness values of 2.26 and 1.76, respectively.\nTable 4- Centroids of Each Cluster\n\r\rCluster\r1\r2\r3\r4\r5\r6\r7\r8\r\r\r\rDanceability\r-0.564\r-0.415\r0.805\r0.748\r-0.511\r-1.229\r-0.874\r0.055\r\rEnergy\r0.608\r0.629\r0.077\r0.283\r0.412\r-2.230\r-1.312\r-0.972\r\rLoudness\r0.456\r0.378\r0.229\r0.361\r-0.024\r-2.737\r-1.505\r-0.283\r\rSpeechiness\r-0.162\r0.124\r2.308\r-0.298\r-0.193\r-0.442\r0.344\r-0.512\r\rAcousticness\r-0.547\r-0.375\r-0.062\r-0.337\r-0.449\r2.263\r1.762\r0.817\r\rInstrumentalness\r-0.412\r-0.276\r-0.470\r-0.431\r2.008\r1.644\r0.898\r-0.388\r\rLiveness\r-0.163\r2.437\r0.041\r-0.236\r-0.109\r-0.348\r-0.065\r-0.336\r\rValence\r0.273\r-0.224\r0.642\r0.794\r-0.559\r-1.206\r-0.619\r-0.468\r\rTempo\r0.903\r-0.066\r-0.296\r-0.363\r0.146\r-0.568\r-0.273\r-0.198\r\rduration_ms\r-0.145\r-0.107\r-0.105\r-0.199\r0.302\r-0.028\r6.044\r0.031\r\rchorus_hit\r-0.155\r0.410\r-0.082\r-0.084\r0.261\r0.196\r0.224\r-0.061\r\rsections\r-0.101\r-0.232\r-0.034\r-0.172\r0.121\r0.008\r5.755\r0.062\r\r\r\rIn order to assess which variables play the largest role in separating out the clusters, a separate one-way ANOVA can be performed to test for a difference in the mean of each variable for each cluster. By ranking the F-values, we can identify which variables are most effective at separating the groups. Table 5 shows the three variables with the largest F-values. This suggests energy, acousticness and then loudness are the variables that are most responsible for the differences between the clusters. This is consistent with the conclusion of the Principal Component Analysis, where these three variables had the highest loadings on the first principal component.\nTable 5- F-Values for Variables across the Clusters\n\r\rVariable\rF-Value\r\r\r\rEnergy\r3345.34\r\rAcousticness\r2371.59\r\rLoudness\r1346.10\r\r\r\rThe number of songs in each cluster reveals that the seventh cluster contains relatively few songs. In particular, this cluster is characterized by long song duration and a large number of sections in each song (Table 4) and indicates songs with quite different characteristics to those in the rest of the dataset.\nSome of the clusters of songs have far higher proportions of 'hits' than other clusters. Songs in clusters 3 and 4 display a 77% and 73% chance of being hits, respectively (Table 6). These clusters have higher average danceability scores than the other clusters (Table 4). Hence, songs that are better for dancing are more likely to be popular. This is consistent with the results from Table 1, where it was shown that danceability was positively correlated with song popularity.\nTable 6- Percentage of Songs in each Cluster that are Hits\n\r\rCluster\r1\r2\r3\r4\r5\r6\r7\r8\r\r\r\rPercentage of Hits (%)\r47.74\r37.76\r77.02\r73.88\r2.48\r0.73\r1.41\r56.14\r\r\r\r\r\r5 Discussion\rPrior research has concluded that successful songs tend to be \u0026quot;‚Äòhappier‚Äô, more ‚Äòparty-like‚Äô, less ‚Äòrelaxed‚Äô and more ‚Äòfemale‚Äô than most\u0026quot; (Interiano et al., 2018, p.1). The evidence presented here is fairly consistent with these results, as songs with higher danceability (more 'party-like') and more energy (less 'relaxed') tend to perform better. On a more general level, this information could be used to inform music artists about how to increase the likelihood that a song will be a 'hit' by altering the characteristics of that song to be similar to songs in clusters with a larger proportion of 'hits'.\nOne can also see that between clusters there is a great deal of heterogeneity. The third cluster is the only cluster to have a large value for 'speechiness', in particular containing songs with a large amount of spoken words, possibly of the rap genre of music (Table 4). Further, the results of the Exploratory, Principal Component and Cluster Analyses are consistent. Energy, instrumentalness and loudness are the three variables that play the largest role in separating out the clusters (Table 3). These three variables also have the highest loadings on the first principal component, suggesting that they explain more of the variation in the data than the other variables. Furthermore, clusters of songs with high danceability and low instrumentalness were associated with greater popularity. This is consistent with the results in Table 1.\nCluster analysis is a useful tool for generating hypotheses. For example, consider clusters 2 and 5. The centroids of these clusters are fairly similar, with roughly equal levels of danceability. The distinguishing difference between songs in each cluster are the levels of instrumentalness and liveness (Table 4). One could then formulate and test the hypothesis that the difference in the performance of songs between these two clusters is driven by differences in instrumentalness. If this is true, this suggests that an artist could increase the popularity of their song if it has similar characteristics to songs in cluster 5 by reducing the level of instrumentalness (i.e. by adding more vocal elements to their song).\nWe may also consider different hypotheses about the performance of songs contained within a given cluster. For example, are there differences in the characteristics of songs in cluster 1 that are 'hits' and 'flops' and which variables are the most important in explaining the within cluster popularity? This would enable an artist within a given genre (songs that have certain characteristics) to alter their music to increase the likelihood that the songs would be successful. The characteristics that increase song success may also differ between clusters.\nIn order to make predictions regarding the popularity of a new song based on its attributes, a Principal Component Logistic Regression model which utilizes the principal components as regressors could be fitted. This is useful in overcoming issues of collinearity between variables, by excluding some of the low variance principal components from the model. Hence, a new song based on the discussed variables that has maximal probability of being a hit could be created.\n\r6 Conclusion\rThis report has uncovered several insights through the use of Principal Component and Cluster Analyses, some of which were not obvious when first exploring the data. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. Clusters of songs with higher rates of success were associated with higher levels of danceability and lower instrumentalness (i.e. more vocal elements). Energy, acousticness and loudness were the variables that had the greatest influence on separating out the clusters and these variables also had the highest absolute loadings on the first principal component axes. From these conclusions, several hypotheses concerning differences in the characteristics and performance of songs across different clusters and within the same cluster were generated. These hypotheses included \u0026quot;Is the difference in the performance between songs in clusters 2 and 5 driven by differences in instrumentalness?\u0026quot; or \u0026quot;Do songs in cluster 1 that are 'hits' exhibit different characteristics to those songs that are 'flops'?\u0026quot; and suggest areas for future research.\n\rReferences\rBrian S. Everitt. 2005. \u0026quot;An R and S-Plus Companion to Multivariate Analysis\u0026quot;. Springer.\nDaniel Zelterman. 2015. \u0026quot;Applied Multivariate Statistics with R\u0026quot;. Springer.\nAurelien Geron. 2019. \u0026quot;Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow\u0026quot;. O'Reilly Media. pp 213-274.\nMichael H. Kutner, Christopher J. Nachtsheim, John Neter, William Li. 2013. \u0026quot;Applied Linear Statistical Models.\u0026quot; McGraw Hill Education.\nGarrett Grolemund, Hadley Wickham. 2017. \u0026quot;R for Data Science.\u0026quot; O'Reilly Media. doi: https://r4ds.had.co.nz/.\nMyra Interiano, Kamyar Kazemi, Lijia Wang, Jienian Yang, Zhaoxia Yu and Natalia L. Komarova. 2018. \u0026quot;Musical Trends and Predictability of Success in Contemporary Songs in and out of the Top Charts.\u0026quot; Royal Society Open Science. doi: https://royalsocietypublishing.org/doi/10.1098/rsos.171274.\nEric A. Strobl, Clive Tucker. 2000. \u0026quot;The Dynamics of Chart Success in the U.K. Pre-Recorded Popular Music Industry.\u0026quot; Journal of Cultural Economics 24. doi: https://doi.org/10.1023/A:1007601402245.\nFarooq Ansari. 2020. Spotify Hit Predictor Dataset. kaggle.com doi: https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?fbclid=IwAR1kE9neO0sdKb3pv6g-Z-SOvPXii9Ubqx0PTRIDkZYdqaBGEhtLGTrFkLA.\nRStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL http://www.rstudio.com/.\n\rAppendix\rA) Exploratory Data Analysis\rHistograms Histograms for each continuous variable in the dataset.\n#histograms for each variable to assess the distribution and look for outliers.\rpar(mfrow = c(1, 1))\rfor (i in numeric_variables) {\rhist(spotify_data[,i], main = c(\u0026quot;Histogram of \u0026quot;, i), xlab = i, breaks = 20)\r}\rprint(\u0026#39;Range of Song Duration / milliseconds:\u0026#39;)\r## [1] \u0026quot;Range of Song Duration / milliseconds:\u0026quot;\rprint(c(range(spotify_data$duration_ms)))\r## [1] 15920 4170227\rPlot of Outliers Scatterplot of Song Duration against the number of Sections in a song, to identify outlying observations.\n#Analysis of outliers in the dataset\r#List of variables with clear outliers\routliers \u0026lt;- c(\u0026quot;duration_ms\u0026quot;, \u0026quot;sections\u0026quot;)\rplot(spotify_data[, outliers], main = \u0026quot;Plot of Outliers\u0026quot;, xlab = \u0026quot;Song Duration\u0026quot;, ylab = \u0026quot;Sections\u0026quot;)\rMatrix of Scatterplots Pairwise scatterplots for each continuous variable.\npairs(spotify_no_outliers[numeric_variables][1:200,])\rTesting for Multivariate Normality A simple test of multivariate normality is conducted by plotting the (ordered) squared Mahalanobis Distance against the corresponding quantiles of a \\(\\chi^2\\)-distribution with degrees of freedom equal to the number of variables in the dataset. If the transformed data lie along a straight line then the data are multivariate normally distributed.\n\rB) Principal Component Analysis Without Scaling\rThis appendix contains the results of a Principal Component Analysis on the unscaled numerical data. In particular, the first principal contains almost all of the variation in the data and this is provided by the 'duration' variable.\n#PCA Without Scaling\rpca.spotify.unscaled \u0026lt;- prcomp(spotify_numeric)\rsummary(pca.spotify.unscaled)\r## Importance of components:\r## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8\r## Standard deviation 112174 30.23 20.11 4.966 2.136 0.2979 0.2365 0.2245\r## Proportion of Variance 1 0.00 0.00 0.000 0.000 0.0000 0.0000 0.0000\r## Cumulative Proportion 1 1.00 1.00 1.000 1.000 1.0000 1.0000 1.0000\r## PC9 PC10 PC11 PC12\r## Standard deviation 0.1688 0.1381 0.1034 0.08701\r## Proportion of Variance 0.0000 0.0000 0.0000 0.00000\r## Cumulative Proportion 1.0000 1.0000 1.0000 1.00000\rpca.spotify.unscaled$rotation\r## PC1 PC2 PC3 PC4\r## danceability -1.302208e-07 -8.650312e-04 -8.958769e-04 -1.181289e-02\r## energy -3.102277e-07 1.729092e-03 2.404089e-04 -3.530510e-02\r## loudness -7.337131e-06 2.773744e-02 -8.162803e-03 -9.964386e-01\r## speechiness 1.318074e-08 -5.034456e-05 -7.637436e-05 -2.022660e-03\r## acousticness 3.824303e-07 -1.529234e-03 -2.182361e-04 3.912435e-02\r## instrumentalness 4.572867e-07 -3.172999e-04 1.134257e-03 2.521589e-02\r## liveness -3.717663e-08 2.124559e-04 2.317251e-04 -3.759318e-03\r## valence -3.518179e-07 1.845762e-04 -8.102301e-04 -1.605659e-02\r## tempo -8.319169e-06 9.965844e-01 7.776454e-02 2.675845e-02\r## duration_ms 1.000000e+00 9.430128e-06 -1.429315e-05 -8.819774e-06\r## chorus_hit 1.652100e-05 -7.717809e-02 9.959979e-01 -8.118795e-03\r## sections 3.641884e-05 9.347518e-03 -4.325897e-02 4.999516e-02\r## PC5 PC6 PC7 PC8\r## danceability -4.964583e-03 3.733586e-01 3.111789e-01 -1.021434e-01\r## energy 3.988151e-03 -1.320574e-01 2.489982e-01 3.197516e-01\r## loudness -4.925853e-02 -2.149171e-02 -2.318400e-02 -5.028992e-02\r## speechiness -1.256685e-03 4.660923e-02 3.200967e-02 2.167349e-02\r## acousticness -4.364288e-03 1.514271e-01 -3.634591e-01 -8.138958e-01\r## instrumentalness 1.295951e-02 -7.139996e-01 5.418293e-01 -4.239904e-01\r## liveness 1.471401e-03 -5.298768e-02 -4.544922e-02 1.110710e-01\r## valence -1.037914e-02 5.519963e-01 6.417573e-01 -1.725608e-01\r## tempo 7.282709e-03 1.279704e-03 -3.406733e-05 -6.739756e-04\r## duration_ms 3.675594e-05 9.349871e-07 -2.608640e-08 7.858695e-09\r## chorus_hit -4.433113e-02 6.574808e-04 -2.330242e-05 -2.036780e-04\r## sections -9.976053e-01 -1.716226e-02 2.436889e-03 4.258439e-03\r## PC9 PC10 PC11 PC12\r## danceability -3.101105e-01 -6.427783e-01 4.055585e-01 -2.817126e-01\r## energy 3.170263e-01 4.086354e-01 6.385141e-01 -3.766503e-01\r## loudness -6.092071e-03 -6.119759e-03 -1.309125e-02 8.556876e-03\r## speechiness 1.120431e-01 -8.222999e-02 4.896812e-01 8.586177e-01\r## acousticness 2.436457e-01 1.597009e-01 2.732911e-01 -1.464008e-01\r## instrumentalness -3.280515e-02 -1.097029e-01 -3.381879e-02 4.240195e-02\r## liveness 8.186357e-01 -5.330207e-01 -1.545167e-01 -6.798993e-02\r## valence 2.452160e-01 3.023282e-01 -2.969051e-01 1.166974e-01\r## tempo -5.212203e-04 -8.387501e-04 1.209447e-04 6.572693e-06\r## duration_ms 4.495584e-08 1.004793e-07 -1.165984e-07 3.722348e-08\r## chorus_hit -2.501794e-04 -1.733233e-04 1.018916e-04 -7.168368e-05\r## sections 1.417663e-04 -8.156361e-04 1.786454e-03 -1.727792e-03\r\rC) Clustering with Hierarchical Methods\rHierarchical clustering using Ward's method performs similarly to K-means on this dataset.\n#Euclidean Distance matrix\rDEuclidean \u0026lt;- dist(spotify_scaled)\r#cluster using Ward\u0026#39;s method\rHClusters \u0026lt;- hclust(DEuclidean, \u0026quot;ward.D2\u0026quot;)\r#Dendrogram\rplot(HClusters, xlab = \u0026quot;Observation Number\u0026quot;)\rPlotting the clusters on the first two principal component axes shows that cluster locations are generally similar to that of the K-means method.\n#Cut observations into 8 clusters\rMyClusters \u0026lt;- cutree(HClusters, 8)\r#copy dataset\rspotify_trial \u0026lt;- spotify_no_outliers\r#extract clusters\rspotify_trial$cluster \u0026lt;- MyClusters\rspotify_trial$cluster \u0026lt;- as.factor(spotify_trial$cluster)\r#plot clusters on PC axes\rggplot(spotify_trial, aes(x = scores_1, y = scores_2, colour = cluster)) + geom_point(shape=1) +\rggtitle(\u0026quot;Hierarchical Clusters on the Principal Component Axes\u0026quot;) + labs(y=\u0026quot;PC2\u0026quot;, x = \u0026quot;PC1\u0026quot;)\r\rD) Code used in the Main Report\r#1. Import Data and Relevant Libraries\r#Import data\rspotify_data \u0026lt;- read.csv(\u0026quot;C:/Users/User/Documents/St Andrews/Multivariate Analysis/Data/spotify dataset.csv\u0026quot;)\r#Remove the track, artist and url for each song (irrelevant to the purposes of this general analysis)\rspotify_data \u0026lt;- spotify_data[, 4:19]\r#Load relevant libraries:\r#three-dimensional data cloud plots\rlibrary(scatterplot3d)\r#multivariate visualization\rlibrary(lattice)\r#visualization of correlation matrix\rlibrary(corrplot)\r#Dendrograms\rlibrary(ape)\r#Choose appropriate number of clusters\rlibrary(NbClust)\r#Analysis of the Composition of different clusters\rlibrary(tidyverse)\rlibrary(dplyr)\rlibrary(ggplot2)\r#2. Dropping of Outliers from the Data and Extraction of Variables used in the PCA\r#Extract continuous variables\rvariables \u0026lt;- sapply(spotify_data, is.numeric)\rnumeric_variables \u0026lt;- vars[vars == TRUE]\r#Drop outlying observations\routliers \u0026lt;- which(spotify_data$duration_ms \u0026gt; 3e6)\rspotify_no_outliers \u0026lt;- spotify_data[-c(outliers), ]\r#Extract numeric variables for later PCA and Cluster Analysis\rspotify_numeric \u0026lt;- spotify_no_outliers[numeric_variables]\r#3. CoPlot to illustrate complex relationships in the data cloud\r#CoPlot for danceability and acousticness, stratified by Energy\rxyplot(spotify_no_outliers$danceability ~ spotify_no_outliers$acousticness |\rcut(spotify_no_outliers$energy, 3), main = \u0026quot;Coplot for Danceability and Acousticness, Cut by Energy\u0026quot;, xlab = \u0026quot;Acousticness\u0026quot;, ylab = \u0026quot;Danceability\u0026quot;)\r#4. Correlation Between Variables\r#correlation matrix\rcorr_matrix \u0026lt;- cor(spotify_no_outliers)\rsort(corr_matrix[, 16], decreasing = TRUE)\r#correlation plot\rcorrplot(corr_matrix, method=\u0026quot;ellipse\u0026quot;)\r#5. Principal Component Analysis\r#Principal Component Analysis with scaling\rpca.spotify \u0026lt;- prcomp(spotify_numeric, scale. = TRUE)\rsummary(pca.spotify)\r#Scree Plot\rplot(pca.spotify$sdev^2, xlab = \u0026quot;Principal Component\u0026quot;, ylab = \u0026quot;variance\u0026quot;, main = \u0026quot;Scree Plot for Spotify Principal Component Analysis (PCA)\u0026quot;)\rlines(pca.spotify$sdev^2)\r#Principal Component loadings (rotations)\rpca.spotify$rotation\r#Mardia\u0026#39;s Criterion to select variables with high loadings on each PC axis\rfor(i in 1:8){\rwhich.pass\u0026lt;-abs(pca.spotify$rotation[,i])\u0026gt;(0.7*max(abs(pca.spotify$rotation[,i])))\rcat(\u0026quot;\\nPC\u0026quot;,i,\u0026quot;\\n\u0026quot;,sep=\u0026quot;\u0026quot;)\rprint(pca.spotify$rotation[which.pass,i])\r}\r#use sample of data to visualize scores\rscores1 \u0026lt;- pca.spotify$x[1:500,1]\rscores2 \u0026lt;- pca.spotify$x[1:500,2]\r#plot scores\rpar(mfrow = c(1,2))\rplot(scores1,scores2,ylim=range(scores2),xlab=\u0026quot;PC1\u0026quot;,ylab=\u0026quot;PC2\u0026quot;, type=\u0026quot;n\u0026quot;,lwd=2, main = \u0026quot;Hit (1) or Flop (0)\u0026quot;)\r#Add target variable in reduced spatial plot\rtext(scores1,scores2,labels=spotify_no_outliers$target[1:500],cex=0.7,lwd=2)\r#add scroes in reduced spatial plot\rplot(scores1,scores2,ylim=range(scores2),xlab=\u0026quot;PC1\u0026quot;,ylab=\u0026quot;PC2\u0026quot;, cex = 0.1 * spotify_no_outliers$sections, main = \u0026quot;Sections\u0026quot;)\r#6. Cluster Analysis\r#Scale data\rspotify_scaled \u0026lt;- scale(spotify_numeric)\rn\u0026lt;-length(spotify_scaled[,1])\r#find within group sum of squares (WSS) for different numbers of clusters\r#WSS for first cluster\rwss1 \u0026lt;- (n - 1) * sum(apply(spotify_scaled, 2, var))\rwss \u0026lt;- numeric(0)\r#calculate WSS for 2 to 20 group partitions given by k-means clustering\rset.seed(160001695)\rfor (i in 2: 20) {\rW \u0026lt;- sum(kmeans(spotify_scaled, i)$withinss)\rwss \u0026lt;-c (wss,W)\r}\rwss\u0026lt;-c(wss1, wss)\r#Plot WSS against each cluster to select number of clusters for K-Means\rplot(1:20, wss, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;Number of groups\u0026quot;, ylab = \u0026quot;within groups sum of squares\u0026quot;, lwd = 2, main = \u0026quot;Selecting the Number of Clusters for K-Means\u0026quot;)\rset.seed(160001695)\r#perform initial cluster analysis on sub-sample of data\rk8_initial \u0026lt;- kmeans(spotify_scaled[1:500, ], 8)\rset.seed(160001695)\r#use initial clusters to start the full clustering\rk8 \u0026lt;- kmeans(spotify_scaled, centers = k8_initial$centers)\rscores_1 \u0026lt;- pca.spotify$x[,1]\rscores_2 \u0026lt;- pca.spotify$x[,2]\r#Add clusters as new variable, so we can test differences and generate hypotheses #between songs in different (or the same) cluster. spotify_no_outliers$cluster \u0026lt;- k8$cluster\rscores_3 \u0026lt;- pca.spotify$x[, 3]\rpar(mfrow = c(1,1))\r#Plot clusters on PC axes\rspotify_no_outliers$cluster \u0026lt;- as.factor(spotify_no_outliers$cluster)\rggplot(spotify_no_outliers, aes(x = scores_1, y = scores_2, colour = cluster)) +\rgeom_point(shape=1) + ggtitle(\u0026quot;Clusters on the Principal Component Axes\u0026quot;) + labs(y=\u0026quot;PC2\u0026quot;, x = \u0026quot;PC1\u0026quot;)\r#Centres of each of the five clusters, by variable\rround(k8$centers, 3)\r#matrix to store F Values\rF.Results \u0026lt;- matrix(NA, nrow = length(variables), ncol = 2)\rF.Results \u0026lt;- as.data.frame(F.Results)\rF.Results$Variables \u0026lt;- variables\r#aov does not enable lists to be used as variables, so we must calculate all of the F-statistics\r#individually\r#F-statistic for assessing difference in mean danceability across clusters- anova of each variable\r#for each cluster\ri \u0026lt;- 0\rfor(num_var in c(numeric_variables)){\ri \u0026lt;- i + 1\rF.Results[i, 2] \u0026lt;- summary(aov(spotify_no_outliers[[num_var]] ~\rspotify_no_outliers$cluster))[[1]][[\u0026quot;F value\u0026quot;]][1]\r}\r#Count number of songs in each cluster\rCluster \u0026lt;- table(spotify_no_outliers$cluster)\rCounts \u0026lt;- data.frame(\rNo_Clusters = factor(c(seq(1, 8)), levels=c(seq(1, 8))),\rSongs = Cluster\r)\r#Plot number of songs in each cluster\rggplot(data = Counts, aes(x = No_Clusters, y = Songs.Freq, fill = No_Clusters)) +\rgeom_bar(colour=\u0026quot;black\u0026quot;, stat=\u0026quot;identity\u0026quot;) + ggtitle(\u0026quot;Number of Songs Contained Within Each Cluster\u0026quot;) + labs(y=\u0026quot;Song Frequency\u0026quot;, x = \u0026quot;Cluster Number\u0026quot;)\r#Percentage of songs that are \u0026#39;hits\u0026#39; in each cluster\r#Number of Clusters\rN_clusters \u0026lt;- c(seq(1, 8))\r#Empty vector of Percentages\rPercentage_target \u0026lt;- c(rep(NA, 8))\rfor (i in N_clusters) {\rx \u0026lt;- filter(spotify_no_outliers, spotify_no_outliers$cluster == i)\rPercentage_target[i] \u0026lt;- (dim(filter(x, x$target == 1))[1] / dim(x)[1]) * 100\r}\r\r\r","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"07ac9ee104b1bdab9cace174d750a76f","permalink":"https://domscruton.github.io/project/spotify/spotify/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/project/spotify/spotify/","section":"project","summary":"Abstract\nThis report utilizes Principal Component and Cluster Analysis to assess differences between the performance of 5872 songs from the 2000's on the basis of 13 numeric variables. Firstly, the data contains a large amount of independent information, with the first three principal components consisting of just 54.","tags":["Other","R","Unsupervised Learning"],"title":"Spotify PCA \u0026 Cluster Analysis","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://domscruton.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Nelson Bighetti","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://domscruton.github.io/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://domscruton.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]