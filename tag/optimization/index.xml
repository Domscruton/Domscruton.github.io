<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimization | Dominic Scruton</title>
    <link>https://domscruton.github.io/tag/optimization/</link>
      <atom:link href="https://domscruton.github.io/tag/optimization/index.xml" rel="self" type="application/rss+xml" />
    <description>Optimization</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Dominic Scruton</copyright><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://domscruton.github.io/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_512x512_fill_lanczos_center_3.png</url>
      <title>Optimization</title>
      <link>https://domscruton.github.io/tag/optimization/</link>
    </image>
    
    <item>
      <title>GLMs: Intuition behind the Link function and Derivation of Iteratively Reweighted Least Squares</title>
      <link>https://domscruton.github.io/post/irls/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://domscruton.github.io/post/irls/</guid>
      <description>
&lt;script src=&#34;https://domscruton.github.io/post/irls/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;Intuition behind the Link function, discussion of the various model fitting techniques and their advantages &amp;amp; disadvantages, derivation of IRLS using Newton-Raphson and Fisher Scoring&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;glms---a-natural-extension-of-the-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GLMs - A Natural Extension of the Linear Model&lt;/h2&gt;
&lt;p&gt;The first model we learn on any statistics-based course is Simple Linear Regression (SLR). Despite placing strong (linear) assumptions on the relationship between the response and covariates, as well as the error distribution if we are interested in statistical inference, the Linear model is a surprisingly useful tool for representing many natural processes. However, when we wish to deal with non-linear random variable generating processes, such as the probability of occurrence of an event from a binary or multinomial distribution, or for the modelling of counts within a given time period, we need to generalize the Linear Model.&lt;/p&gt;
&lt;p&gt;Generalized Linear Models enable us to explicitly model the mean of a distribution from the exponential family, such that predictions lie in a feasible range and the relationship between the mean and the variance is appropriate to perform reliable inference. In this brief blog we discuss the intuition behind the Generalized Linear Model and Link function and prove the method of Iteratively ReWeighted Least Squares enables us to fit a GLM.&lt;/p&gt;
&lt;p&gt;Generalized Linear Models have 3 components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1) Random Component Error Structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i \sim exponential \; family \; distibution\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We also typically assume each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is independent and identically distributed, although this assumption can be relaxed through the use of Generalized Estimating Equations (GEE’s).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Systematic Component/ Linear Predictor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\eta_i = \beta_0 + \sum_{i = 1}^{p}\beta_p x_{i, p}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) Link Function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathop{\mathbb{E}}[y_i | x_{1, i}, ..., x_{p, i}] = \mu_i\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[g(\mu_i) = \eta_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It should be made clear that we are not modellig the response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; explicitly, but rather modelling the mean of the distibution, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;. Predictions for each observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; are given by &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;, with each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; assumed to be centred around &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; in expectation but with an error term that has a distibution specified by the member of the exponential family used. Therefore, the link function does not transform the response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; but instead transforms the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;. Note that the linear model is a specific type of GLM, where &lt;span class=&#34;math inline&#34;&gt;\(y_i \sim Normal\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i) = \mu_i = \eta_i = \beta_0 + \sum_{i = 1}^{p}\beta_p x_{i, p}\)&lt;/span&gt;. For a Poisson GLM, each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is a r.v. simulated from the Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;, hence &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; has a Poisson error distribution, the difference between &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{y_i} = \mu_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Why Bother with Parametric Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;
- Large amounts of data can be modelled as random variables from the exponential family of distributions
- If there are relatively few observations, providing a structure for the model generating process can improve predictive performance
- Enables us to carry out inference on model covariates
- Simple and intuitive to understand. In some industries (such as insurance), this has huge benefits- it is transparent and can fit into a rate structure&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;
- Validity of inference dependent on assumptions being satisfied
- Places a very rigid structure on the relationship between the independent and dependent variables
- Typically has worse predictive performance than non-linear models, such as Boosted Trees and Neural Networks, due to linearity and inability to account for complex interactions&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;link-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Link Functions&lt;/h2&gt;
&lt;p&gt;So Generalized Linear models explicitly introduce a link function (the link function for the linear model is simply the identity, &lt;span class=&#34;math inline&#34;&gt;\(g(\mu) = \mu\)&lt;/span&gt;) and through the specification of a mean-variance relationship (the response belongs to a member of the exponential family). Using a link function allows us to transform values of the linear predictor to predictions of the mean, such that these predictions are always contained within the range of possible values for the mean, &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When choosing a link function, there is no ‘correct’ choice, however there are a few properties we require to be able to interpret and fit a model:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The link function transforms the linear predictor such that the prediction of &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; for each &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is within the range of possible values of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The link function must be &lt;em&gt;monotonic&lt;/em&gt; and therefore have a &lt;em&gt;unique inverse&lt;/em&gt;. That is, each value on the linear predictor must be mapped to a unique value of the mean and the link function must preserve the order/ranking of predictions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The link function must be &lt;em&gt;differentiable&lt;/em&gt;, in order to estimate model coefficients.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For OLS, the linear predictor &lt;span class=&#34;math inline&#34;&gt;\(X\beta\)&lt;/span&gt; can take on any value in the range &lt;span class=&#34;math inline&#34;&gt;\((-\infty, \infty)\)&lt;/span&gt;. For a Poisson model, we require the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (equivalent to the commonly used &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;), to lie in the range &lt;span class=&#34;math inline&#34;&gt;\((0, \infty)\)&lt;/span&gt;, thus we need a link function that transforms the linear predictor &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; to lie in this range. The common choice of link function for a Poisson GLM is the log-link (&lt;span class=&#34;math inline&#34;&gt;\(\log(\mu_i) = \eta_i\)&lt;/span&gt;). Exponentiating the linear predictor results in &lt;span class=&#34;math inline&#34;&gt;\(\mu_i \in (0, \infty)\)&lt;/span&gt; as required for count data modeled via a Poisson. The log-link also results in a nice interpretation, since it exponentiates the linear predictor resulting in a multiplication of exponentiated coefficients: &lt;span class=&#34;math inline&#34;&gt;\(log(\mu_i) = exp(\beta_0 + \beta_1 x_{1, i}) = \exp(\beta_0) \exp(\beta_1 x_{1, i})\)&lt;/span&gt;. Note that we can’t use the square root function as a link function since it does not have a &lt;em&gt;unique inverse&lt;/em&gt; (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\sqrt(4) = \pm 2\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;For the Binomial, we choose a link function that maps &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt;, the probability of success to the interval &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;. The link function &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i) = log(\frac{p_i}{n - p_i}) = X \beta_i\)&lt;/span&gt; is one candidate. This transforms the Linear predictor: &lt;span class=&#34;math inline&#34;&gt;\(\frac{p_i}{1 - p_i} = \exp(X \beta_i)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\implies p_i = (\frac{e^{X \beta_i}}{1 + e ^ {X \beta_i}}) \in [0, 1]\)&lt;/span&gt; to the required range.&lt;/p&gt;
&lt;p&gt;Whilst we are able to choose any link function that satisfies these properties, the usual choice is to select the &lt;em&gt;Canonical&lt;/em&gt; link function, which arises from writing the distribution in its exponential form. These link functions have nice mathematical properties and simplify the derivation of the Maximum Likelihood Estimators. We could also use an Information Criteria such as AIC to choose the best-fitting link function, although there is usually little deviation in performance, so the common choice is to use the link function with the most intuitive interpretation (which is often the canonical link function anyway).&lt;/p&gt;
&lt;p&gt;Some common distributions and Canonical links are show below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;43%&#34; /&gt;
&lt;col width=&#34;46%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Family&lt;/th&gt;
&lt;th&gt;Canonical Link (&lt;span class=&#34;math inline&#34;&gt;\(\eta = g(\mu)\)&lt;/span&gt;)&lt;/th&gt;
&lt;th&gt;Inverse Link (&lt;span class=&#34;math inline&#34;&gt;\(\mu = g^{-1}(\eta)\)&lt;/span&gt;)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Binomial&lt;/td&gt;
&lt;td&gt;Logit: &lt;span class=&#34;math inline&#34;&gt;\(\eta = log \left( \frac{\mu}{n - \mu} \right)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{n}{1 + e^{-n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Gaussian&lt;/td&gt;
&lt;td&gt;Identity: &lt;span class=&#34;math inline&#34;&gt;\(\eta = \mu\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \eta\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Poisson&lt;/td&gt;
&lt;td&gt;Log: &lt;span class=&#34;math inline&#34;&gt;\(\eta = log(\mu)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = e^{\eta}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Gamma&lt;/td&gt;
&lt;td&gt;Inverse: &lt;span class=&#34;math inline&#34;&gt;\(\eta = \frac{1}{\mu}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{1}{\eta}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;the-exponential-family-of-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Exponential Family of Distributions&lt;/h2&gt;
&lt;p&gt;A random variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; has a distribution from the exponential family if its probability density function is of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(y;\theta, \phi) = exp \left( \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the location parameter of the distribution&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(a(\phi)\)&lt;/span&gt; is the scale/dispersion parameter&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(c(y, \phi)\)&lt;/span&gt; is a normalization term&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It can also be shown that &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathbb{E}}[y] = b&amp;#39;(\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Var[y] = b&amp;#39;&amp;#39;(\theta) a(\phi)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-analysis-and-newton-raphson-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood Analysis and Newton-Raphson Method&lt;/h2&gt;
&lt;p&gt;The fitting of any form of Statistical Learning algorithm involves an optimization problem. Optimization refers to the task of either minimizing or maximizing some function &lt;span class=&#34;math inline&#34;&gt;\(f(\mathbf{\beta})\)&lt;/span&gt; by altering &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. We usually phrase most optimization problems in terms of minimizing &lt;span class=&#34;math inline&#34;&gt;\(f(\beta)\)&lt;/span&gt;. For the case of parametric learning models (we place distributional assumptions on the target/response &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, such that they are drawn independently from some probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p_{model}(y, \theta)\)&lt;/span&gt;), one can also use the process of Maximum Likelihood to find model coefficients. Under this approach, we have the following optimization problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\beta}^* =arg max(l(y_i | \mathbf{\beta}))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(l(y_i | \mathbf{\beta})\)&lt;/span&gt; is the likelihood function. The likelihood can be interpreted as the probability of seeing the data in our sample given the parameters and we naturally wish to maximize this quantity to obtain a good model fit.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Why Maximum Likelihood?&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Simple and intuitive method to find estimates for any &lt;em&gt;parametric&lt;/em&gt; model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For large samples, MLE’s have useful properties, assuming large n and i.i.d (independent and identically distributed) samples.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathbb{E}}[\hat{\theta}_{MLE}] = \theta\)&lt;/span&gt; (&lt;strong&gt;Unbiased&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(\hat{\theta}_{MLE}) = \frac{1}{n I(\theta)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt; is the Fisher Information in the sample. That is, we can calculate the variance of model coefficients and hence perform inference&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The MLE also achieves the &lt;strong&gt;Cramer Lower Bound&lt;/strong&gt;, that is it has the smallest variance of any estimator, thus is &lt;strong&gt;Asymptotically Efficient&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, there are several disadvantages to Maximum Likelihood, particularly if the purpose of modelling is for prediction. Under Maximum Likelihood, we fit exactly to the training dataset, resulting in overfitting and poor generalization to unseen data.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Consider the general form of the probability density function for a member of the exponential family of distributions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(y;\theta, \phi) = exp \left( \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The likelihood is then (assuming independence of observations):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(f(y_i)) = \prod_{i = 1}^n exp \left( \frac{1}{a(\phi)} (y_i \theta_i - b(\theta_i) + c(y_i, \phi) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with log-likelihood (since the log of the product of exponentials is the sum of the exponentiated terms):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log(L(f(y_i))) = l(f(y_i)) = \sum_{i = 1}^n \frac{1}{a(\phi)}(y_i \theta_i - b(\theta_i)) + c(y_i, \phi)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the logarithmic function is monotonically increasing, order is preserved hence finding the maximum of the log-likelihood yields the same result as finding the maximum of the likelihood. We wish to maximize this log-likelihood, hence we can differentiate, equate to zero and solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (We could also ensure the second derivative evaluated at &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; is negative, therefore we have maximized (and not minimized) the log-likelihood). Via the Chain Rule we have:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial l(f(y_i))}{\partial \beta_j} = \sum_{i = 1}^n \frac{\partial l(f(y_i))}{\partial \theta_i} \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial \beta_j} = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is also known as the Score function, since it tells us how sensitive the model is to changes in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; at a given value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Since we differentiate with respect to each coefficent, &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(j \in [0, 1, ..., K]\)&lt;/span&gt;), we have a system of &lt;span class=&#34;math inline&#34;&gt;\((K + 1)\)&lt;/span&gt; equations to solve.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Chain Rule&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Chain Rule is often used in Optimization problems. Here we utilize the chain rule by recognizing that, as seen in &lt;strong&gt;Equation 1&lt;/strong&gt;, the likelihood is a function of the location parameter of the distribution, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, which in turn is a function of the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Via the link function, we have that &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i) = \eta_i\)&lt;/span&gt; and via the linear predictor, &lt;span class=&#34;math inline&#34;&gt;\(\eta_i = \beta_0 + \sum_{i = 1}^{p}\beta_p x_{i, p}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each of these individual partial derivatives can then be identified:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial l_i}{\partial \theta_i} = \frac{y_i - b&amp;#39;(\theta_i)}{a(\phi)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{V(\mu_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;since &lt;span class=&#34;math inline&#34;&gt;\(\frac{\mu_i}{\theta_i} = b&amp;#39;&amp;#39;(\theta_i) = V(\mu_i)\)&lt;/span&gt; where V is the variance function of the model as it dictates the mean-variance relationship.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \mu_i}{\partial \eta_i} = \frac{1}{g&amp;#39;(\mu_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;since &lt;span class=&#34;math inline&#34;&gt;\(g(\mu_i) = \eta_i \implies \frac{\partial \eta_i}{\partial \mu_i} = g&amp;#39;(\mu_i)\)&lt;/span&gt;. Finally:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \eta_i}{\partial \beta_j} = x_j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Putting this all together yields the Maximum Likelihood Equations:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{i = 1}^n \frac{(y_i - \mu_i) x_{i, j}}{a(\phi) V(\mu_i) g&amp;#39;(\mu_i)} = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Least Square Vs Maximum Likelihood&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Under Gaussian assumptions, both Ordinary Least Squares (OLS) and Maximum Likelihood yield the same solution (assuming there is no collinearity/near collinearity between features such that the design matrix is invertible). Both methods aim to minimize the Residual Sum of Squares under these assumptions. One could also use numeric optimization, such as &lt;em&gt;Gradient Descent&lt;/em&gt;, although this will not yield an exact solution.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Equation 2&lt;/strong&gt; does not have a closed form solution, except when we have a Gaussian Linear Model.&lt;/p&gt;
&lt;p&gt;To solve the &lt;strong&gt;Equation 2&lt;/strong&gt;, we must find coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (which for each observation &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; affect our prediction of &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\mu_i)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V(\mu_i)\)&lt;/span&gt; via our distributional assumptions for the relationship between the mean and the variance), such that summing these terms over all observations yields 0. To solve this, we use the numerical Newton - Raphson Method, which when applied to the fitting of GLMs is knows as &lt;em&gt;Iteratively Reweighted Least Squares&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iteratively-reweighted-least-squares-irls&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Iteratively Reweighted Least Squares (IRLS)&lt;/h2&gt;
&lt;p&gt;Recall the Newton - Raphson method for a single dimension. We wish to find the root of the function; in this case the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; such that the derivative of the log-likelihood is 0. In One-Dimension, to find the root of function f we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{t+1} = x_t - \frac{f(x_t)}{f&amp;#39;(x_t)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The closer &lt;span class=&#34;math inline&#34;&gt;\(f(x_t)\)&lt;/span&gt; is to 0, the closer we are to the root, hence the step change between iterations will be smaller.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;NewtonRaphson.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Newton Raphson- Finding the root of f(x) via iteration in one dimension&lt;/p&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;Instead of the log-likelihood function &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, we want to optimize its derivative:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x_{t+1} = x_t - \frac{f&amp;#39;(x_t)}{f&amp;#39;&amp;#39;(x_t)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or for the vector of coefficient estimates at iteration t:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{t+1} = \beta_t - \left( \frac{\partial l}{\partial \beta_j}(\beta_t) \right) \left( \frac{\partial^2 l}{\partial \beta_j \partial \beta_k} \right)^{-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Newton-Raphson technique is derived by considering the &lt;em&gt;Taylor Expansion&lt;/em&gt; about the solution &lt;span class=&#34;math inline&#34;&gt;\(\beta^*\)&lt;/span&gt; (that sets &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial l}{\partial \beta}\)&lt;/span&gt; to zero).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0 = \frac{\partial l}{\partial \beta}(\beta^*) - (\beta - \beta^*) \frac{\partial^2 l}{\partial \beta_j \beta_k} + ...\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we ignore all derivative terms higher than &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; order, we can derive an iterative solution. Generalizing to multiple dimensions we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{t + 1} = \beta_t - \mathbf{H}^{-1}_t \mathbf{U}_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{U}_t\)&lt;/span&gt; is the Score Vector evaluated at &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}_t\)&lt;/span&gt; denotes the (p + 1) x (p + 1) Hessian matrix of second Derivatives.&lt;/p&gt;
&lt;p&gt;Given the Score function &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial l}{\beta_j} = \nabla_{\beta} l = \frac{(y_i - \mu_i)}{a(\phi)} \frac{x_{i,j}}{V(\mu_i)}\frac{1}{g&amp;#39;(\mu_i)}\)&lt;/span&gt;, the Hessian is:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\nabla^2_{\beta} = \sum_{i = 1}^n \frac{x_{i,j}}{a(\phi)} \left( (y_i - \mu_i)&amp;#39; \frac{1}{g&amp;#39;(\mu_i)} \frac{1}{V(\mu_i)} \right) + (y_i - \mu_i) \left( \frac{1}{g&amp;#39;(\mu_i)} \frac{1}{V(\mu_i)} \right) &amp;#39; \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;by Product Rule. &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; is a data point so does not depend on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. The partial derivative of &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \mu_i}{\partial \beta_k} = \frac{\mu_i}{\eta_i} \frac{\eta_i}{\beta_k} = \frac{1}{g&amp;#39;(\mu_i)} x_k\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;substituting, the first term of &lt;strong&gt;equation 3&lt;/strong&gt; becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{x_{i,j}}{a(\phi)} \left( - \frac{x_{i, k}}{g&amp;#39;(\mu_i)} \right) \frac{1}{g&amp;#39;(\mu_i)} \frac{1}{V(\mu_i)} = - \frac{x_{i, j} x_{i, k}}{a(\phi)(g&amp;#39;(\mu_i))^2} \frac{1}{V(\mu_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now consider the &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; term in &lt;strong&gt;equation 3&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\left( \frac{1}{g&amp;#39;(\mu_i)} \frac{1}{V(\mu_i)} \right)&amp;#39;\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using Newton-Raphson, we would need to calculate this derivative. However, we can instead utilize &lt;strong&gt;Fisher Scoring&lt;/strong&gt;, ensuring this derivative term cancels out.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Fisher Scoring&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Fisher Scoring is a form of Newton’s Method used in statistics to solve Maximum Likelihood equations numerically. Instead of using the inverse of the Hessian, we use the inverse of the Fisher Information matrix.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{t+1} = \beta_t + J^{-1} \nabla l\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(J = \mathop{\mathbb{E}}[- \nabla^2 l]\)&lt;/span&gt;, the expected value of the negative Hessian.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Taking the negative expected value from &lt;strong&gt;equation 3&lt;/strong&gt;, the first term becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathop{\mathbb{E}} \left( \frac{x_{i,j} x_{i,k}}{a(\phi) (g&amp;#39;(\mu_i))^2} \frac{1}{V(\mu_i)} \right) = \frac{x_{i,j} x_{i,k}}{a(\phi) (g&amp;#39;(\mu_i))^2} \frac{1}{V(\mu_i)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;since none of the above values depend on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; hence are all constant. The &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; term in &lt;strong&gt;equation 3&lt;/strong&gt; becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathop{\mathbb{E}} \left( - \frac{x_{i,j}}{a(\phi)}(y_i - \mu_i) \left( \frac{1}{g&amp;#39;(\mu_i) V(\mu_i)} \right) &amp;#39; \right) \\ = - \frac{x_{i,j}}{a(\phi)}(y_i - \mu_i) \left( \frac{1}{g&amp;#39;(\mu_i) V(\mu_i)} \right)&amp;#39; \mathop{\mathbb{E}}(y_i - \mu_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but &lt;span class=&#34;math inline&#34;&gt;\(\mathop{\mathbb{E}}(y_i - \mu_i) = 0\)&lt;/span&gt; hence the second term of &lt;strong&gt;equation 3&lt;/strong&gt; vanishes. We then have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[J = \sum_{i = 1}^n \frac{x_{i,j} x_{i,k}}{a(\phi) (g&amp;#39;(\mu_i))^2 \frac{1}{V(\mu_i)}} = \mathbf{X}^T \mathbf{W} \mathbf{X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{W} = \frac{1}{a(\phi)} \begin{bmatrix} \frac{1}{V(\mu_1) (g&amp;#39;(\mu_1))^2} &amp;amp;  &amp;amp; \\ &amp;amp; ... &amp;amp; \\ &amp;amp; &amp;amp; \frac{1}{V(\mu_n)(g&amp;#39;(\mu_n))^2}\\ \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From Fisher Scoring, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\beta}_{t+1} = \mathbf{\beta_t} = \mathbf{J}^{-1} \nabla_{\beta} l (\beta_t)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can rewrite &lt;span class=&#34;math inline&#34;&gt;\(\nabla_{\beta}l = \sum_{i = 1}^n \frac{y_i - \mu_i}{a(\phi)} \frac{1}{a(\phi)} \frac{x_{i,j}}{V(\mu_i g&amp;#39;(\mu_i))}\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^T \mathbf{D} \mathbf{V}^{-1} (y - \mu)\)&lt;/span&gt; where:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{D} = \begin{bmatrix} \frac{1}{g&amp;#39;(\mu_1)} &amp;amp; &amp;amp; \\ &amp;amp; ... &amp;amp; \\ &amp;amp; &amp;amp; \frac{1}{g&amp;#39;(\mu_n)} \\ \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{V}^{-1} = \frac{1}{a(\phi)} \begin{bmatrix} \frac{1}{V(\mu_1)} &amp;amp; &amp;amp; \\ &amp;amp; ... &amp;amp; \\ &amp;amp; &amp;amp; \frac{1}{V(\mu_n)} \\ \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then for the Fisher Equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\beta}_{t+1} = \mathbf{\beta}_t + (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{D} \mathbf{V}^{-1} (\mathbf{y} - \mathbf{\mu})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write this more generally, by noting that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; is the same as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D} \mathbf{V}^{-1}\)&lt;/span&gt;, except we have &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{(g&amp;#39;(\mu_i))^2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\implies \mathbf{D} \mathbf{V}^{-1} = \mathbf{W M}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{M} = \begin{bmatrix}  \frac{1}{g&amp;#39;(\mu_1)} &amp;amp; &amp;amp; \\ &amp;amp; ... &amp;amp; \\ &amp;amp; &amp;amp; \frac{1}{g&amp;#39;(\mu_1)} \\ \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\implies \beta_{t + 1} = \beta_t + (X^T W X)^{-1} X^T W M (y - \mu)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Each term can be calculated and used to generate an iterative model-fitting algorithm for updating the esimates &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt; at each iteration. We then set a convergence condition such at if the increase in the value of the likelihood function between iterations is arbitrarily small (&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;), the algorithm stops and outputs the values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt; at iteration &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, as estimates for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This result can be further simplified&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equation 4- Iteratively Reweighted Least Squares&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_{t+1} = (X^T W X)^{-1} (X^T W X) \beta_t + (X^T W X)^{-1} X^T W M (y - \mu) \\ =
  (X^T W X)^{-1} X^T W (X \beta_t + M (y - \mu)) \\ =
  (X^T W X)^{-1} X^T W Z_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Z_t := \eta_t + M(y - \mu)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From &lt;strong&gt;Equation 4&lt;/strong&gt;, the step size at each iteration mainly depends on &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Larger &lt;span class=&#34;math inline&#34;&gt;\((y - \mu)\)&lt;/span&gt; (in a relative sense) indicates a model which is poorly fit hence we require larger adjustments to &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt; to converge more quickly to a reasonable fit before we start iterating in a more granular manner.&lt;/p&gt;
&lt;p&gt;Recall, we have shown &lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial l_i}{\beta_j} \propto \frac{1}{g&amp;#39;(\mu_i)}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\mu_i)\)&lt;/span&gt; is the derivative of the link function, giving the rate of change of model predictions w.r.t. &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Thus, larger values of &lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\mu_i)\)&lt;/span&gt; indicate a model whose fit is sensitive to small changes in &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, so is not close to at least a local (ideally a global) optima. In this case, larger step changes are required.&lt;/p&gt;
&lt;p&gt;The Hessian matrix of &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; derivatives adjusts the step change depending on the curvature of the gradient of the likelihood function at &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt;, information encapsulated in the matrix &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, allowing one to optimize step changes depending on the shape of the likelihood function at &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. For example, if the &lt;span class=&#34;math inline&#34;&gt;\(2^{nd}\)&lt;/span&gt; derivative is negative (the gradient of the likelihood function is becoming more negative for a small increase &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;), the likelihood function curves downwards, so the likelihood will decrease by more than anticipated by the &lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; derivative alone.&lt;/p&gt;
&lt;div class=&#34;alert alert-info&#34;&gt;
&lt;p&gt;&lt;strong&gt;Derivation of Model Fitting Algorithms/Coefficients&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This derivation of Iteratively Reweighted Least Squares for GLMs follows a similar procedure to the derivation of any numerical model fitting algorithm. Firstly, we identify an objective function over which to optimize. Typical Machine Learning problems involve minimizing some loss function, which measures the discrepency between actual and predicted target values. &lt;em&gt;Gradient Descent&lt;/em&gt; and its variations are often used for solving such optimization problems.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Generalized Linear Models are used in a wide array of fields. GLM’s are particularly useful when we expect the target variable to have been generated by a distribution from the exponential family, with a mean-variance relationship proportional to that of the modelled distribution.&lt;/p&gt;
&lt;p&gt;Whilst GLM’s tend to be outperformed by models capable of accounting for non-linearities and multi-dimensional interactions, they are highly appropriate for inference and may outperform more complex models if there is a lack of data available.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
