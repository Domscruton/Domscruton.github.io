<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Dominic Scruton" />

  
  
  
    
  
  <meta name="description" content="1) Introduction- Framing the Problem The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year." />

  
  <link rel="alternate" hreflang="en-us" href="https://domscruton.github.io/project/carinsurance/" />

  









  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.481af39c39ffd87b2d14f39943e7c723.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://domscruton.github.io/project/carinsurance/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Dominic Scruton" />
  <meta property="og:url" content="https://domscruton.github.io/project/carinsurance/" />
  <meta property="og:title" content="Predicting Car Insurance Claims | Dominic Scruton" />
  <meta property="og:description" content="1) Introduction- Framing the Problem The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year." /><meta property="og:image" content="https://domscruton.github.io/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://domscruton.github.io/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2020-06-01T17:28:03&#43;01:00"
      />
    
    <meta property="article:modified_time" content="2020-06-01T17:28:03&#43;01:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://domscruton.github.io/project/carinsurance/"
  },
  "headline": "Predicting Car Insurance Claims",
  
  "datePublished": "2020-06-01T17:28:03+01:00",
  "dateModified": "2020-06-01T17:28:03+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Dominic Scruton"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Dominic Scruton",
    "logo": {
      "@type": "ImageObject",
      "url": "https://domscruton.github.io/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "1) Introduction- Framing the Problem The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year."
}
</script>

  

  

  

  





  <title>Predicting Car Insurance Claims | Dominic Scruton</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="86268d884501a98db00a5dbc5de0d02e" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Dominic Scruton</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Dominic Scruton</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article article-project">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Predicting Car Insurance Claims</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 1, 2020
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="1-introduction--framing-the-problem">1) Introduction- Framing the Problem</h2>
<p>The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year. The problem itself is one of classification, however the predicted probabilities of making a claim are of more interest, as often they are far below 0.5, suggesting that none of the models would classify any individuals as making a claim. This was due to only 3.64% of individuals in the training data making a claim and caused also by a lack of correlation between the target variable and most of the features, suggesting that the features lacked enough information to fully explain differences in the claims rate between individuals. The classification problem can be expressed as follows:</p>
<p>$$y_i = f(X_i, \theta)$$</p>
<p>In this case, the response, $y_i$, is the probability that individual i makes a claim within the next year. It could also be converted into a prediction (&lsquo;claim&rsquo; or &lsquo;no claim&rsquo;) by assigning a threshold; if the probability of claiming is greater than this threshold, the individual is predicted to make a claim, otherwise they are not predicted to make a claim. The goal of machine learning is to find and estimate a model, $f(X_i, \theta)$, that takes as its arguments the data for the ith instance ($X_i$) and the estimated parameter values ($\theta$), and provides the most accurate predictions on unseen data.</p>
<p>In general, one aims to find the model that makes these best predictions, either by classifying the individuals into &lsquo;claim&rsquo; or &lsquo;no claim&rsquo; as accurately as possible, or providing probabilities that each individual will claim that are as close as possible to the true, underlying probabilites (which may be unknown). However, in this situation, it may also be prudent to consider measures of performance that take into account the economic implications of incorrectly predicting an individual won&rsquo;t claim when in fact they do. Equivalently, it may be more economical to overpredict the probabilities that certain individuals will claim, rather that underpredict these. Later, we use the ROC curve and the Area Under Curve (AUC) to compare the performance of models that takes these ideas into account by implicitly considering false negatives when comparing model performance.</p>
<p>We implement a Logistic Regression, Random Forest classification and Neural Network and the justification for these models is presented in Section 6. The performance of all models is assessed by 3-fold cross-validation, using the AUC score as a measure of model performance.</p>
<h2 id="2-import-and-tidy-the-data">2) Import and Tidy the Data</h2>
<p>The first step is to import the data, assess its structure and perform any preliminary steps to tidy the data.</p>
<pre><code class="language-python">#import relevant libraries
#Numpy for scientific computation
import numpy as np
#Pandas for data manipulation
import pandas as pd
#Matplotlib plotting library
%matplotlib inline
import matplotlib.pyplot as plt
#Seaborn for statistical data visualization
import seaborn as sns
</code></pre>
<pre><code class="language-python">#import test and training data
training_data = pd.read_csv(&quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/train.csv&quot;)
test_data = pd.read_csv(&quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/test.csv&quot;)
</code></pre>
<pre><code class="language-python">#Assess the structure of the data
training_data.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
      <th>ps_ind_01</th>
      <th>ps_ind_02_cat</th>
      <th>ps_ind_03</th>
      <th>ps_ind_04_cat</th>
      <th>ps_ind_05_cat</th>
      <th>ps_ind_06_bin</th>
      <th>ps_ind_07_bin</th>
      <th>ps_ind_08_bin</th>
      <th>...</th>
      <th>ps_calc_11</th>
      <th>ps_calc_12</th>
      <th>ps_calc_13</th>
      <th>ps_calc_14</th>
      <th>ps_calc_15_bin</th>
      <th>ps_calc_16_bin</th>
      <th>ps_calc_17_bin</th>
      <th>ps_calc_18_bin</th>
      <th>ps_calc_19_bin</th>
      <th>ps_calc_20_bin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>5</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>9</td>
      <td>1</td>
      <td>5</td>
      <td>8</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>9</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13</td>
      <td>0</td>
      <td>5</td>
      <td>4</td>
      <td>9</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>4</td>
      <td>2</td>
      <td>7</td>
      <td>7</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>9</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 59 columns</p>
</div>
<p>The data contains 57 features, which are a range of categorical, binomial and continuous variables, the column of target values and an id indicating a unique identity for each individual. The training data constitutes instances for 595212 individuals.</p>
<pre><code class="language-python">print(&quot;Number of instances in the training set:&quot;, len(training_data))
</code></pre>
<pre><code>Number of instances in the training set: 595212
</code></pre>
<pre><code class="language-python">#summary of the training data
training_data.describe()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
      <th>ps_ind_01</th>
      <th>ps_ind_02_cat</th>
      <th>ps_ind_03</th>
      <th>ps_ind_04_cat</th>
      <th>ps_ind_05_cat</th>
      <th>ps_ind_06_bin</th>
      <th>ps_ind_07_bin</th>
      <th>ps_ind_08_bin</th>
      <th>...</th>
      <th>ps_calc_11</th>
      <th>ps_calc_12</th>
      <th>ps_calc_13</th>
      <th>ps_calc_14</th>
      <th>ps_calc_15_bin</th>
      <th>ps_calc_16_bin</th>
      <th>ps_calc_17_bin</th>
      <th>ps_calc_18_bin</th>
      <th>ps_calc_19_bin</th>
      <th>ps_calc_20_bin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5.952120e+05</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>...</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
      <td>595212.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>7.438036e+05</td>
      <td>0.036448</td>
      <td>1.900378</td>
      <td>1.358943</td>
      <td>4.423318</td>
      <td>0.416794</td>
      <td>0.405188</td>
      <td>0.393742</td>
      <td>0.257033</td>
      <td>0.163921</td>
      <td>...</td>
      <td>5.441382</td>
      <td>1.441918</td>
      <td>2.872288</td>
      <td>7.539026</td>
      <td>0.122427</td>
      <td>0.627840</td>
      <td>0.554182</td>
      <td>0.287182</td>
      <td>0.349024</td>
      <td>0.153318</td>
    </tr>
    <tr>
      <th>std</th>
      <td>4.293678e+05</td>
      <td>0.187401</td>
      <td>1.983789</td>
      <td>0.664594</td>
      <td>2.699902</td>
      <td>0.493311</td>
      <td>1.350642</td>
      <td>0.488579</td>
      <td>0.436998</td>
      <td>0.370205</td>
      <td>...</td>
      <td>2.332871</td>
      <td>1.202963</td>
      <td>1.694887</td>
      <td>2.746652</td>
      <td>0.327779</td>
      <td>0.483381</td>
      <td>0.497056</td>
      <td>0.452447</td>
      <td>0.476662</td>
      <td>0.360295</td>
    </tr>
    <tr>
      <th>min</th>
      <td>7.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>-1.000000</td>
      <td>0.000000</td>
      <td>-1.000000</td>
      <td>-1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>3.719915e+05</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>6.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>7.435475e+05</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.115549e+06</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>7.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>9.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.488027e+06</td>
      <td>1.000000</td>
      <td>7.000000</td>
      <td>4.000000</td>
      <td>11.000000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>19.000000</td>
      <td>10.000000</td>
      <td>13.000000</td>
      <td>23.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 59 columns</p>
</div>
<p>From the above, some of the continuous features have right-skew in their distributions, such as &lsquo;ps_ind_01&rsquo;, with a mean value of 1.9 that is much lower than its maximum value and upper quartile. Later some of these continuous variables will be scaled to improve the performance of the models that are fitted to the data.</p>
<pre><code class="language-python">#The test data contains no target variable
test_data.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>ps_ind_01</th>
      <th>ps_ind_02_cat</th>
      <th>ps_ind_03</th>
      <th>ps_ind_04_cat</th>
      <th>ps_ind_05_cat</th>
      <th>ps_ind_06_bin</th>
      <th>ps_ind_07_bin</th>
      <th>ps_ind_08_bin</th>
      <th>ps_ind_09_bin</th>
      <th>...</th>
      <th>ps_calc_11</th>
      <th>ps_calc_12</th>
      <th>ps_calc_13</th>
      <th>ps_calc_14</th>
      <th>ps_calc_15_bin</th>
      <th>ps_calc_16_bin</th>
      <th>ps_calc_17_bin</th>
      <th>ps_calc_18_bin</th>
      <th>ps_calc_19_bin</th>
      <th>ps_calc_20_bin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>8</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>5</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>5</td>
      <td>1</td>
      <td>0</td>
      <td>5</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>5</td>
      <td>1</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 58 columns</p>
</div>
<pre><code class="language-python">print(&quot;Number of instances in the test data:&quot;, len(test_data))
</code></pre>
<pre><code>Number of instances in the test data: 892816
</code></pre>
<pre><code class="language-python">test_data.describe()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>ps_ind_01</th>
      <th>ps_ind_02_cat</th>
      <th>ps_ind_03</th>
      <th>ps_ind_04_cat</th>
      <th>ps_ind_05_cat</th>
      <th>ps_ind_06_bin</th>
      <th>ps_ind_07_bin</th>
      <th>ps_ind_08_bin</th>
      <th>ps_ind_09_bin</th>
      <th>...</th>
      <th>ps_calc_11</th>
      <th>ps_calc_12</th>
      <th>ps_calc_13</th>
      <th>ps_calc_14</th>
      <th>ps_calc_15_bin</th>
      <th>ps_calc_16_bin</th>
      <th>ps_calc_17_bin</th>
      <th>ps_calc_18_bin</th>
      <th>ps_calc_19_bin</th>
      <th>ps_calc_20_bin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>8.928160e+05</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>...</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
      <td>892816.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>7.441535e+05</td>
      <td>1.902371</td>
      <td>1.358613</td>
      <td>4.413734</td>
      <td>0.417361</td>
      <td>0.408132</td>
      <td>0.393246</td>
      <td>0.257191</td>
      <td>0.163659</td>
      <td>0.185905</td>
      <td>...</td>
      <td>5.438478</td>
      <td>1.440265</td>
      <td>2.875013</td>
      <td>7.540367</td>
      <td>0.123720</td>
      <td>0.627756</td>
      <td>0.554660</td>
      <td>0.287796</td>
      <td>0.349344</td>
      <td>0.152428</td>
    </tr>
    <tr>
      <th>std</th>
      <td>4.296830e+05</td>
      <td>1.986503</td>
      <td>0.663002</td>
      <td>2.700149</td>
      <td>0.493453</td>
      <td>1.355068</td>
      <td>0.488471</td>
      <td>0.437086</td>
      <td>0.369966</td>
      <td>0.389030</td>
      <td>...</td>
      <td>2.330081</td>
      <td>1.200620</td>
      <td>1.694072</td>
      <td>2.745882</td>
      <td>0.329262</td>
      <td>0.483403</td>
      <td>0.497004</td>
      <td>0.452736</td>
      <td>0.476763</td>
      <td>0.359435</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>-1.000000</td>
      <td>0.000000</td>
      <td>-1.000000</td>
      <td>-1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>3.720218e+05</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>4.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>6.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>7.443070e+05</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.116308e+06</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>7.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>9.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.488026e+06</td>
      <td>7.000000</td>
      <td>4.000000</td>
      <td>11.000000</td>
      <td>1.000000</td>
      <td>6.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>20.000000</td>
      <td>11.000000</td>
      <td>15.000000</td>
      <td>28.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 58 columns</p>
</div>
<p>A simple visual comparison of the mean of each feature for the test data suggests it is very similar to that of the training data.</p>
<h2 id="3-exploratory-data-analysis">3) Exploratory Data Analysis</h2>
<h3 id="summary-of-the-features">Summary of the Features</h3>
<p>In order to understand the relationships between the different features and the binary response, and to assess the structure of each of the variables, the following information was gathered to provide a brief summary of the groups of features:</p>
<ul>
<li>Target: Binary response variable indicating a claim (1) or no claim (0)</li>
<li>IND: these 18 variables refer to characeristics of each individual driver</li>
<li>REG: 3 variables that refer to the region of the claim</li>
<li>CAR: 16 variables related to the particular car of each individual on which the claim was made</li>
<li>CALL: 10 variables, which are feature engineered variables relating to the theory behind pricing of autoinsurance quotations</li>
</ul>
<p>Missing values for the binary variables are indicated by a -1, whilst the postfix &lsquo;bin&rsquo; indicates binary features and &lsquo;cat&rsquo; indicates categorical features. Features without a postfix are numerical. The exact meaning of each individual feature is not available (Porto Seguro&rsquo;s Safe Driver Prediction- Welcome, 2017).</p>
<p>Histograms show the distribution of each feature. We separate the plotting of histograms of the features by the four feature types as discussed above, which makes it easier to visualise the distributions of features within groups. These histograms also include missing values (-1 in value) for each of the features, which enables us to assess which features contain the majority of missing values and also understand how each feature should be prepared for machine learning algorithms.</p>
<pre><code class="language-python">#histogram for each 'individual-related' feature in the dataset
training_data.iloc[:, 2: 20].hist(bins = 50, figsize = (20, 15))
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_ce64c9f7af36c6b2d9c3381773802230.png 400w,
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_a7ac0eade7223662b3fcb2720f3959fb.png 760w,
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_ce64c9f7af36c6b2d9c3381773802230.png"
               width="760"
               height="556"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The attributes that relate to individual characteristics are mainly binary and contain few missing values.</p>
<pre><code class="language-python">#histogram for each 'Regional-based' feature in the dataset
training_data.iloc[:, 20: 23].hist(bins = 50, figsize = (20, 15))
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_45753a53e4d0764a0ebbf13c670554b9.png 400w,
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_e2be04db48fe77ed5c4f4f7c22cc7db4.png 760w,
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_45753a53e4d0764a0ebbf13c670554b9.png"
               width="760"
               height="556"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The attribute &lsquo;ps_reg_03&rsquo; contains a large number of missing values and this is something we may need to take into consideration. All three attributes of the &lsquo;region-based&rsquo; features are either left or right skewed, so will be normalized to lie in the interval (0,1), after removing missing values. They also contain no clear outliers, so scaling in this way will not be adversely affected by outlying observations.</p>
<pre><code class="language-python">#histogram for each 'Car-related' feature in the dataset
training_data.iloc[:, 23: 39].hist(bins = 50, figsize = (20, 15))
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_7b4021d3d48c34aada13fb10a1f24b5a.png 400w,
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_fa491335f2ce2310b122410f0da21446.png 760w,
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_7b4021d3d48c34aada13fb10a1f24b5a.png"
               width="760"
               height="556"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The &lsquo;car-related&rsquo; features consist of a range of categorical, binary and numeric features. Some have a significant number of missing values, however these may in themselves provide explanatory power.</p>
<pre><code class="language-python">#histogram for each 'Calculated (engineered)' feature in the dataset
training_data.iloc[:, 39:].hist(bins = 50, figsize = (20, 15))
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_9428190a560fb830c2b12c3f65792e3b.png 400w,
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_21787236b475cad66fc20918f75723ca.png 760w,
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_9428190a560fb830c2b12c3f65792e3b.png"
               width="760"
               height="556"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>None of the engineered features contain missing values and the continuous features also contain no obvious outliers, so standardising these features to lie in the interval (0,1) should not be adversely affected by the presence of outlying observations.</p>
<pre><code class="language-python">#Proportion of target instances that made a claim (1) and didn't make a claim (0)
training_data[&quot;target&quot;].value_counts() / len(training_data)
</code></pre>
<pre><code>0    0.963552
1    0.036448
Name: target, dtype: float64
</code></pre>
<p>The correlation matrix can be used to assess if there are any linear relationships between the target variable and the attributes in the training dataset:</p>
<pre><code class="language-python">corr_matrix = training_data.corr()
corr_matrix[&quot;target&quot;].sort_values(ascending = False)
</code></pre>
<pre><code>target            1.000000
ps_car_13         0.053899
ps_car_12         0.038790
ps_ind_17_bin     0.037053
ps_reg_02         0.034800
ps_ind_07_bin     0.034218
ps_car_04_cat     0.032900
ps_car_03_cat     0.032401
ps_reg_03         0.030888
ps_ind_05_cat     0.029165
ps_car_15         0.027667
ps_reg_01         0.022888
ps_car_05_cat     0.020754
ps_ind_01         0.018570
ps_car_01_cat     0.016256
ps_ind_08_bin     0.013147
ps_car_06_cat     0.011537
ps_ind_04_cat     0.009360
ps_ind_03         0.008360
ps_ind_12_bin     0.007810
ps_ind_14         0.007443
ps_car_11_cat     0.006129
ps_car_09_cat     0.005322
ps_ind_18_bin     0.004555
ps_ind_02_cat     0.004534
ps_ind_13_bin     0.002460
ps_ind_11_bin     0.002028
ps_calc_03        0.001907
ps_ind_10_bin     0.001815
ps_calc_01        0.001782
ps_calc_14        0.001362
ps_calc_02        0.001360
ps_calc_10        0.001061
ps_car_10_cat     0.001038
ps_calc_05        0.000771
ps_calc_09        0.000719
ps_calc_16_bin    0.000624
ps_calc_18_bin    0.000552
ps_calc_11        0.000371
ps_calc_06        0.000082
ps_calc_04        0.000033
ps_calc_07       -0.000103
ps_calc_17_bin   -0.000170
id               -0.000188
ps_calc_13       -0.000446
ps_calc_15_bin   -0.000490
ps_calc_08       -0.001006
ps_calc_20_bin   -0.001072
ps_calc_12       -0.001133
ps_car_11        -0.001213
ps_calc_19_bin   -0.001744
ps_car_14        -0.004474
ps_ind_09_bin    -0.008237
ps_car_08_cat    -0.020342
ps_ind_15        -0.021506
ps_ind_16_bin    -0.027778
ps_car_02_cat    -0.031534
ps_ind_06_bin    -0.034017
ps_car_07_cat    -0.036395
Name: target, dtype: float64
</code></pre>
<p>The fact that many of the features show little correlation with the binary target suggests that either the features lack information to explain and predict the target binary response, or the features have non-linear relationships with the target variable and hence a more complex model, such as a Neural Network, which can work more effectively on datasets that are not linearly separable, could be used. Noticeably, some of the attributes exhibit very little correlation with the binary target variable and they may just be &lsquo;noise&rsquo; and have no real relationship with the response. On the other hand, several features relating to the type of car that each individual insures appear to have greater correlation with the target variable. It could also be the case that missing values provide information about the probability of a claim; those individuals who are more likely to claim may provide less information in order to try reduce their insurance quotations.</p>
<h2 id="4-preparing-the-data-for-machine-learning-algorithms">4) Preparing the Data for Machine Learning Algorithms</h2>
<h3 id="missing-data">Missing Data</h3>
<p>Most machine learning algorithms can&rsquo;t work with missing values. There are a number of strategies that could be used in order to deal with this problem (Geron, 2019):</p>
<p><em><strong>i) Get rid of the corresponding instances</strong></em></p>
<p>We have already seen that some features have a large number of missing values. Therefore, getting rid of the corresponding instances of these attributes will lead to a greatly reduced sample size and the loss of potentially important information.</p>
<p><em><strong>ii) Get rid of the whole attribute</strong></em></p>
<p>Many of the attributes with missing values have high correlation with the target. For example, &lsquo;ps_reg_03&rsquo; has a relatively high correlation of 0.0324 with the target variable and a large number of missing values. Getting rid of this feature may lead to reduced predictive power.</p>
<p><em><strong>iii) Set the values to some value (e.g. the median)</strong></em></p>
<p>For the continuous and ordinal categorical features with many levels, it is natural that we might want to fill their missing values with some value, such as the median of that feature. However, it does not make sense to fill the missing values of binary features with their median value. Therefore, binary features with missing values are converted to multinomial features, with a new category representing missing values.</p>
<p>We use a preparation pipeline to prepare the data for a machine learning algorithm, ensuring that all binary and continous features are correctly added to the model. We separate the features into two types, with different transformations carried out on each of the two types of feature:</p>
<p>a) Continuous and Categorical Features</p>
<p>For these features, missing values are inputted using the median value of that respective feature, using $SimpleImputer$. Then the data is normalized to lie in the interval (0,1) (via $MinMaxScaler$). Neural Networks generally expect an input value between 0 and 1 and as previously discussed, there are no extreme outliers so this scaling method is not unduly affected by outliers, providing justification for scaling these featues in this way. The categorical features have a natural ordering and  are already encoded numerically, so there is no need to separate them into further columns using dummy indexing.</p>
<p>b) Binary Features</p>
<p>The missing values are used as factor levels, which changes these features from binary to multinomial, as individuals with missing values may provide predictive power and information. The missing values are first converted to the number 2, so that they can be appropriately encoded using the $OneHotEncoder$ (this does not accept negative integers).</p>
<p>Given the low correlation between the target and some features, there may be a case for dropping some features from the models that we later fit. However, the correlation coefficient only measures linear correlations and may completely miss nonlinear relationships (Geron, 2019). Therefore, given the complex nature of the data, we retain all features when fitting each model.</p>
<pre><code class="language-python">#Split the training data into the labels and features
training_labels = training_data[&quot;target&quot;]
training_features = training_data.drop([&quot;target&quot;, &quot;id&quot;], axis = 1)
</code></pre>
<p>We first need to fill in the missing values for the ordinal categorical and numerical features, so that we can impute the median values for these features.</p>
<pre><code class="language-python">#Extract the features that we will treat as numerical
num_attributes = [&quot;ps_ind_01&quot;, &quot;ps_ind_02_cat&quot;, &quot;ps_ind_03&quot;, &quot;ps_ind_05_cat&quot;, &quot;ps_ind_14&quot;, &quot;ps_ind_15&quot;, &quot;ps_reg_01&quot;, 
                  &quot;ps_reg_02&quot;, &quot;ps_reg_03&quot;, &quot;ps_car_01_cat&quot;, &quot;ps_ind_04_cat&quot;, &quot;ps_car_04_cat&quot;, &quot;ps_car_06_cat&quot;, 
                  &quot;ps_car_09_cat&quot;, &quot;ps_car_10_cat&quot;, &quot;ps_car_11&quot;, &quot;ps_car_11_cat&quot;, &quot;ps_car_12&quot;, &quot;ps_car_13&quot;, 
                  &quot;ps_car_14&quot;, &quot;ps_car_15&quot;, &quot;ps_calc_01&quot;, &quot;ps_calc_02&quot;, &quot;ps_calc_03&quot;, &quot;ps_calc_04&quot;, &quot;ps_calc_05&quot;, &quot;ps_calc_06&quot;, 
                  &quot;ps_calc_07&quot;, &quot;ps_calc_08&quot;, &quot;ps_calc_09&quot;, &quot;ps_calc_10&quot;, &quot;ps_calc_11&quot;, &quot;ps_calc_12&quot;, &quot;ps_calc_13&quot;, &quot;ps_calc_14&quot;]
</code></pre>
<p>We replace the -1 values (corresponding to missing values) for these features with NA, so that we can then impute the median of each feature in place of the missing values.</p>
<pre><code class="language-python">#replace the numerical features with missing values encoded as NA for training data
training_features[num_attributes] = training_features[num_attributes].replace(-1,np.NaN)
</code></pre>
<pre><code class="language-python">#replace the numerical features with missing values encoded as NA for test data
test_data[num_attributes] = test_data[num_attributes].replace(-1,np.NaN)
</code></pre>
<pre><code class="language-python">#The numerical missing values have now been fillied with NA's
training_features.info()
</code></pre>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 595212 entries, 0 to 595211
Data columns (total 57 columns):
ps_ind_01         595212 non-null int64
ps_ind_02_cat     594996 non-null float64
ps_ind_03         595212 non-null int64
ps_ind_04_cat     595129 non-null float64
ps_ind_05_cat     589403 non-null float64
ps_ind_06_bin     595212 non-null int64
ps_ind_07_bin     595212 non-null int64
ps_ind_08_bin     595212 non-null int64
ps_ind_09_bin     595212 non-null int64
ps_ind_10_bin     595212 non-null int64
ps_ind_11_bin     595212 non-null int64
ps_ind_12_bin     595212 non-null int64
ps_ind_13_bin     595212 non-null int64
ps_ind_14         595212 non-null int64
ps_ind_15         595212 non-null int64
ps_ind_16_bin     595212 non-null int64
ps_ind_17_bin     595212 non-null int64
ps_ind_18_bin     595212 non-null int64
ps_reg_01         595212 non-null float64
ps_reg_02         595212 non-null float64
ps_reg_03         487440 non-null float64
ps_car_01_cat     595105 non-null float64
ps_car_02_cat     595212 non-null int64
ps_car_03_cat     595212 non-null int64
ps_car_04_cat     595212 non-null int64
ps_car_05_cat     595212 non-null int64
ps_car_06_cat     595212 non-null int64
ps_car_07_cat     595212 non-null int64
ps_car_08_cat     595212 non-null int64
ps_car_09_cat     594643 non-null float64
ps_car_10_cat     595212 non-null int64
ps_car_11_cat     595212 non-null int64
ps_car_11         595207 non-null float64
ps_car_12         595211 non-null float64
ps_car_13         595212 non-null float64
ps_car_14         552592 non-null float64
ps_car_15         595212 non-null float64
ps_calc_01        595212 non-null float64
ps_calc_02        595212 non-null float64
ps_calc_03        595212 non-null float64
ps_calc_04        595212 non-null int64
ps_calc_05        595212 non-null int64
ps_calc_06        595212 non-null int64
ps_calc_07        595212 non-null int64
ps_calc_08        595212 non-null int64
ps_calc_09        595212 non-null int64
ps_calc_10        595212 non-null int64
ps_calc_11        595212 non-null int64
ps_calc_12        595212 non-null int64
ps_calc_13        595212 non-null int64
ps_calc_14        595212 non-null int64
ps_calc_15_bin    595212 non-null int64
ps_calc_16_bin    595212 non-null int64
ps_calc_17_bin    595212 non-null int64
ps_calc_18_bin    595212 non-null int64
ps_calc_19_bin    595212 non-null int64
ps_calc_20_bin    595212 non-null int64
dtypes: float64(16), int64(41)
memory usage: 258.8 MB
</code></pre>
<pre><code class="language-python">#Import relevant packages
#MinMaxScaler to normalize the features
from sklearn.preprocessing import MinMaxScaler
#SimpleImputer to impute the median values in place of missing values
from sklearn.impute import SimpleImputer
#Pipeline to create transformation pipelines to process the data
from sklearn.pipeline import Pipeline
</code></pre>
<pre><code class="language-python">#Pipeline for the features that we treat as numerical
num_pipeline = Pipeline([
    #Impute missing values with the median
    ('imputer', SimpleImputer(strategy = &quot;median&quot;)),
    #Standardise values to lie in the range [0,1]
    ('min_max_scaler', MinMaxScaler())
])
</code></pre>
<p>Now consider the binary features, which will become multinomial with three levels, once we specify the missing values as a new factor level. After converting these features to multinomial, we apply One Hot Encoding. This creates one binary attribute per category, adding two extra columns for each of these features to the feature matrix.</p>
<pre><code class="language-python">#extract binary features
cat_attributes = [&quot;ps_ind_06_bin&quot;, &quot;ps_ind_07_bin&quot;, &quot;ps_ind_08_bin&quot;, &quot;ps_ind_09_bin&quot;, &quot;ps_ind_10_bin&quot;, &quot;ps_ind_11_bin&quot;, 
                  &quot;ps_ind_12_bin&quot;, &quot;ps_ind_13_bin&quot;, &quot;ps_ind_16_bin&quot;, &quot;ps_ind_17_bin&quot;, &quot;ps_ind_18_bin&quot;, &quot;ps_car_02_cat&quot;, 
                  &quot;ps_car_03_cat&quot;, &quot;ps_car_05_cat&quot;, &quot;ps_car_07_cat&quot;, &quot;ps_car_08_cat&quot;, &quot;ps_calc_15_bin&quot;, &quot;ps_calc_16_bin&quot;, 
                  &quot;ps_calc_17_bin&quot;, &quot;ps_calc_18_bin&quot;, &quot;ps_calc_19_bin&quot;, &quot;ps_calc_20_bin&quot;]
</code></pre>
<pre><code class="language-python">#Replace -1 with 2 as the OneHotEncoder can't work for negative integers
training_features[cat_attributes] = training_features[cat_attributes].replace(-1,2)
</code></pre>
<pre><code class="language-python">#apply OneHotEncoding again, now for the test data
test_data[cat_attributes] = test_data[cat_attributes].replace(-1,2)
</code></pre>
<pre><code class="language-python">from sklearn.preprocessing import OneHotEncoder
</code></pre>
<pre><code class="language-python">from sklearn.compose import ColumnTransformer
full_pipeline = ColumnTransformer([
    #pipeline for numerical features
    (&quot;num&quot;, num_pipeline, num_attributes),
    #OneHotEncoder for (previously) binary features
    (&quot;cat&quot;, OneHotEncoder(), cat_attributes)
])
</code></pre>
<pre><code class="language-python">#use pipeline to prepare training data
training_prepared = full_pipeline.fit_transform(training_features)
</code></pre>
<pre><code>C:\Users\User\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.
If you want the future behaviour and silence this warning, you can specify &quot;categories='auto'&quot;.
In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.
  warnings.warn(msg, FutureWarning)
</code></pre>
<pre><code class="language-python">#transform the test data based on the scaling estimated from the training data
test_prepared = full_pipeline.transform(test_data)
</code></pre>
<h3 id="5-the-confusion-matrix-and-roc-curve-as-measures-of-model-performance">5) The Confusion Matrix and ROC Curve as Measures of Model Performance</h3>
<p>It is important, before carrying out the model selection procedure, to consider an appropriate measure of model performance. This avoids biases that may occur when selecting a performance measure after different models have been fit.</p>
<p>In the given situation, using accuracy as a measure of performance is not suitable, because all the models fitted predict very low probabilities that individuals will make a claim and hence using the standard threshold probability of 0.5, always predict that individuals will not make a claim (i.e. always predict a negative). This is caused by the low rate of individuals making a claim within a year (just 3.64%) and the lack of information contained within the features relating to the binary target.</p>
<pre><code class="language-python">#Proportion of positive and negative target values ('claim' Vs 'no claim') in the training data
print(&quot;Proportion of individuals who didn't make a claim:&quot;, round((573518 / (21694 + 573518)), 4))
print(&quot;Proportion of individuals who made a claim:&quot;, round((21694 / (21694 + 573518)), 4))
</code></pre>
<pre><code>Proportion of individuals who didn't make a claim: 0.9636
Proportion of individuals who made a claim: 0.0364
</code></pre>
<p>Accuracy also ignores the types of errors that are made. In the context of automotive insurance, incorrectly predicting that an individual won&rsquo;t claim when they actually do (false negatives) will be more costly (financially) to Porto Seguro than incorrectly predicting that an individual will claim and then they actually don&rsquo;t (false positives). Hence, more emphasis should be placed on reducing the false negative rate. The ROC curves of different models and the ROC AUC scores can be compared to assess model performance in this manner. The ROC curve plots the true positive rate against the falser postive rate. The true postive rate (recall) takes into account false negatives, as it is the ratio of positive instances that are correctly detected by the classifier:</p>
<p>$$Recall = True Positive Rate = \frac{TP}{TP + FN}$$</p>
<p>Thus, if the number of false negatives increases, the recall will fall. The ROC curve also allows us to assess the trade-off between the true positive rate (recall) and the false positive rate; the higher the recall, the higher the false positive rate will be, on average. That is, if the recall rises, this may be due to the fact we are always predicting a positive outcome, however conequently this will lead to an increase in the number of false positive that are predicted. Thus, this measure of performance enables us to assess this trade-off and incorporate the problems of false negative classification. The Area Under the Curve (AUC) provides a quantitative measure of performance between models, calculating the area under the ROC curve for a given model. A higher AUC score indicates a better performing model.</p>
<h2 id="5-selection-and-training-of-models">5) Selection and Training of Models</h2>
<p>The No Free Lunch Theorem states that if we make absolutely no assumptions about the data, then there is no reason to prefer one model over another. The only way to know for sure is to evaluate them all (Geron, 2019). However, in practice we can make a few reasonable assumptions about the data. the Porto Seguro automotive insurance data provides a range of features; categorical, numeric and binary. We have shown that most of the variables have little correlation with the binary target variable of &lsquo;claim&rsquo; or &lsquo;no claim&rsquo;. This suggests that a more complex model, such as a neural network may be beneficial to make predictions for datasets that are not close to being linearly separable. However, given the large number of features in relation to the size of the data, we expect that neural networks may not achieve their &lsquo;potential&rsquo; as they may overfit the data quite quickly. This suggests that more simple methods for classification, such as logistic regression may also perform relatively well when making predictions on unseen data. Finally, we fit a Random Forest classification model to the training data and again assess its performance using cross-validation. Random Forest models are Ensemble Methods that average out the predictions of decision trees and introduce extra randomness when growing trees, resulting in extra tree diversity (different features are used to split nodes, not necessarily the optimal features) that leads to an increase in bias and a reduced variance. This may enable random forest models to generalize better to unseen data.</p>
<h3 id="cross-validation">Cross-Validation</h3>
<p>K-Fold Cross Validation is a common way to evaluate different models (and with different hyperparameters). This process splits the training data into k folds (here k = 3), where (k-1) of the folds are used to train the model, the performance of which is then assessed by comparing its predictions to the target values of the unused fold, which acts as a validation set. This ensures the model is assessed on instances that were previously unseen when fitting the model.</p>
<p>Cross-Validation allows one to assess potential issues of overfitting and underfitting across models and provides an estimate of how each model will generalize to an independent sample of unseen data. Using this approach, we can train multiple models using different hyperparameters on the reduced training set and select the model that performs best on average for each of the k validation folds.</p>
<p>All models discussed are types of supervised machine learning; the training set fed into the algorithm includes the desired solutions (binary classifications) called labels.</p>
<h3 id="a-logistic-regression">a) Logistic Regression</h3>
<p>Logistic regression can be used to estimate the probability that a given instance belongs to one of the binary classes. In a similar manner to linear regression, logistic regression computes a weighted sum of input features, plus a bias term. However, it then uses the logistic function to output a number (probability) between 0 and 1. The weights used in the regression are estimated by minimizing the log loss function:</p>
<p>$$J(\theta) = \frac{-1}{m}\sum_{i=1}^{m}[ y^i \log(p^i) + (1 - y^i) \log(1 - p^i)]$$</p>
<p>That is, when the true value (label) for a given instance is 1 (&lsquo;claim&rsquo;, i.e. $y^i = 1$), the loss for individual i becomes $\log(p^i)$. Hence, a higher predicted probability leads to a reduced loss in this case and when $p^i = 1$, $\log(p^i) =0$ and the loss or cost for instance i is zero. Hence, on average, the optimal solution will predict low probabilities for instances/ individuals that did not make a claim and high probabilities for those that did make a claim, using the input features, to minimize this loss. Logistic Regression can be used for binary classification by specifying a threshold probability, beyond which an instance is predicted to make a claim.</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(random_state = 160001695)
</code></pre>
<pre><code class="language-python">from sklearn.model_selection import cross_val_predict
</code></pre>
<p>We now use cross-validation to train the model on</p>
<pre><code class="language-python">#extract the predicted probabilities for the k validation sets
y_train_pred = cross_val_predict(log_reg, training_prepared, training_labels, cv = 3,  method = &quot;predict_proba&quot;)
</code></pre>
<pre><code>C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
</code></pre>
<pre><code class="language-python">#predicted values of a 'claim' (target = 1)
logistic_predictions = y_train_pred[:,1]
</code></pre>
<p>Now we can look at the ROC curve to assess the performance of the model</p>
<pre><code class="language-python">from sklearn.metrics import roc_curve

#false positve rate, true positive rate, probability thresholds
fpr, tpr, thresholds = roc_curve(training_labels, logistic_predictions)
</code></pre>
<pre><code class="language-python">#Create a function to plot the ROC curve
def plot_roc_curve(fpr, tpr, label = None):
    #plot roc curve for fitted model
    plt.plot(fpr, tpr, linewidth = 2, label = label)
    #Add line for random guess model
    plt.plot([0,1], [0,1], 'k--')
    plt.xlabel('False Positive Rate', fontsize = 14)
    plt.ylabel('True Positive Rate (Recall)', fontsize = 14)
    plt.grid(True)
    plt.title('ROC Curve', fontsize = 18)
</code></pre>
<pre><code class="language-python">plot_roc_curve(fpr, tpr, &quot;Logistic Regression&quot;)
plt.savefig(&quot;roc_curve1.png&quot;)
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_b93638acff3cfdede3f0617b6e32b6d7.png 400w,
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_ed91ecf3db106bf6cea6577323208c8c.png 760w,
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_b93638acff3cfdede3f0617b6e32b6d7.png"
               width="393"
               height="286"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>Now we assess the area under the curve (AUC) For the Receiver Operating Characteristic (ROC) Curve:</p>
<pre><code class="language-python">from sklearn.metrics import roc_auc_score
print(&quot;AUC Score for the Logistic Regression Model:&quot;, round(roc_auc_score(training_labels, logistic_predictions), 4))
</code></pre>
<pre><code>AUC Score for the Logistic Regression Model: 0.6215
</code></pre>
<h3 id="b-random-forest-model">b) Random Forest Model</h3>
<p>A Random Forest is an ensemble of decision trees, which makes predictions by taking a weighted average of the probabilities (in this case) of each tree. It also introduces extra randomness when growing trees; instead of always splitting a node by the best feature (as determined by the training set), it searches for the best feature among a random subset of the features. This may allow the model to improve its ability to generalize to new datasets, by raising its bias but reducing its variance in making predictions on new, unseen data.</p>
<p>Each individual decision tree that makes up the Random Forest is estimated using the CART Training Algorithm. This is a &lsquo;greedy&rsquo; algorithm, in that it greedily searches the optimal feature and threshold to split the data by, without considering whether this split will lead to the best gini impurity several levels down. Thus, this model is often considered as reasonable but not optimal. Regularization for the tree can simply be controlled via the maximum depth of each decision tree, which here is set to 3.</p>
<p>Use Scikit-Learn&rsquo;s SGD classifier as a method for binary classification</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state = 160001695, max_depth = 3)
#extract predicted probabilities for the k validation sets
y_probas_forest = cross_val_predict(forest_clf, training_prepared, training_labels, cv = 3, method = &quot;predict_proba&quot;)
</code></pre>
<pre><code>C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
</code></pre>
<pre><code class="language-python">#false positive rate, true positive rate, probability thresholds
fpr_rf, tpr_rf, thresholds_rf = roc_curve(training_labels, y_probas_forest[:,1])
</code></pre>
<pre><code class="language-python">#Plot the ROC curves for logistic and random forest models
plt.plot(fpr, tpr, &quot;b:&quot;, label = &quot;Logistic Regression&quot;)
plot_roc_curve(fpr_rf, tpr_rf, &quot;Random Forest&quot;)
plt.legend(loc = &quot;lower right&quot;)
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_76314540f2730bd33fef959b2a72074f.png 400w,
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_94c2e9fc29e6ac95ee03c1752e65c5f7.png 760w,
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_76314540f2730bd33fef959b2a72074f.png"
               width="393"
               height="286"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<pre><code class="language-python">print(&quot;AUC Score for the Random Forest Model:&quot;, round(roc_auc_score(training_labels, y_probas_forest[:,1]), 4))
</code></pre>
<pre><code>AUC Score for the Random Forest Model: 0.6036
</code></pre>
<p>We see that the performance of the Random Forest Regressor is not as good as that of the Logistic Regression model, as measured by the ROC curve and AUC score. In particular, the ROC curve for the Random Forest model lies under that of the Logistic Regression model for all thresholds. Hence, for a given false positive rate, the true positive rate for the random forest model is smaller, suggesting it is underestimating probabilities for those individuals that made a claim. This information is confirmed by the lower AUC score of 0.6036.</p>
<h3 id="c-neural-network">c) Neural Network</h3>
<p>Multilayer perceptrons can be used to create a classification model for this task. For a binary classification problem, we require a single output neuron using the logistic activation function (Geron, 2019). The output will be a number between zero and one, which can be interpreted as the estimated probability of the positive class (making a claim).</p>
<p>A perceptron with a single layer and single neuron (i.e. a threshold logic unit, TLU) is equivalent to a logistic regression model, as it computes a linear combination of feature inputs and uses the sigmoid function to transform these weighted linear combinations into a probability lying in the interval (0,1). Multilayer perceptrons are composed of an input layer, one or more layers of TLU&rsquo;s, called hidden layers and the output layer. Every layer except the outer layer contains a bias neuron and is fully connected to the next layer. Hence, by adding more layers, we hope to train a model that improves upon the Logistic Regression model in terms of its predictive performance as measured by AUC.</p>
<p>The model is trained using backpropagation. This works by splitting the input data used to train the model into &lsquo;mini-batches&rsquo;. Each mini-batch of data is passed through the model and the network&rsquo;s output error is measured by a loss function, in this case for the sigmoid function, which is the same as that of the logistic regression loss function:</p>
<p>$$J(\theta) = \frac{-1}{m}\sum_{i=1}^{m}[ y^i \log(p^i) + (1 - y^i) \log(1 - p^i)]$$</p>
<p>where m is the number of instances in the mini-batch fed in to the neural network. Then the contribution to this error of the outputs from each neuron is computed. This is a reverse pass, and measures the error gradient backward through the network. The connection weights for each neuron are then corrected using Gradient Descent to reduce the error. One cycle of a forward and reverse pass is known as an epoch.</p>
<pre><code class="language-python">#import tensorflow and keras for neural networks
import tensorflow as tf
from tensorflow import keras
</code></pre>
<p>We first fit a Neural Network with two hidden layers and ten neurons per layer, using the default learning rate. Later these hyperparameters are tuned to search for a better model as assessed by AUC.</p>
<pre><code class="language-python">Neural_Net = keras.models.Sequential([
    #first hidden layer
    keras.layers.Dense(10, activation = &quot;relu&quot;, input_shape = training_prepared.shape[1:]),
    #second hidden layer
    keras.layers.Dense(10, activation = &quot;relu&quot;),
    #output neuron
    keras.layers.Dense(1, activation = &quot;sigmoid&quot;)
])    
</code></pre>
<pre><code class="language-python">#Compile model
Neural_Net.compile(loss = &quot;binary_crossentropy&quot;, 
                  optimizer = &quot;sgd&quot;, metrics = [&quot;accuracy&quot;])
</code></pre>
<p>We now create a function that carries out K-fold cross-validation on the Neural Net model. This is customized to calculate the AUC score as the measure of performance for each fold and then the AUC score is later averaged across the folds to provide a final score for the model.</p>
<pre><code class="language-python">#KFold function splits data into k consecutive folds
from sklearn.model_selection import KFold
</code></pre>
<p>Each of the three Neural Nets is trained with just 3 epochs to reduce computational time and allow for a quicker assessment of models:</p>
<pre><code class="language-python">#Carry out K-Fold cross-validation with k = 3 folds by default
def Neural_Net_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 3):
    #empty array to store predictions
    NN_pred = np.zeros(len(training_labels))
    for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared):
        #split data into training and test set
        x_train, x_test=training_prepared[train_index], training_prepared[test_index]
        #split corresponding test and training labels
        y_train, y_test=training_labels[train_index], training_labels[test_index]
        #Fit model on training set
        Neural_Net.fit(x_train, y_train,epochs= epochs)
        #Make predictions on test test
        NN_pred = Neural_Net.predict(x_test)
        #Calculate AUC score for the kth fold
        print(&quot;AUC cross-validation score: &quot;, round(roc_auc_score(y_test, NN_pred), 5))
</code></pre>
<pre><code class="language-python">#Carry out cross-validation on the model
Neural_Net_CrossVal(Neural_Net, k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 72s 180us/sample - loss: 0.1558 - accuracy: 0.9637
Epoch 2/3
396808/396808 [==============================] - 68s 172us/sample - loss: 0.1542 - accuracy: 0.9637
Epoch 3/3
396808/396808 [==============================] - 72s 182us/sample - loss: 0.1538 - accuracy: 0.9637
AUC cross-validation score:  0.60311
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 66s 167us/sample - loss: 0.1545 - accuracy: 0.9634
Epoch 2/3
396808/396808 [==============================] - 63s 160us/sample - loss: 0.1542 - accuracy: 0.9634
Epoch 3/3
396808/396808 [==============================] - 71s 180us/sample - loss: 0.1541 - accuracy: 0.9634- loss: 0.1541 - ac
AUC cross-validation score:  0.61718
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1534 - accuracy: 0.9636
Epoch 2/3
396808/396808 [==============================] - 66s 166us/sample - loss: 0.1533 - accuracy: 0.9636
Epoch 3/3
396808/396808 [==============================] - 65s 164us/sample - loss: 0.1532 - accuracy: 0.9636
AUC cross-validation score:  0.61913
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, (0.60311 + 0.61718 + 0.61913) / 3)
</code></pre>
<pre><code>Average AUC score 0.6131399999999999
</code></pre>
<p>Therefore, we see that the average AUC score for the neural network model (0.61314) is slightly lower than that of the logistic regression model (0.6215). We can also see that the accuracy is roughly equal to the proportion of individuals in the training set who made a claim. As the predicted probabilities are so low, all individuals are predicted as not claiming and hence the accuracy is a useless measure of model performance in this case.</p>
<h2 id="7-fine-tune-models">7) Fine-Tune Models</h2>
<p>Given the results of the AUC scores, we now only consider hyperparameter tuning of the Neural Net as there is evidence that this model may provide the best predictions on unseen data if the hyperparameters are tuned well enough to improve model performance beyond that of the logistic regression model. Because the AUC score is not a standard metric in keras, we use the cross-validation function and manually assess models with different hyperparameters, instead of using grid search. The first neural network model that we fitted had two hidden layers, each with 10 neurons and used the default learning rate of 0.001.</p>
<p>In order to assess the performance of the neural network binary classifier, we use the custom cross validation function that calculates the AUC score for each fold for each set of hyperparameters. By testing on the unseen fold, an unbiased assessment of how each model might generalize to new data can be carried out.</p>
<p>There are several different hyperparameters for the neural network that we consider:</p>
<p><em><strong>a) Number of Hidden Layers</strong></em></p>
<p>For many problems, a single hidden layer can provide reasonable results. However, given the (relatively) complex nature of this problem, with a large number of fairly uninformative features, adding more layers may improve the predictive ability of a neural network.</p>
<p><em><strong>b) Number of Neurons per hidden layer</strong></em></p>
<p>As the number of neurons per layer increases, we expect the predictive power of the model to increase. However, as with the number of hidden layers, overfitting may become an issue if too many neurons are added per layer, so cross-validation can be used to find an optimal trade-off of bias and variance of the probability predictions.</p>
<p><em><strong>c) Learning Rate</strong></em></p>
<p>This hyperparameter determines the size of steps that the gradient descent takes when minimizing the sigmoid cost function. The lower the learning rate, the more iterations the gradient descent algorithm will take to converge. However, if the learning rate is too high, the algorithm may overshoot and diverge. The learning rate is one of the most influential hyperparameters in fitting a Neural network, so we consider three different values and compare their performance to the original learning rate of 1e-3.</p>
<p>We now create a function that can build simple Sequential Neural Networks for a given set of input values:</p>
<pre><code class="language-python">def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3):
    #Define the keras model
    model = keras.models.Sequential()
    #Add an input layer
    model.add(keras.layers.InputLayer(input_shape = training_prepared.shape[1:]))
    for layer in range(n_hidden):
        #Assume each hidden layer contains the same number of neurons
        model.add(keras.layers.Dense(n_neurons, activation = &quot;relu&quot;))
    #Add the output layer
    model.add(keras.layers.Dense(1, activation = &quot;sigmoid&quot;))
    #Use SGD for backpropagation
    optimizer = keras.optimizers.SGD(lr = learning_rate)
    #compile model
    model.compile(loss = &quot;binary_crossentropy&quot;, optimizer = optimizer)
    return model
</code></pre>
<h3 id="a-number-of-hidden-layers">a) Number of Hidden Layers</h3>
<p>We use the customized cross-validation function to assess the performance of the original neural network model but instead with 1 and 3 hidden layers.</p>
<pre><code class="language-python">#Then manually compare each model by considering cross-validation
Neural_Net_CrossVal(build_model(n_hidden = 1, n_neurons = 10), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1757
Epoch 2/3
396808/396808 [==============================] - 57s 142us/sample - loss: 0.1552
Epoch 3/3
396808/396808 [==============================] - 69s 175us/sample - loss: 0.1546
AUC cross-validation score:  0.58507
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1552
Epoch 2/3
396808/396808 [==============================] - 57s 143us/sample - loss: 0.1550
Epoch 3/3
396808/396808 [==============================] - 56s 142us/sample - loss: 0.1548
AUC cross-validation score:  0.59959
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 53s 132us/sample - loss: 0.1540
Epoch 2/3
396808/396808 [==============================] - 66s 166us/sample - loss: 0.1539
Epoch 3/3
396808/396808 [==============================] - 63s 158us/sample - loss: 0.1538
AUC cross-validation score:  0.60671
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.58507 + 0.59959 + 0.60671) / 3), 4))
</code></pre>
<pre><code>Average AUC score 0.5971
</code></pre>
<pre><code class="language-python">Neural_Net_CrossVal(build_model(n_hidden = 3, n_neurons = 10), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 71s 178us/sample - loss: 0.1575
Epoch 2/3
396808/396808 [==============================] - 65s 164us/sample - loss: 0.1548
Epoch 3/3
396808/396808 [==============================] - 55s 139us/sample - loss: 0.1542
AUC cross-validation score:  0.59475
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 55s 138us/sample - loss: 0.1548
Epoch 2/3
396808/396808 [==============================] - 54s 136us/sample - loss: 0.1546
Epoch 3/3
396808/396808 [==============================] - 56s 140us/sample - loss: 0.1545
AUC cross-validation score:  0.60568
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 149us/sample - loss: 0.1537
Epoch 2/3
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1536
Epoch 3/3
396808/396808 [==============================] - 57s 144us/sample - loss: 0.1535
AUC cross-validation score:  0.6113
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.59475 + 0.60568 + 0.6113) / 3),4))
</code></pre>
<pre><code>Average AUC score 0.6039
</code></pre>
<p>We can see that the neural network might be suffering from issues of overfitting for 3 hidden layers and underfitting for 1 hidden layer, as reflected by the lower AUC scores in comparison to the original Neural Network model with just 1 hidden layer.</p>
<h3 id="b-number-of-neurons-per-hidden-layer">b) Number of Neurons per Hidden Layer</h3>
<p>Again apply the custom cross-validation function to models with 5 and 15 neurons in each of the two hidden layers.</p>
<pre><code class="language-python">Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 5), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1595
Epoch 2/3
396808/396808 [==============================] - 56s 141us/sample - loss: 0.1556
Epoch 3/3
396808/396808 [==============================] - 55s 140us/sample - loss: 0.1548
AUC cross-validation score:  0.57591
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1554
Epoch 2/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1551
Epoch 3/3
396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 56s 141us/sample - loss: 0.1549
AUC cross-validation score:  0.59693
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1542
Epoch 2/3
396808/396808 [==============================] - 60s 150us/sample - loss: 0.1540
Epoch 3/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1539
AUC cross-validation score:  0.60563
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.57591 + 0.59693 + 0.60563) / 3),4))
</code></pre>
<pre><code>Average AUC score 0.5928
</code></pre>
<pre><code class="language-python">Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 15), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1587
Epoch 2/3
396808/396808 [==============================] - 56s 142us/sample - loss: 0.1545
Epoch 3/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1540
AUC cross-validation score:  0.59848
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1547
Epoch 2/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1545
Epoch 3/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1543
AUC cross-validation score:  0.60942
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1536
Epoch 2/3
396808/396808 [==============================] - 61s 155us/sample - loss: 0.1535
Epoch 3/3
396808/396808 [==============================] - 62s 157us/sample - loss: 0.1534
AUC cross-validation score:  0.61443
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.59848 + 0.60942 + 0.61443) / 3), 4))
</code></pre>
<pre><code>Average AUC score 0.6074
</code></pre>
<p>Similarly, the AUC scores for 5 and 15 neurons per hidden layer are lower than the AUC score for 10 hidden layers.</p>
<h3 id="c-learning-rate">c) Learning Rate</h3>
<p>Finally, we compare the sensitivity of the AUC score for the models under cross-validation for different values of the learning rate (1e-4 and 1e-2).</p>
<pre><code class="language-python">Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-4), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 61s 154us/sample - loss: 0.2657
Epoch 2/3
396808/396808 [==============================] - 58s 145us/sample - loss: 0.1578
Epoch 3/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1569
AUC cross-validation score:  0.53327
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1577
Epoch 2/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1576
Epoch 3/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1575
AUC cross-validation score:  0.54585
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1568
Epoch 2/3
396808/396808 [==============================] - 61s 153us/sample - loss: 0.1567
Epoch 3/3
396808/396808 [==============================] - 61s 154us/sample - loss: 0.1566 
AUC cross-validation score:  0.55127
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.53327 + 0.54585 + 0.55127) / 3), 4))
</code></pre>
<pre><code>Average AUC score 0.5435
</code></pre>
<pre><code class="language-python">Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2), k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1560 - loss: 
Epoch 2/3
396808/396808 [==============================] - 63s 158us/sample - loss: 0.1539
Epoch 3/3
396808/396808 [==============================] - 64s 161us/sample - loss: 0.1535
AUC cross-validation score:  0.61233
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 60s 151us/sample - loss: 0.1542
Epoch 2/3
396808/396808 [==============================] - 64s 162us/sample - loss: 0.1540
Epoch 3/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1539
AUC cross-validation score:  0.61899
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1533
Epoch 2/3
396808/396808 [==============================] - 72s 180us/sample - loss: 0.1532
Epoch 3/3
396808/396808 [==============================] - 71s 178us/sample - loss: 0.1532
AUC cross-validation score:  0.61907
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.61233 + 0.61899 + 0.61907) / 3), 4))
</code></pre>
<pre><code>Average AUC score 0.6168
</code></pre>
<p>The results of this hyperparameter tuning support a model with two hidden layers, ten neurons per hidden layer and a learning rate of 1e-2 as the best trialled model, using the AUC score as a measure of performance, with an average AUC of 0.61689.</p>
<h2 id="8-assessing-model-performance">8) Assessing Model Performance</h2>
<p>We now fit the updated Neural Network model with a learning rate of 1e-2, which was chosen as the optimal model with the highest AUC score but with 5 epochs to ensure convergence to the optimal weights at each layer (we would have used more epochs but that was too time consuming for this PC). The fitted model is then used to make predictions on the training set via cross-validation and the ROC curve is plotted to get a more accurate assessment of the model&rsquo;s performance.</p>
<pre><code class="language-python">#Final model with 2 hidden layers, 10 neurons per hidden layer and a learning rate of 0.01
NN_final = build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2)
</code></pre>
<pre><code class="language-python">#Compile Model
NN_final.compile(loss = &quot;binary_crossentropy&quot;, 
                  optimizer = &quot;sgd&quot;, metrics = [&quot;accuracy&quot;])
#Fit model to training data
NN_final.fit(training_prepared, training_labels, epochs = 5)
#Extract predicted probabilities on the test set
y_pred = NN_final.predict(test_prepared)
</code></pre>
<pre><code>Train on 595212 samples
Epoch 1/5
595212/595212 [==============================] - 91s 154us/sample - loss: 0.1541 - accuracy: 0.9636
Epoch 2/5
595212/595212 [==============================] - 89s 150us/sample - loss: 0.1537 - accuracy: 0.9636
Epoch 3/5
595212/595212 [==============================] - 93s 156us/sample - loss: 0.1536 - accuracy: 0.9636
Epoch 4/5
595212/595212 [==============================] - 90s 152us/sample - loss: 0.1535 - accuracy: 0.9636
Epoch 5/5
595212/595212 [==============================] - 90s 151us/sample - loss: 0.1534 - accuracy: 0.9636
</code></pre>
<p>We now calculate the AUC score for the final model, however this time we train each fold on 5 epochs to ensure convergence to the optimal weights at each layer. We then average the AUC score for each fold to estimate the overall AUC score.</p>
<pre><code class="language-python">def Neural_Net_Final_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 5):
    #empty array to store predictions
    NN_pred = np.zeros(len(training_labels))
    for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared):
        #split data into training and test set
        x_train, x_test=training_prepared[train_index], training_prepared[test_index]
        #split corresponding test and training labels
        y_train, y_test=training_labels[train_index], training_labels[test_index]
        #Fit model on training set
        Neural_Net.fit(x_train, y_train,epochs= epochs)
        #Make predictions on test test
        NN_pred = Neural_Net.predict(x_test)
        #Calculate AUC score for the kth fold
        print(&quot;AUC cross-validation score: &quot;, round(roc_auc_score(y_test, NN_pred), 5))
</code></pre>
<pre><code class="language-python">Neural_Net_Final_CrossVal(NN_final, k = 3)
</code></pre>
<pre><code>Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637
Epoch 2/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637
Epoch 3/5
396808/396808 [==============================] - 59s 149us/sample - loss: 0.1529 - accuracy: 0.9637
Epoch 4/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1529 - accuracy: 0.9637
Epoch 5/5
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1529 - accuracy: 0.9637
AUC cross-validation score:  0.62041
Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 60s 150us/sample - loss: 0.1538 - accuracy: 0.9634
Epoch 2/5
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 3/5
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 4/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 5/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634
AUC cross-validation score:  0.6219
Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1531 - accuracy: 0.9636
Epoch 2/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 3/5
396808/396808 [==============================] - 61s 153us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 4/5
396808/396808 [==============================] - 63s 159us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 5/5
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1530 - accuracy: 0.9636
AUC cross-validation score:  0.62102
</code></pre>
<pre><code class="language-python">print(&quot;Average AUC score&quot;, round(((0.62041 + 0.62102 + 0.6219) / 3), 4))
</code></pre>
<pre><code>Average AUC score 0.6211
</code></pre>
<p>Whilst the AUC score provides a formal metric to assess the performance of each model, we may also consider plotting the ROC curve for the fitted Neural Network model and compare it to the ROC curves under Logistic Regression and the Random Forest classifier. Unfortunately, there is no easy way to extract all predictions from the Cross-Validation carried out, so we instead split the training data randomly into a training and validation set purely for the purpose of plotting a ROC curve to roughly illustrate the differences in performance between the different models. To do this, we randomly shuffle the indices of the data and split the test and training set into 80% training set and 20% validation set. We then approximate the true ROC curve using predictions solely on the validation set.</p>
<pre><code class="language-python">import random
#Set random seed for reproducibility
random.seed(160001695)
#shuffle indices of full training set
shuffled_indices = np.random.permutation(len(training_prepared))
#validation set is 20% of full training data
validation_set_size = int(len(training_prepared) * 0.2)
#extract the validation indices
validation_indices = shuffled_indices[:validation_set_size]
#extract the training indices
train_indices = shuffled_indices[validation_set_size:]
#create training set
train_set = training_prepared[train_indices]
#create validation set
validation_set = training_prepared[validation_indices]
</code></pre>
<pre><code class="language-python">#Train on the training data and predict for the validation data
NN_fit_ROC = NN_final.fit(train_set, training_labels[train_indices], epochs = 5)
NN_pred_ROC = NN_final.predict(validation_set)
</code></pre>
<pre><code>Train on 476170 samples
Epoch 1/5
476170/476170 [==============================] - 83s 175us/sample - loss: 0.1533 - accuracy: 0.9635
Epoch 2/5
476170/476170 [==============================] - 80s 167us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 3/5
476170/476170 [==============================] - 70s 146us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 4/5
476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 5/5
476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635
</code></pre>
<pre><code class="language-python">#Extract the false positive rate, true positive rate and thresholds for the validation set predictions
fpr_NN, tpr_NN, thresholds_NN = roc_curve(training_labels[validation_indices], NN_pred_ROC[:, 0])
</code></pre>
<pre><code class="language-python">#Plot the ROC curve for all three different models assessed
plt.plot(fpr, tpr, &quot;b:&quot;, label = &quot;Logistic Regression&quot;)
plot_roc_curve(fpr_rf, tpr_rf, &quot;Random Forest&quot;)
plot_roc_curve(fpr_NN, tpr_NN, &quot;Tuned Neural Network&quot;)
plt.legend(loc = &quot;lower right&quot;)
plt.savefig(&quot;roc_curve.png&quot;)
plt.show()
</code></pre>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt="png" srcset="
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_447a0ddd472adf9489cd47152c4a00f6.png 400w,
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_7cb1bcaa309cbe6951b469d350e2c649.png 760w,
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_1200x1200_fit_lanczos_3.png 1200w"
               src="/project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_447a0ddd472adf9489cd47152c4a00f6.png"
               width="393"
               height="286"
               loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The above ROC curve illustrates that the tuned Neural Network model tends to outperform the other fitted models for the majority of threshold values, when estimating its probabilities on the randomly sampled validation set. Despite having a slightly lower (but almost identical) AUC score of 0.6211 to that of the Logistic Regression model, the ROC curve seems to slightly favour the Tuned Neural Network model and hence we use this model as the final model to make predictions on the test data for upload to Kaggle (Also increasing the number of epochs to 20 for example would have likely raised the AUC Score for this model).</p>
<p>[Note- both the AUC score and ROC Curves are dependent on how the data is split for each fold in the cross-validation process and this splitting is done randomly. Therefore, it is not contradictory that the ROC curve for the Tuned Neural Network is generally higher than that of the Logistic Regression model, despite the fact that the Logistic Regression model had a larger AUC score. This is because the AUC was calculated on a different set of sampled cross-validations. Essentially we have two seemingly equally performing models and in this case we choose the Neural Network. It may or may not generalize as well to the actual test data]</p>
<p>We now use the following code to upload the best model (Neural network with 2 hidden layers, 10 nodes per hidden layer and a learning rate of 1e-2) to kaggle:</p>
<pre><code class="language-python">#use fitted model to predict for test data
target_NN = NN_final.predict(test_prepared)
</code></pre>
<pre><code class="language-python">#extract probalities for 'claim'
target_NN = target_NN[:, 0]
</code></pre>
<pre><code class="language-python">#Extract identity column
identity = test_data[&quot;id&quot;]
#Prepare the data into 2 columns for submission
submission = pd.DataFrame({'id': identity, 'target': target_NN})
filename = 'Porto Seguro Predictions.csv'
#Convert file to csv for Kaggle submission
submission.to_csv(filename,index=False)
</code></pre>
<h2 id="9-conclusion">9) Conclusion</h2>
<p>In this project, we have assessed the performance of three models that could be used for binary classification for the Porto Seguro automotive insurance data. The three assessed models are: Logistic Regression, Random Forest Classifier and a Multi-Layer Perceptron Neural Network. The ROC curve and AUC score were used as measures of performance, as these take into account the false negative rate that could be more important for economic reasons to this insurance company. Cross-validation was then used to assess the performance of the different models, allowing an unbiased calculation of the AUC score by training a model and validating on a previously unseen kth fold of the data. The model that was chosen in the end was a Neural Network Multi-Layer Perceptron with two hidden layers, ten neurons per layer and a learning rate of 1e-2. Its AUC score was 0.6211 using three folds and five training epochs.</p>
<p>There are further methods that could also be considered beyond the analysis presented here to improve the performance of these models. Primarily, the number of features could be reduced and this may improve the tuning of neural networks as they may suffer less from overfitting, hence more layers could be added to create a model that acts on the reduced set of features which contain the most information. Principal component analysis could also be used to condense some of the numerical and categorical variables into new features containing most of the information (variation) from the original features, which would again slow the onset of overfitting and lead to potentially more powerful neural networks.</p>
<h2 id="references">References</h2>
<p>Geron, A. (2019). &lsquo;Hands-On Machine Learning with Scikit-Learn, Keras &amp; Tensorflow&rsquo;, O&rsquo;Reilly Media.</p>
<p>Severance, C. (2016). &lsquo;Python for Everybody: Exploring Data in Python 3&rsquo;, CreateSpace Independent Publishing Platform.</p>
<p>Python Software Foundation (2016). Python Language Reference, version 2.3. URL: <a href="http://www.python.org" target="_blank" rel="noopener">http://www.python.org</a></p>
<p>RStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA. URL: <a href="http://www.rstudio.com/" target="_blank" rel="noopener">http://www.rstudio.com/</a>.</p>
<p>Kaggle.com (2017). Porto Seguro&rsquo;s Safe Driver Prediction Data. URL: <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data" target="_blank" rel="noopener">https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data</a></p>
<p>Kaggle.com (2017). Porto Seguro&rsquo;s Safe Driver Prediction- Welcome. URL: <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222" target="_blank" rel="noopener">https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222</a></p>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/other/">Other</a>
  
  <a class="badge badge-light" href="/tag/supervised-learning/">Supervised Learning</a>
  
  <a class="badge badge-light" href="/tag/python/">Python</a>
  
  <a class="badge badge-light" href="/tag/sklearn/">SKLearn</a>
  
  <a class="badge badge-light" href="/tag/keras/">Keras</a>
  
  <a class="badge badge-light" href="/tag/tensorflow/">Tensorflow</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://domscruton.github.io/project/carinsurance/&amp;text=Predicting%20Car%20Insurance%20Claims" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://domscruton.github.io/project/carinsurance/&amp;t=Predicting%20Car%20Insurance%20Claims" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Predicting%20Car%20Insurance%20Claims&amp;body=https://domscruton.github.io/project/carinsurance/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://domscruton.github.io/project/carinsurance/&amp;title=Predicting%20Car%20Insurance%20Claims" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Predicting%20Car%20Insurance%20Claims%20https://domscruton.github.io/project/carinsurance/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://domscruton.github.io/project/carinsurance/&amp;title=Predicting%20Car%20Insurance%20Claims" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://domscruton.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/dominic/avatar_hudf214ac2928f699b4fd4bbf2f7037a2a_94986_270x270_fill_q75_lanczos_center.jpg" alt="Dominic Scruton"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://domscruton.github.io/">Dominic Scruton</a></h5>
      <h6 class="card-subtitle">Data Scientist</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Domscruton" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://uk.linkedin.com/in/dominicscruton" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>
















  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/project/spotify/spotify/">Spotify PCA &amp; Cluster Analysis</a></li>
      
    </ul>
  </div>
  





    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      

      
      
      
    </div>
  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    © 2022 Dominic Scruton
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script>

    






</body>
</html>
