<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Dominic Scruton</title>
    <link>https://domscruton.github.io/project/</link>
      <atom:link href="https://domscruton.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Dominic Scruton</copyright><lastBuildDate>Mon, 01 Jun 2020 17:28:03 +0100</lastBuildDate>
    <image>
      <url>https://domscruton.github.io/media/icon_huf497c2b2f164bad8bc149e8f8dbb116c_2858_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://domscruton.github.io/project/</link>
    </image>
    
    <item>
      <title>Predicting Car Insurance Claims</title>
      <link>https://domscruton.github.io/project/carinsurance/</link>
      <pubDate>Mon, 01 Jun 2020 17:28:03 +0100</pubDate>
      <guid>https://domscruton.github.io/project/carinsurance/</guid>
      <description>&lt;h2 id=&#34;1-introduction--framing-the-problem&#34;&gt;1) Introduction- Framing the Problem&lt;/h2&gt;
&lt;p&gt;The objective of this assignment is to build a model that predicts the probability that individual motorists will initiate an auto insurance claim within the next year. The problem itself is one of classification, however the predicted probabilities of making a claim are of more interest, as often they are far below 0.5, suggesting that none of the models would classify any individuals as making a claim. This was due to only 3.64% of individuals in the training data making a claim and caused also by a lack of correlation between the target variable and most of the features, suggesting that the features lacked enough information to fully explain differences in the claims rate between individuals. The classification problem can be expressed as follows:&lt;/p&gt;
&lt;p&gt;$$y_i = f(X_i, \theta)$$&lt;/p&gt;
&lt;p&gt;In this case, the response, $y_i$, is the probability that individual i makes a claim within the next year. It could also be converted into a prediction (&amp;lsquo;claim&amp;rsquo; or &amp;lsquo;no claim&amp;rsquo;) by assigning a threshold; if the probability of claiming is greater than this threshold, the individual is predicted to make a claim, otherwise they are not predicted to make a claim. The goal of machine learning is to find and estimate a model, $f(X_i, \theta)$, that takes as its arguments the data for the ith instance ($X_i$) and the estimated parameter values ($\theta$), and provides the most accurate predictions on unseen data.&lt;/p&gt;
&lt;p&gt;In general, one aims to find the model that makes these best predictions, either by classifying the individuals into &amp;lsquo;claim&amp;rsquo; or &amp;lsquo;no claim&amp;rsquo; as accurately as possible, or providing probabilities that each individual will claim that are as close as possible to the true, underlying probabilites (which may be unknown). However, in this situation, it may also be prudent to consider measures of performance that take into account the economic implications of incorrectly predicting an individual won&amp;rsquo;t claim when in fact they do. Equivalently, it may be more economical to overpredict the probabilities that certain individuals will claim, rather that underpredict these. Later, we use the ROC curve and the Area Under Curve (AUC) to compare the performance of models that takes these ideas into account by implicitly considering false negatives when comparing model performance.&lt;/p&gt;
&lt;p&gt;We implement a Logistic Regression, Random Forest classification and Neural Network and the justification for these models is presented in Section 6. The performance of all models is assessed by 3-fold cross-validation, using the AUC score as a measure of model performance.&lt;/p&gt;
&lt;h2 id=&#34;2-import-and-tidy-the-data&#34;&gt;2) Import and Tidy the Data&lt;/h2&gt;
&lt;p&gt;The first step is to import the data, assess its structure and perform any preliminary steps to tidy the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#import relevant libraries
#Numpy for scientific computation
import numpy as np
#Pandas for data manipulation
import pandas as pd
#Matplotlib plotting library
%matplotlib inline
import matplotlib.pyplot as plt
#Seaborn for statistical data visualization
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#import test and training data
training_data = pd.read_csv(&amp;quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/train.csv&amp;quot;)
test_data = pd.read_csv(&amp;quot;C:/Users/User/Documents/St Andrews/Datamining/Project 2/test.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Assess the structure of the data
training_data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;target&lt;/th&gt;
      &lt;th&gt;ps_ind_01&lt;/th&gt;
      &lt;th&gt;ps_ind_02_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_03&lt;/th&gt;
      &lt;th&gt;ps_ind_04_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_05_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_06_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_07_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_08_bin&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ps_calc_11&lt;/th&gt;
      &lt;th&gt;ps_calc_12&lt;/th&gt;
      &lt;th&gt;ps_calc_13&lt;/th&gt;
      &lt;th&gt;ps_calc_14&lt;/th&gt;
      &lt;th&gt;ps_calc_15_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_16_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_17_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_18_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_19_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_20_bin&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 59 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The data contains 57 features, which are a range of categorical, binomial and continuous variables, the column of target values and an id indicating a unique identity for each individual. The training data constitutes instances for 595212 individuals.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Number of instances in the training set:&amp;quot;, len(training_data))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of instances in the training set: 595212
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#summary of the training data
training_data.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;target&lt;/th&gt;
      &lt;th&gt;ps_ind_01&lt;/th&gt;
      &lt;th&gt;ps_ind_02_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_03&lt;/th&gt;
      &lt;th&gt;ps_ind_04_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_05_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_06_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_07_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_08_bin&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ps_calc_11&lt;/th&gt;
      &lt;th&gt;ps_calc_12&lt;/th&gt;
      &lt;th&gt;ps_calc_13&lt;/th&gt;
      &lt;th&gt;ps_calc_14&lt;/th&gt;
      &lt;th&gt;ps_calc_15_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_16_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_17_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_18_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_19_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_20_bin&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;5.952120e+05&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
      &lt;td&gt;595212.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;7.438036e+05&lt;/td&gt;
      &lt;td&gt;0.036448&lt;/td&gt;
      &lt;td&gt;1.900378&lt;/td&gt;
      &lt;td&gt;1.358943&lt;/td&gt;
      &lt;td&gt;4.423318&lt;/td&gt;
      &lt;td&gt;0.416794&lt;/td&gt;
      &lt;td&gt;0.405188&lt;/td&gt;
      &lt;td&gt;0.393742&lt;/td&gt;
      &lt;td&gt;0.257033&lt;/td&gt;
      &lt;td&gt;0.163921&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;5.441382&lt;/td&gt;
      &lt;td&gt;1.441918&lt;/td&gt;
      &lt;td&gt;2.872288&lt;/td&gt;
      &lt;td&gt;7.539026&lt;/td&gt;
      &lt;td&gt;0.122427&lt;/td&gt;
      &lt;td&gt;0.627840&lt;/td&gt;
      &lt;td&gt;0.554182&lt;/td&gt;
      &lt;td&gt;0.287182&lt;/td&gt;
      &lt;td&gt;0.349024&lt;/td&gt;
      &lt;td&gt;0.153318&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;4.293678e+05&lt;/td&gt;
      &lt;td&gt;0.187401&lt;/td&gt;
      &lt;td&gt;1.983789&lt;/td&gt;
      &lt;td&gt;0.664594&lt;/td&gt;
      &lt;td&gt;2.699902&lt;/td&gt;
      &lt;td&gt;0.493311&lt;/td&gt;
      &lt;td&gt;1.350642&lt;/td&gt;
      &lt;td&gt;0.488579&lt;/td&gt;
      &lt;td&gt;0.436998&lt;/td&gt;
      &lt;td&gt;0.370205&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2.332871&lt;/td&gt;
      &lt;td&gt;1.202963&lt;/td&gt;
      &lt;td&gt;1.694887&lt;/td&gt;
      &lt;td&gt;2.746652&lt;/td&gt;
      &lt;td&gt;0.327779&lt;/td&gt;
      &lt;td&gt;0.483381&lt;/td&gt;
      &lt;td&gt;0.497056&lt;/td&gt;
      &lt;td&gt;0.452447&lt;/td&gt;
      &lt;td&gt;0.476662&lt;/td&gt;
      &lt;td&gt;0.360295&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;7.000000e+00&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;3.719915e+05&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;7.435475e+05&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;5.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;1.115549e+06&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;9.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;1.488027e+06&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;19.000000&lt;/td&gt;
      &lt;td&gt;10.000000&lt;/td&gt;
      &lt;td&gt;13.000000&lt;/td&gt;
      &lt;td&gt;23.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;8 rows × 59 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the above, some of the continuous features have right-skew in their distributions, such as &amp;lsquo;ps_ind_01&amp;rsquo;, with a mean value of 1.9 that is much lower than its maximum value and upper quartile. Later some of these continuous variables will be scaled to improve the performance of the models that are fitted to the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#The test data contains no target variable
test_data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;ps_ind_01&lt;/th&gt;
      &lt;th&gt;ps_ind_02_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_03&lt;/th&gt;
      &lt;th&gt;ps_ind_04_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_05_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_06_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_07_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_08_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_09_bin&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ps_calc_11&lt;/th&gt;
      &lt;th&gt;ps_calc_12&lt;/th&gt;
      &lt;th&gt;ps_calc_13&lt;/th&gt;
      &lt;th&gt;ps_calc_14&lt;/th&gt;
      &lt;th&gt;ps_calc_15_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_16_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_17_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_18_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_19_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_20_bin&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 58 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Number of instances in the test data:&amp;quot;, len(test_data))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of instances in the test data: 892816
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_data.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;ps_ind_01&lt;/th&gt;
      &lt;th&gt;ps_ind_02_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_03&lt;/th&gt;
      &lt;th&gt;ps_ind_04_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_05_cat&lt;/th&gt;
      &lt;th&gt;ps_ind_06_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_07_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_08_bin&lt;/th&gt;
      &lt;th&gt;ps_ind_09_bin&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ps_calc_11&lt;/th&gt;
      &lt;th&gt;ps_calc_12&lt;/th&gt;
      &lt;th&gt;ps_calc_13&lt;/th&gt;
      &lt;th&gt;ps_calc_14&lt;/th&gt;
      &lt;th&gt;ps_calc_15_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_16_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_17_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_18_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_19_bin&lt;/th&gt;
      &lt;th&gt;ps_calc_20_bin&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;8.928160e+05&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
      &lt;td&gt;892816.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;7.441535e+05&lt;/td&gt;
      &lt;td&gt;1.902371&lt;/td&gt;
      &lt;td&gt;1.358613&lt;/td&gt;
      &lt;td&gt;4.413734&lt;/td&gt;
      &lt;td&gt;0.417361&lt;/td&gt;
      &lt;td&gt;0.408132&lt;/td&gt;
      &lt;td&gt;0.393246&lt;/td&gt;
      &lt;td&gt;0.257191&lt;/td&gt;
      &lt;td&gt;0.163659&lt;/td&gt;
      &lt;td&gt;0.185905&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;5.438478&lt;/td&gt;
      &lt;td&gt;1.440265&lt;/td&gt;
      &lt;td&gt;2.875013&lt;/td&gt;
      &lt;td&gt;7.540367&lt;/td&gt;
      &lt;td&gt;0.123720&lt;/td&gt;
      &lt;td&gt;0.627756&lt;/td&gt;
      &lt;td&gt;0.554660&lt;/td&gt;
      &lt;td&gt;0.287796&lt;/td&gt;
      &lt;td&gt;0.349344&lt;/td&gt;
      &lt;td&gt;0.152428&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;4.296830e+05&lt;/td&gt;
      &lt;td&gt;1.986503&lt;/td&gt;
      &lt;td&gt;0.663002&lt;/td&gt;
      &lt;td&gt;2.700149&lt;/td&gt;
      &lt;td&gt;0.493453&lt;/td&gt;
      &lt;td&gt;1.355068&lt;/td&gt;
      &lt;td&gt;0.488471&lt;/td&gt;
      &lt;td&gt;0.437086&lt;/td&gt;
      &lt;td&gt;0.369966&lt;/td&gt;
      &lt;td&gt;0.389030&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;2.330081&lt;/td&gt;
      &lt;td&gt;1.200620&lt;/td&gt;
      &lt;td&gt;1.694072&lt;/td&gt;
      &lt;td&gt;2.745882&lt;/td&gt;
      &lt;td&gt;0.329262&lt;/td&gt;
      &lt;td&gt;0.483403&lt;/td&gt;
      &lt;td&gt;0.497004&lt;/td&gt;
      &lt;td&gt;0.452736&lt;/td&gt;
      &lt;td&gt;0.476763&lt;/td&gt;
      &lt;td&gt;0.359435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;0.000000e+00&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;-1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;3.720218e+05&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;7.443070e+05&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;5.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;1.116308e+06&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;9.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;1.488026e+06&lt;/td&gt;
      &lt;td&gt;7.000000&lt;/td&gt;
      &lt;td&gt;4.000000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;6.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;20.000000&lt;/td&gt;
      &lt;td&gt;11.000000&lt;/td&gt;
      &lt;td&gt;15.000000&lt;/td&gt;
      &lt;td&gt;28.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;8 rows × 58 columns&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A simple visual comparison of the mean of each feature for the test data suggests it is very similar to that of the training data.&lt;/p&gt;
&lt;h2 id=&#34;3-exploratory-data-analysis&#34;&gt;3) Exploratory Data Analysis&lt;/h2&gt;
&lt;h3 id=&#34;summary-of-the-features&#34;&gt;Summary of the Features&lt;/h3&gt;
&lt;p&gt;In order to understand the relationships between the different features and the binary response, and to assess the structure of each of the variables, the following information was gathered to provide a brief summary of the groups of features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Target: Binary response variable indicating a claim (1) or no claim (0)&lt;/li&gt;
&lt;li&gt;IND: these 18 variables refer to characeristics of each individual driver&lt;/li&gt;
&lt;li&gt;REG: 3 variables that refer to the region of the claim&lt;/li&gt;
&lt;li&gt;CAR: 16 variables related to the particular car of each individual on which the claim was made&lt;/li&gt;
&lt;li&gt;CALL: 10 variables, which are feature engineered variables relating to the theory behind pricing of autoinsurance quotations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Missing values for the binary variables are indicated by a -1, whilst the postfix &amp;lsquo;bin&amp;rsquo; indicates binary features and &amp;lsquo;cat&amp;rsquo; indicates categorical features. Features without a postfix are numerical. The exact meaning of each individual feature is not available (Porto Seguro&amp;rsquo;s Safe Driver Prediction- Welcome, 2017).&lt;/p&gt;
&lt;p&gt;Histograms show the distribution of each feature. We separate the plotting of histograms of the features by the four feature types as discussed above, which makes it easier to visualise the distributions of features within groups. These histograms also include missing values (-1 in value) for each of the features, which enables us to assess which features contain the majority of missing values and also understand how each feature should be prepared for machine learning algorithms.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#histogram for each &#39;individual-related&#39; feature in the dataset
training_data.iloc[:, 2: 20].hist(bins = 50, figsize = (20, 15))
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_ce64c9f7af36c6b2d9c3381773802230.png 400w,
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_a7ac0eade7223662b3fcb2720f3959fb.png 760w,
               /project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_26_0_huce4ae79d51bd0fe85e0ce3144b3ef42b_51854_ce64c9f7af36c6b2d9c3381773802230.png&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The attributes that relate to individual characteristics are mainly binary and contain few missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#histogram for each &#39;Regional-based&#39; feature in the dataset
training_data.iloc[:, 20: 23].hist(bins = 50, figsize = (20, 15))
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_45753a53e4d0764a0ebbf13c670554b9.png 400w,
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_e2be04db48fe77ed5c4f4f7c22cc7db4.png 760w,
               /project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_28_0_hu3c77533132e4ce3749a99a9603277f87_23486_45753a53e4d0764a0ebbf13c670554b9.png&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The attribute &amp;lsquo;ps_reg_03&amp;rsquo; contains a large number of missing values and this is something we may need to take into consideration. All three attributes of the &amp;lsquo;region-based&amp;rsquo; features are either left or right skewed, so will be normalized to lie in the interval (0,1), after removing missing values. They also contain no clear outliers, so scaling in this way will not be adversely affected by outlying observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#histogram for each &#39;Car-related&#39; feature in the dataset
training_data.iloc[:, 23: 39].hist(bins = 50, figsize = (20, 15))
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_7b4021d3d48c34aada13fb10a1f24b5a.png 400w,
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_fa491335f2ce2310b122410f0da21446.png 760w,
               /project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_30_0_hu51b31bc1e527919cdd3d80ae1d2f2159_57314_7b4021d3d48c34aada13fb10a1f24b5a.png&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The &amp;lsquo;car-related&amp;rsquo; features consist of a range of categorical, binary and numeric features. Some have a significant number of missing values, however these may in themselves provide explanatory power.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#histogram for each &#39;Calculated (engineered)&#39; feature in the dataset
training_data.iloc[:, 39:].hist(bins = 50, figsize = (20, 15))
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_9428190a560fb830c2b12c3f65792e3b.png 400w,
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_21787236b475cad66fc20918f75723ca.png 760w,
               /project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_32_0_hu7d65a492426514beaa6e64e747c240e1_56388_9428190a560fb830c2b12c3f65792e3b.png&#34;
               width=&#34;760&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;None of the engineered features contain missing values and the continuous features also contain no obvious outliers, so standardising these features to lie in the interval (0,1) should not be adversely affected by the presence of outlying observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Proportion of target instances that made a claim (1) and didn&#39;t make a claim (0)
training_data[&amp;quot;target&amp;quot;].value_counts() / len(training_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    0.963552
1    0.036448
Name: target, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation matrix can be used to assess if there are any linear relationships between the target variable and the attributes in the training dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corr_matrix = training_data.corr()
corr_matrix[&amp;quot;target&amp;quot;].sort_values(ascending = False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;target            1.000000
ps_car_13         0.053899
ps_car_12         0.038790
ps_ind_17_bin     0.037053
ps_reg_02         0.034800
ps_ind_07_bin     0.034218
ps_car_04_cat     0.032900
ps_car_03_cat     0.032401
ps_reg_03         0.030888
ps_ind_05_cat     0.029165
ps_car_15         0.027667
ps_reg_01         0.022888
ps_car_05_cat     0.020754
ps_ind_01         0.018570
ps_car_01_cat     0.016256
ps_ind_08_bin     0.013147
ps_car_06_cat     0.011537
ps_ind_04_cat     0.009360
ps_ind_03         0.008360
ps_ind_12_bin     0.007810
ps_ind_14         0.007443
ps_car_11_cat     0.006129
ps_car_09_cat     0.005322
ps_ind_18_bin     0.004555
ps_ind_02_cat     0.004534
ps_ind_13_bin     0.002460
ps_ind_11_bin     0.002028
ps_calc_03        0.001907
ps_ind_10_bin     0.001815
ps_calc_01        0.001782
ps_calc_14        0.001362
ps_calc_02        0.001360
ps_calc_10        0.001061
ps_car_10_cat     0.001038
ps_calc_05        0.000771
ps_calc_09        0.000719
ps_calc_16_bin    0.000624
ps_calc_18_bin    0.000552
ps_calc_11        0.000371
ps_calc_06        0.000082
ps_calc_04        0.000033
ps_calc_07       -0.000103
ps_calc_17_bin   -0.000170
id               -0.000188
ps_calc_13       -0.000446
ps_calc_15_bin   -0.000490
ps_calc_08       -0.001006
ps_calc_20_bin   -0.001072
ps_calc_12       -0.001133
ps_car_11        -0.001213
ps_calc_19_bin   -0.001744
ps_car_14        -0.004474
ps_ind_09_bin    -0.008237
ps_car_08_cat    -0.020342
ps_ind_15        -0.021506
ps_ind_16_bin    -0.027778
ps_car_02_cat    -0.031534
ps_ind_06_bin    -0.034017
ps_car_07_cat    -0.036395
Name: target, dtype: float64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The fact that many of the features show little correlation with the binary target suggests that either the features lack information to explain and predict the target binary response, or the features have non-linear relationships with the target variable and hence a more complex model, such as a Neural Network, which can work more effectively on datasets that are not linearly separable, could be used. Noticeably, some of the attributes exhibit very little correlation with the binary target variable and they may just be &amp;lsquo;noise&amp;rsquo; and have no real relationship with the response. On the other hand, several features relating to the type of car that each individual insures appear to have greater correlation with the target variable. It could also be the case that missing values provide information about the probability of a claim; those individuals who are more likely to claim may provide less information in order to try reduce their insurance quotations.&lt;/p&gt;
&lt;h2 id=&#34;4-preparing-the-data-for-machine-learning-algorithms&#34;&gt;4) Preparing the Data for Machine Learning Algorithms&lt;/h2&gt;
&lt;h3 id=&#34;missing-data&#34;&gt;Missing Data&lt;/h3&gt;
&lt;p&gt;Most machine learning algorithms can&amp;rsquo;t work with missing values. There are a number of strategies that could be used in order to deal with this problem (Geron, 2019):&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;i) Get rid of the corresponding instances&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We have already seen that some features have a large number of missing values. Therefore, getting rid of the corresponding instances of these attributes will lead to a greatly reduced sample size and the loss of potentially important information.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ii) Get rid of the whole attribute&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many of the attributes with missing values have high correlation with the target. For example, &amp;lsquo;ps_reg_03&amp;rsquo; has a relatively high correlation of 0.0324 with the target variable and a large number of missing values. Getting rid of this feature may lead to reduced predictive power.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;iii) Set the values to some value (e.g. the median)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For the continuous and ordinal categorical features with many levels, it is natural that we might want to fill their missing values with some value, such as the median of that feature. However, it does not make sense to fill the missing values of binary features with their median value. Therefore, binary features with missing values are converted to multinomial features, with a new category representing missing values.&lt;/p&gt;
&lt;p&gt;We use a preparation pipeline to prepare the data for a machine learning algorithm, ensuring that all binary and continous features are correctly added to the model. We separate the features into two types, with different transformations carried out on each of the two types of feature:&lt;/p&gt;
&lt;p&gt;a) Continuous and Categorical Features&lt;/p&gt;
&lt;p&gt;For these features, missing values are inputted using the median value of that respective feature, using $SimpleImputer$. Then the data is normalized to lie in the interval (0,1) (via $MinMaxScaler$). Neural Networks generally expect an input value between 0 and 1 and as previously discussed, there are no extreme outliers so this scaling method is not unduly affected by outliers, providing justification for scaling these featues in this way. The categorical features have a natural ordering and  are already encoded numerically, so there is no need to separate them into further columns using dummy indexing.&lt;/p&gt;
&lt;p&gt;b) Binary Features&lt;/p&gt;
&lt;p&gt;The missing values are used as factor levels, which changes these features from binary to multinomial, as individuals with missing values may provide predictive power and information. The missing values are first converted to the number 2, so that they can be appropriately encoded using the $OneHotEncoder$ (this does not accept negative integers).&lt;/p&gt;
&lt;p&gt;Given the low correlation between the target and some features, there may be a case for dropping some features from the models that we later fit. However, the correlation coefficient only measures linear correlations and may completely miss nonlinear relationships (Geron, 2019). Therefore, given the complex nature of the data, we retain all features when fitting each model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Split the training data into the labels and features
training_labels = training_data[&amp;quot;target&amp;quot;]
training_features = training_data.drop([&amp;quot;target&amp;quot;, &amp;quot;id&amp;quot;], axis = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first need to fill in the missing values for the ordinal categorical and numerical features, so that we can impute the median values for these features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Extract the features that we will treat as numerical
num_attributes = [&amp;quot;ps_ind_01&amp;quot;, &amp;quot;ps_ind_02_cat&amp;quot;, &amp;quot;ps_ind_03&amp;quot;, &amp;quot;ps_ind_05_cat&amp;quot;, &amp;quot;ps_ind_14&amp;quot;, &amp;quot;ps_ind_15&amp;quot;, &amp;quot;ps_reg_01&amp;quot;, 
                  &amp;quot;ps_reg_02&amp;quot;, &amp;quot;ps_reg_03&amp;quot;, &amp;quot;ps_car_01_cat&amp;quot;, &amp;quot;ps_ind_04_cat&amp;quot;, &amp;quot;ps_car_04_cat&amp;quot;, &amp;quot;ps_car_06_cat&amp;quot;, 
                  &amp;quot;ps_car_09_cat&amp;quot;, &amp;quot;ps_car_10_cat&amp;quot;, &amp;quot;ps_car_11&amp;quot;, &amp;quot;ps_car_11_cat&amp;quot;, &amp;quot;ps_car_12&amp;quot;, &amp;quot;ps_car_13&amp;quot;, 
                  &amp;quot;ps_car_14&amp;quot;, &amp;quot;ps_car_15&amp;quot;, &amp;quot;ps_calc_01&amp;quot;, &amp;quot;ps_calc_02&amp;quot;, &amp;quot;ps_calc_03&amp;quot;, &amp;quot;ps_calc_04&amp;quot;, &amp;quot;ps_calc_05&amp;quot;, &amp;quot;ps_calc_06&amp;quot;, 
                  &amp;quot;ps_calc_07&amp;quot;, &amp;quot;ps_calc_08&amp;quot;, &amp;quot;ps_calc_09&amp;quot;, &amp;quot;ps_calc_10&amp;quot;, &amp;quot;ps_calc_11&amp;quot;, &amp;quot;ps_calc_12&amp;quot;, &amp;quot;ps_calc_13&amp;quot;, &amp;quot;ps_calc_14&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We replace the -1 values (corresponding to missing values) for these features with NA, so that we can then impute the median of each feature in place of the missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#replace the numerical features with missing values encoded as NA for training data
training_features[num_attributes] = training_features[num_attributes].replace(-1,np.NaN)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#replace the numerical features with missing values encoded as NA for test data
test_data[num_attributes] = test_data[num_attributes].replace(-1,np.NaN)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#The numerical missing values have now been fillied with NA&#39;s
training_features.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 595212 entries, 0 to 595211
Data columns (total 57 columns):
ps_ind_01         595212 non-null int64
ps_ind_02_cat     594996 non-null float64
ps_ind_03         595212 non-null int64
ps_ind_04_cat     595129 non-null float64
ps_ind_05_cat     589403 non-null float64
ps_ind_06_bin     595212 non-null int64
ps_ind_07_bin     595212 non-null int64
ps_ind_08_bin     595212 non-null int64
ps_ind_09_bin     595212 non-null int64
ps_ind_10_bin     595212 non-null int64
ps_ind_11_bin     595212 non-null int64
ps_ind_12_bin     595212 non-null int64
ps_ind_13_bin     595212 non-null int64
ps_ind_14         595212 non-null int64
ps_ind_15         595212 non-null int64
ps_ind_16_bin     595212 non-null int64
ps_ind_17_bin     595212 non-null int64
ps_ind_18_bin     595212 non-null int64
ps_reg_01         595212 non-null float64
ps_reg_02         595212 non-null float64
ps_reg_03         487440 non-null float64
ps_car_01_cat     595105 non-null float64
ps_car_02_cat     595212 non-null int64
ps_car_03_cat     595212 non-null int64
ps_car_04_cat     595212 non-null int64
ps_car_05_cat     595212 non-null int64
ps_car_06_cat     595212 non-null int64
ps_car_07_cat     595212 non-null int64
ps_car_08_cat     595212 non-null int64
ps_car_09_cat     594643 non-null float64
ps_car_10_cat     595212 non-null int64
ps_car_11_cat     595212 non-null int64
ps_car_11         595207 non-null float64
ps_car_12         595211 non-null float64
ps_car_13         595212 non-null float64
ps_car_14         552592 non-null float64
ps_car_15         595212 non-null float64
ps_calc_01        595212 non-null float64
ps_calc_02        595212 non-null float64
ps_calc_03        595212 non-null float64
ps_calc_04        595212 non-null int64
ps_calc_05        595212 non-null int64
ps_calc_06        595212 non-null int64
ps_calc_07        595212 non-null int64
ps_calc_08        595212 non-null int64
ps_calc_09        595212 non-null int64
ps_calc_10        595212 non-null int64
ps_calc_11        595212 non-null int64
ps_calc_12        595212 non-null int64
ps_calc_13        595212 non-null int64
ps_calc_14        595212 non-null int64
ps_calc_15_bin    595212 non-null int64
ps_calc_16_bin    595212 non-null int64
ps_calc_17_bin    595212 non-null int64
ps_calc_18_bin    595212 non-null int64
ps_calc_19_bin    595212 non-null int64
ps_calc_20_bin    595212 non-null int64
dtypes: float64(16), int64(41)
memory usage: 258.8 MB
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Import relevant packages
#MinMaxScaler to normalize the features
from sklearn.preprocessing import MinMaxScaler
#SimpleImputer to impute the median values in place of missing values
from sklearn.impute import SimpleImputer
#Pipeline to create transformation pipelines to process the data
from sklearn.pipeline import Pipeline
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Pipeline for the features that we treat as numerical
num_pipeline = Pipeline([
    #Impute missing values with the median
    (&#39;imputer&#39;, SimpleImputer(strategy = &amp;quot;median&amp;quot;)),
    #Standardise values to lie in the range [0,1]
    (&#39;min_max_scaler&#39;, MinMaxScaler())
])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now consider the binary features, which will become multinomial with three levels, once we specify the missing values as a new factor level. After converting these features to multinomial, we apply One Hot Encoding. This creates one binary attribute per category, adding two extra columns for each of these features to the feature matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#extract binary features
cat_attributes = [&amp;quot;ps_ind_06_bin&amp;quot;, &amp;quot;ps_ind_07_bin&amp;quot;, &amp;quot;ps_ind_08_bin&amp;quot;, &amp;quot;ps_ind_09_bin&amp;quot;, &amp;quot;ps_ind_10_bin&amp;quot;, &amp;quot;ps_ind_11_bin&amp;quot;, 
                  &amp;quot;ps_ind_12_bin&amp;quot;, &amp;quot;ps_ind_13_bin&amp;quot;, &amp;quot;ps_ind_16_bin&amp;quot;, &amp;quot;ps_ind_17_bin&amp;quot;, &amp;quot;ps_ind_18_bin&amp;quot;, &amp;quot;ps_car_02_cat&amp;quot;, 
                  &amp;quot;ps_car_03_cat&amp;quot;, &amp;quot;ps_car_05_cat&amp;quot;, &amp;quot;ps_car_07_cat&amp;quot;, &amp;quot;ps_car_08_cat&amp;quot;, &amp;quot;ps_calc_15_bin&amp;quot;, &amp;quot;ps_calc_16_bin&amp;quot;, 
                  &amp;quot;ps_calc_17_bin&amp;quot;, &amp;quot;ps_calc_18_bin&amp;quot;, &amp;quot;ps_calc_19_bin&amp;quot;, &amp;quot;ps_calc_20_bin&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Replace -1 with 2 as the OneHotEncoder can&#39;t work for negative integers
training_features[cat_attributes] = training_features[cat_attributes].replace(-1,2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#apply OneHotEncoding again, now for the test data
test_data[cat_attributes] = test_data[cat_attributes].replace(-1,2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import OneHotEncoder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.compose import ColumnTransformer
full_pipeline = ColumnTransformer([
    #pipeline for numerical features
    (&amp;quot;num&amp;quot;, num_pipeline, num_attributes),
    #OneHotEncoder for (previously) binary features
    (&amp;quot;cat&amp;quot;, OneHotEncoder(), cat_attributes)
])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#use pipeline to prepare training data
training_prepared = full_pipeline.fit_transform(training_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;C:\Users\User\Anaconda3\lib\site-packages\sklearn\preprocessing\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.
If you want the future behaviour and silence this warning, you can specify &amp;quot;categories=&#39;auto&#39;&amp;quot;.
In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.
  warnings.warn(msg, FutureWarning)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#transform the test data based on the scaling estimated from the training data
test_prepared = full_pipeline.transform(test_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-the-confusion-matrix-and-roc-curve-as-measures-of-model-performance&#34;&gt;5) The Confusion Matrix and ROC Curve as Measures of Model Performance&lt;/h3&gt;
&lt;p&gt;It is important, before carrying out the model selection procedure, to consider an appropriate measure of model performance. This avoids biases that may occur when selecting a performance measure after different models have been fit.&lt;/p&gt;
&lt;p&gt;In the given situation, using accuracy as a measure of performance is not suitable, because all the models fitted predict very low probabilities that individuals will make a claim and hence using the standard threshold probability of 0.5, always predict that individuals will not make a claim (i.e. always predict a negative). This is caused by the low rate of individuals making a claim within a year (just 3.64%) and the lack of information contained within the features relating to the binary target.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Proportion of positive and negative target values (&#39;claim&#39; Vs &#39;no claim&#39;) in the training data
print(&amp;quot;Proportion of individuals who didn&#39;t make a claim:&amp;quot;, round((573518 / (21694 + 573518)), 4))
print(&amp;quot;Proportion of individuals who made a claim:&amp;quot;, round((21694 / (21694 + 573518)), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Proportion of individuals who didn&#39;t make a claim: 0.9636
Proportion of individuals who made a claim: 0.0364
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accuracy also ignores the types of errors that are made. In the context of automotive insurance, incorrectly predicting that an individual won&amp;rsquo;t claim when they actually do (false negatives) will be more costly (financially) to Porto Seguro than incorrectly predicting that an individual will claim and then they actually don&amp;rsquo;t (false positives). Hence, more emphasis should be placed on reducing the false negative rate. The ROC curves of different models and the ROC AUC scores can be compared to assess model performance in this manner. The ROC curve plots the true positive rate against the falser postive rate. The true postive rate (recall) takes into account false negatives, as it is the ratio of positive instances that are correctly detected by the classifier:&lt;/p&gt;
&lt;p&gt;$$Recall = True Positive Rate = \frac{TP}{TP + FN}$$&lt;/p&gt;
&lt;p&gt;Thus, if the number of false negatives increases, the recall will fall. The ROC curve also allows us to assess the trade-off between the true positive rate (recall) and the false positive rate; the higher the recall, the higher the false positive rate will be, on average. That is, if the recall rises, this may be due to the fact we are always predicting a positive outcome, however conequently this will lead to an increase in the number of false positive that are predicted. Thus, this measure of performance enables us to assess this trade-off and incorporate the problems of false negative classification. The Area Under the Curve (AUC) provides a quantitative measure of performance between models, calculating the area under the ROC curve for a given model. A higher AUC score indicates a better performing model.&lt;/p&gt;
&lt;h2 id=&#34;5-selection-and-training-of-models&#34;&gt;5) Selection and Training of Models&lt;/h2&gt;
&lt;p&gt;The No Free Lunch Theorem states that if we make absolutely no assumptions about the data, then there is no reason to prefer one model over another. The only way to know for sure is to evaluate them all (Geron, 2019). However, in practice we can make a few reasonable assumptions about the data. the Porto Seguro automotive insurance data provides a range of features; categorical, numeric and binary. We have shown that most of the variables have little correlation with the binary target variable of &amp;lsquo;claim&amp;rsquo; or &amp;lsquo;no claim&amp;rsquo;. This suggests that a more complex model, such as a neural network may be beneficial to make predictions for datasets that are not close to being linearly separable. However, given the large number of features in relation to the size of the data, we expect that neural networks may not achieve their &amp;lsquo;potential&amp;rsquo; as they may overfit the data quite quickly. This suggests that more simple methods for classification, such as logistic regression may also perform relatively well when making predictions on unseen data. Finally, we fit a Random Forest classification model to the training data and again assess its performance using cross-validation. Random Forest models are Ensemble Methods that average out the predictions of decision trees and introduce extra randomness when growing trees, resulting in extra tree diversity (different features are used to split nodes, not necessarily the optimal features) that leads to an increase in bias and a reduced variance. This may enable random forest models to generalize better to unseen data.&lt;/p&gt;
&lt;h3 id=&#34;cross-validation&#34;&gt;Cross-Validation&lt;/h3&gt;
&lt;p&gt;K-Fold Cross Validation is a common way to evaluate different models (and with different hyperparameters). This process splits the training data into k folds (here k = 3), where (k-1) of the folds are used to train the model, the performance of which is then assessed by comparing its predictions to the target values of the unused fold, which acts as a validation set. This ensures the model is assessed on instances that were previously unseen when fitting the model.&lt;/p&gt;
&lt;p&gt;Cross-Validation allows one to assess potential issues of overfitting and underfitting across models and provides an estimate of how each model will generalize to an independent sample of unseen data. Using this approach, we can train multiple models using different hyperparameters on the reduced training set and select the model that performs best on average for each of the k validation folds.&lt;/p&gt;
&lt;p&gt;All models discussed are types of supervised machine learning; the training set fed into the algorithm includes the desired solutions (binary classifications) called labels.&lt;/p&gt;
&lt;h3 id=&#34;a-logistic-regression&#34;&gt;a) Logistic Regression&lt;/h3&gt;
&lt;p&gt;Logistic regression can be used to estimate the probability that a given instance belongs to one of the binary classes. In a similar manner to linear regression, logistic regression computes a weighted sum of input features, plus a bias term. However, it then uses the logistic function to output a number (probability) between 0 and 1. The weights used in the regression are estimated by minimizing the log loss function:&lt;/p&gt;
&lt;p&gt;$$J(\theta) = \frac{-1}{m}\sum_{i=1}^{m}[ y^i \log(p^i) + (1 - y^i) \log(1 - p^i)]$$&lt;/p&gt;
&lt;p&gt;That is, when the true value (label) for a given instance is 1 (&amp;lsquo;claim&amp;rsquo;, i.e. $y^i = 1$), the loss for individual i becomes $\log(p^i)$. Hence, a higher predicted probability leads to a reduced loss in this case and when $p^i = 1$, $\log(p^i) =0$ and the loss or cost for instance i is zero. Hence, on average, the optimal solution will predict low probabilities for instances/ individuals that did not make a claim and high probabilities for those that did make a claim, using the input features, to minimize this loss. Logistic Regression can be used for binary classification by specifying a threshold probability, beyond which an instance is predicted to make a claim.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(random_state = 160001695)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import cross_val_predict
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now use cross-validation to train the model on&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#extract the predicted probabilities for the k validation sets
y_train_pred = cross_val_predict(log_reg, training_prepared, training_labels, cv = 3,  method = &amp;quot;predict_proba&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#predicted values of a &#39;claim&#39; (target = 1)
logistic_predictions = y_train_pred[:,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can look at the ROC curve to assess the performance of the model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import roc_curve

#false positve rate, true positive rate, probability thresholds
fpr, tpr, thresholds = roc_curve(training_labels, logistic_predictions)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Create a function to plot the ROC curve
def plot_roc_curve(fpr, tpr, label = None):
    #plot roc curve for fitted model
    plt.plot(fpr, tpr, linewidth = 2, label = label)
    #Add line for random guess model
    plt.plot([0,1], [0,1], &#39;k--&#39;)
    plt.xlabel(&#39;False Positive Rate&#39;, fontsize = 14)
    plt.ylabel(&#39;True Positive Rate (Recall)&#39;, fontsize = 14)
    plt.grid(True)
    plt.title(&#39;ROC Curve&#39;, fontsize = 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_roc_curve(fpr, tpr, &amp;quot;Logistic Regression&amp;quot;)
plt.savefig(&amp;quot;roc_curve1.png&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_b93638acff3cfdede3f0617b6e32b6d7.png 400w,
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_ed91ecf3db106bf6cea6577323208c8c.png 760w,
               /project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_82_0_hu96ba5721fa8cb7563add63ca1826511b_19421_b93638acff3cfdede3f0617b6e32b6d7.png&#34;
               width=&#34;393&#34;
               height=&#34;286&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now we assess the area under the curve (AUC) For the Receiver Operating Characteristic (ROC) Curve:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import roc_auc_score
print(&amp;quot;AUC Score for the Logistic Regression Model:&amp;quot;, round(roc_auc_score(training_labels, logistic_predictions), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AUC Score for the Logistic Regression Model: 0.6215
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;b-random-forest-model&#34;&gt;b) Random Forest Model&lt;/h3&gt;
&lt;p&gt;A Random Forest is an ensemble of decision trees, which makes predictions by taking a weighted average of the probabilities (in this case) of each tree. It also introduces extra randomness when growing trees; instead of always splitting a node by the best feature (as determined by the training set), it searches for the best feature among a random subset of the features. This may allow the model to improve its ability to generalize to new datasets, by raising its bias but reducing its variance in making predictions on new, unseen data.&lt;/p&gt;
&lt;p&gt;Each individual decision tree that makes up the Random Forest is estimated using the CART Training Algorithm. This is a &amp;lsquo;greedy&amp;rsquo; algorithm, in that it greedily searches the optimal feature and threshold to split the data by, without considering whether this split will lead to the best gini impurity several levels down. Thus, this model is often considered as reasonable but not optimal. Regularization for the tree can simply be controlled via the maximum depth of each decision tree, which here is set to 3.&lt;/p&gt;
&lt;p&gt;Use Scikit-Learn&amp;rsquo;s SGD classifier as a method for binary classification&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state = 160001695, max_depth = 3)
#extract predicted probabilities for the k validation sets
y_probas_forest = cross_val_predict(forest_clf, training_prepared, training_labels, cv = 3, method = &amp;quot;predict_proba&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &amp;quot;10 in version 0.20 to 100 in 0.22.&amp;quot;, FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &amp;quot;10 in version 0.20 to 100 in 0.22.&amp;quot;, FutureWarning)
C:\Users\User\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &amp;quot;10 in version 0.20 to 100 in 0.22.&amp;quot;, FutureWarning)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#false positive rate, true positive rate, probability thresholds
fpr_rf, tpr_rf, thresholds_rf = roc_curve(training_labels, y_probas_forest[:,1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Plot the ROC curves for logistic and random forest models
plt.plot(fpr, tpr, &amp;quot;b:&amp;quot;, label = &amp;quot;Logistic Regression&amp;quot;)
plot_roc_curve(fpr_rf, tpr_rf, &amp;quot;Random Forest&amp;quot;)
plt.legend(loc = &amp;quot;lower right&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_76314540f2730bd33fef959b2a72074f.png 400w,
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_94c2e9fc29e6ac95ee03c1752e65c5f7.png 760w,
               /project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_90_0_huac5035e2bd212a9ee6ebc1f9b7051deb_24601_76314540f2730bd33fef959b2a72074f.png&#34;
               width=&#34;393&#34;
               height=&#34;286&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;AUC Score for the Random Forest Model:&amp;quot;, round(roc_auc_score(training_labels, y_probas_forest[:,1]), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;AUC Score for the Random Forest Model: 0.6036
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the performance of the Random Forest Regressor is not as good as that of the Logistic Regression model, as measured by the ROC curve and AUC score. In particular, the ROC curve for the Random Forest model lies under that of the Logistic Regression model for all thresholds. Hence, for a given false positive rate, the true positive rate for the random forest model is smaller, suggesting it is underestimating probabilities for those individuals that made a claim. This information is confirmed by the lower AUC score of 0.6036.&lt;/p&gt;
&lt;h3 id=&#34;c-neural-network&#34;&gt;c) Neural Network&lt;/h3&gt;
&lt;p&gt;Multilayer perceptrons can be used to create a classification model for this task. For a binary classification problem, we require a single output neuron using the logistic activation function (Geron, 2019). The output will be a number between zero and one, which can be interpreted as the estimated probability of the positive class (making a claim).&lt;/p&gt;
&lt;p&gt;A perceptron with a single layer and single neuron (i.e. a threshold logic unit, TLU) is equivalent to a logistic regression model, as it computes a linear combination of feature inputs and uses the sigmoid function to transform these weighted linear combinations into a probability lying in the interval (0,1). Multilayer perceptrons are composed of an input layer, one or more layers of TLU&amp;rsquo;s, called hidden layers and the output layer. Every layer except the outer layer contains a bias neuron and is fully connected to the next layer. Hence, by adding more layers, we hope to train a model that improves upon the Logistic Regression model in terms of its predictive performance as measured by AUC.&lt;/p&gt;
&lt;p&gt;The model is trained using backpropagation. This works by splitting the input data used to train the model into &amp;lsquo;mini-batches&amp;rsquo;. Each mini-batch of data is passed through the model and the network&amp;rsquo;s output error is measured by a loss function, in this case for the sigmoid function, which is the same as that of the logistic regression loss function:&lt;/p&gt;
&lt;p&gt;$$J(\theta) = \frac{-1}{m}\sum_{i=1}^{m}[ y^i \log(p^i) + (1 - y^i) \log(1 - p^i)]$$&lt;/p&gt;
&lt;p&gt;where m is the number of instances in the mini-batch fed in to the neural network. Then the contribution to this error of the outputs from each neuron is computed. This is a reverse pass, and measures the error gradient backward through the network. The connection weights for each neuron are then corrected using Gradient Descent to reduce the error. One cycle of a forward and reverse pass is known as an epoch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#import tensorflow and keras for neural networks
import tensorflow as tf
from tensorflow import keras
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first fit a Neural Network with two hidden layers and ten neurons per layer, using the default learning rate. Later these hyperparameters are tuned to search for a better model as assessed by AUC.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net = keras.models.Sequential([
    #first hidden layer
    keras.layers.Dense(10, activation = &amp;quot;relu&amp;quot;, input_shape = training_prepared.shape[1:]),
    #second hidden layer
    keras.layers.Dense(10, activation = &amp;quot;relu&amp;quot;),
    #output neuron
    keras.layers.Dense(1, activation = &amp;quot;sigmoid&amp;quot;)
])    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Compile model
Neural_Net.compile(loss = &amp;quot;binary_crossentropy&amp;quot;, 
                  optimizer = &amp;quot;sgd&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now create a function that carries out K-fold cross-validation on the Neural Net model. This is customized to calculate the AUC score as the measure of performance for each fold and then the AUC score is later averaged across the folds to provide a final score for the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#KFold function splits data into k consecutive folds
from sklearn.model_selection import KFold
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the three Neural Nets is trained with just 3 epochs to reduce computational time and allow for a quicker assessment of models:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Carry out K-Fold cross-validation with k = 3 folds by default
def Neural_Net_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 3):
    #empty array to store predictions
    NN_pred = np.zeros(len(training_labels))
    for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared):
        #split data into training and test set
        x_train, x_test=training_prepared[train_index], training_prepared[test_index]
        #split corresponding test and training labels
        y_train, y_test=training_labels[train_index], training_labels[test_index]
        #Fit model on training set
        Neural_Net.fit(x_train, y_train,epochs= epochs)
        #Make predictions on test test
        NN_pred = Neural_Net.predict(x_test)
        #Calculate AUC score for the kth fold
        print(&amp;quot;AUC cross-validation score: &amp;quot;, round(roc_auc_score(y_test, NN_pred), 5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Carry out cross-validation on the model
Neural_Net_CrossVal(Neural_Net, k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 72s 180us/sample - loss: 0.1558 - accuracy: 0.9637
Epoch 2/3
396808/396808 [==============================] - 68s 172us/sample - loss: 0.1542 - accuracy: 0.9637
Epoch 3/3
396808/396808 [==============================] - 72s 182us/sample - loss: 0.1538 - accuracy: 0.9637
AUC cross-validation score:  0.60311
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 66s 167us/sample - loss: 0.1545 - accuracy: 0.9634
Epoch 2/3
396808/396808 [==============================] - 63s 160us/sample - loss: 0.1542 - accuracy: 0.9634
Epoch 3/3
396808/396808 [==============================] - 71s 180us/sample - loss: 0.1541 - accuracy: 0.9634- loss: 0.1541 - ac
AUC cross-validation score:  0.61718
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1534 - accuracy: 0.9636
Epoch 2/3
396808/396808 [==============================] - 66s 166us/sample - loss: 0.1533 - accuracy: 0.9636
Epoch 3/3
396808/396808 [==============================] - 65s 164us/sample - loss: 0.1532 - accuracy: 0.9636
AUC cross-validation score:  0.61913
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, (0.60311 + 0.61718 + 0.61913) / 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.6131399999999999
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, we see that the average AUC score for the neural network model (0.61314) is slightly lower than that of the logistic regression model (0.6215). We can also see that the accuracy is roughly equal to the proportion of individuals in the training set who made a claim. As the predicted probabilities are so low, all individuals are predicted as not claiming and hence the accuracy is a useless measure of model performance in this case.&lt;/p&gt;
&lt;h2 id=&#34;7-fine-tune-models&#34;&gt;7) Fine-Tune Models&lt;/h2&gt;
&lt;p&gt;Given the results of the AUC scores, we now only consider hyperparameter tuning of the Neural Net as there is evidence that this model may provide the best predictions on unseen data if the hyperparameters are tuned well enough to improve model performance beyond that of the logistic regression model. Because the AUC score is not a standard metric in keras, we use the cross-validation function and manually assess models with different hyperparameters, instead of using grid search. The first neural network model that we fitted had two hidden layers, each with 10 neurons and used the default learning rate of 0.001.&lt;/p&gt;
&lt;p&gt;In order to assess the performance of the neural network binary classifier, we use the custom cross validation function that calculates the AUC score for each fold for each set of hyperparameters. By testing on the unseen fold, an unbiased assessment of how each model might generalize to new data can be carried out.&lt;/p&gt;
&lt;p&gt;There are several different hyperparameters for the neural network that we consider:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;a) Number of Hidden Layers&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For many problems, a single hidden layer can provide reasonable results. However, given the (relatively) complex nature of this problem, with a large number of fairly uninformative features, adding more layers may improve the predictive ability of a neural network.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;b) Number of Neurons per hidden layer&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As the number of neurons per layer increases, we expect the predictive power of the model to increase. However, as with the number of hidden layers, overfitting may become an issue if too many neurons are added per layer, so cross-validation can be used to find an optimal trade-off of bias and variance of the probability predictions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;c) Learning Rate&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This hyperparameter determines the size of steps that the gradient descent takes when minimizing the sigmoid cost function. The lower the learning rate, the more iterations the gradient descent algorithm will take to converge. However, if the learning rate is too high, the algorithm may overshoot and diverge. The learning rate is one of the most influential hyperparameters in fitting a Neural network, so we consider three different values and compare their performance to the original learning rate of 1e-3.&lt;/p&gt;
&lt;p&gt;We now create a function that can build simple Sequential Neural Networks for a given set of input values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3):
    #Define the keras model
    model = keras.models.Sequential()
    #Add an input layer
    model.add(keras.layers.InputLayer(input_shape = training_prepared.shape[1:]))
    for layer in range(n_hidden):
        #Assume each hidden layer contains the same number of neurons
        model.add(keras.layers.Dense(n_neurons, activation = &amp;quot;relu&amp;quot;))
    #Add the output layer
    model.add(keras.layers.Dense(1, activation = &amp;quot;sigmoid&amp;quot;))
    #Use SGD for backpropagation
    optimizer = keras.optimizers.SGD(lr = learning_rate)
    #compile model
    model.compile(loss = &amp;quot;binary_crossentropy&amp;quot;, optimizer = optimizer)
    return model
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;a-number-of-hidden-layers&#34;&gt;a) Number of Hidden Layers&lt;/h3&gt;
&lt;p&gt;We use the customized cross-validation function to assess the performance of the original neural network model but instead with 1 and 3 hidden layers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Then manually compare each model by considering cross-validation
Neural_Net_CrossVal(build_model(n_hidden = 1, n_neurons = 10), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1757
Epoch 2/3
396808/396808 [==============================] - 57s 142us/sample - loss: 0.1552
Epoch 3/3
396808/396808 [==============================] - 69s 175us/sample - loss: 0.1546
AUC cross-validation score:  0.58507
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1552
Epoch 2/3
396808/396808 [==============================] - 57s 143us/sample - loss: 0.1550
Epoch 3/3
396808/396808 [==============================] - 56s 142us/sample - loss: 0.1548
AUC cross-validation score:  0.59959
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 53s 132us/sample - loss: 0.1540
Epoch 2/3
396808/396808 [==============================] - 66s 166us/sample - loss: 0.1539
Epoch 3/3
396808/396808 [==============================] - 63s 158us/sample - loss: 0.1538
AUC cross-validation score:  0.60671
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.58507 + 0.59959 + 0.60671) / 3), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.5971
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_CrossVal(build_model(n_hidden = 3, n_neurons = 10), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 71s 178us/sample - loss: 0.1575
Epoch 2/3
396808/396808 [==============================] - 65s 164us/sample - loss: 0.1548
Epoch 3/3
396808/396808 [==============================] - 55s 139us/sample - loss: 0.1542
AUC cross-validation score:  0.59475
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 55s 138us/sample - loss: 0.1548
Epoch 2/3
396808/396808 [==============================] - 54s 136us/sample - loss: 0.1546
Epoch 3/3
396808/396808 [==============================] - 56s 140us/sample - loss: 0.1545
AUC cross-validation score:  0.60568
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 149us/sample - loss: 0.1537
Epoch 2/3
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1536
Epoch 3/3
396808/396808 [==============================] - 57s 144us/sample - loss: 0.1535
AUC cross-validation score:  0.6113
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.59475 + 0.60568 + 0.6113) / 3),4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.6039
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the neural network might be suffering from issues of overfitting for 3 hidden layers and underfitting for 1 hidden layer, as reflected by the lower AUC scores in comparison to the original Neural Network model with just 1 hidden layer.&lt;/p&gt;
&lt;h3 id=&#34;b-number-of-neurons-per-hidden-layer&#34;&gt;b) Number of Neurons per Hidden Layer&lt;/h3&gt;
&lt;p&gt;Again apply the custom cross-validation function to models with 5 and 15 neurons in each of the two hidden layers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 5), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1595
Epoch 2/3
396808/396808 [==============================] - 56s 141us/sample - loss: 0.1556
Epoch 3/3
396808/396808 [==============================] - 55s 140us/sample - loss: 0.1548
AUC cross-validation score:  0.57591
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1554
Epoch 2/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1551
Epoch 3/3
396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 56s 141us/sample - loss: 0.1549
AUC cross-validation score:  0.59693
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1542
Epoch 2/3
396808/396808 [==============================] - 60s 150us/sample - loss: 0.1540
Epoch 3/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1539
AUC cross-validation score:  0.60563
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.57591 + 0.59693 + 0.60563) / 3),4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.5928
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 15), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1587
Epoch 2/3
396808/396808 [==============================] - 56s 142us/sample - loss: 0.1545
Epoch 3/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1540
AUC cross-validation score:  0.59848
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1547
Epoch 2/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1545
Epoch 3/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1543
AUC cross-validation score:  0.60942
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1536
Epoch 2/3
396808/396808 [==============================] - 61s 155us/sample - loss: 0.1535
Epoch 3/3
396808/396808 [==============================] - 62s 157us/sample - loss: 0.1534
AUC cross-validation score:  0.61443
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.59848 + 0.60942 + 0.61443) / 3), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.6074
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the AUC scores for 5 and 15 neurons per hidden layer are lower than the AUC score for 10 hidden layers.&lt;/p&gt;
&lt;h3 id=&#34;c-learning-rate&#34;&gt;c) Learning Rate&lt;/h3&gt;
&lt;p&gt;Finally, we compare the sensitivity of the AUC score for the models under cross-validation for different values of the learning rate (1e-4 and 1e-2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-4), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 61s 154us/sample - loss: 0.2657
Epoch 2/3
396808/396808 [==============================] - 58s 145us/sample - loss: 0.1578
Epoch 3/3
396808/396808 [==============================] - 57s 145us/sample - loss: 0.1569
AUC cross-validation score:  0.53327
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1577
Epoch 2/3
396808/396808 [==============================] - 59s 148us/sample - loss: 0.1576
Epoch 3/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1575
AUC cross-validation score:  0.54585
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1568
Epoch 2/3
396808/396808 [==============================] - 61s 153us/sample - loss: 0.1567
Epoch 3/3
396808/396808 [==============================] - 61s 154us/sample - loss: 0.1566 
AUC cross-validation score:  0.55127
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.53327 + 0.54585 + 0.55127) / 3), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.5435
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_CrossVal(build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2), k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1560 - loss: 
Epoch 2/3
396808/396808 [==============================] - 63s 158us/sample - loss: 0.1539
Epoch 3/3
396808/396808 [==============================] - 64s 161us/sample - loss: 0.1535
AUC cross-validation score:  0.61233
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - ETA: 0s - loss: 0.154 - 60s 151us/sample - loss: 0.1542
Epoch 2/3
396808/396808 [==============================] - 64s 162us/sample - loss: 0.1540
Epoch 3/3
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1539
AUC cross-validation score:  0.61899
Train on 396808 samples
Epoch 1/3
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1533
Epoch 2/3
396808/396808 [==============================] - 72s 180us/sample - loss: 0.1532
Epoch 3/3
396808/396808 [==============================] - 71s 178us/sample - loss: 0.1532
AUC cross-validation score:  0.61907
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.61233 + 0.61899 + 0.61907) / 3), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.6168
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results of this hyperparameter tuning support a model with two hidden layers, ten neurons per hidden layer and a learning rate of 1e-2 as the best trialled model, using the AUC score as a measure of performance, with an average AUC of 0.61689.&lt;/p&gt;
&lt;h2 id=&#34;8-assessing-model-performance&#34;&gt;8) Assessing Model Performance&lt;/h2&gt;
&lt;p&gt;We now fit the updated Neural Network model with a learning rate of 1e-2, which was chosen as the optimal model with the highest AUC score but with 5 epochs to ensure convergence to the optimal weights at each layer (we would have used more epochs but that was too time consuming for this PC). The fitted model is then used to make predictions on the training set via cross-validation and the ROC curve is plotted to get a more accurate assessment of the model&amp;rsquo;s performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Final model with 2 hidden layers, 10 neurons per hidden layer and a learning rate of 0.01
NN_final = build_model(n_hidden = 2, n_neurons = 10, learning_rate = 1e-2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Compile Model
NN_final.compile(loss = &amp;quot;binary_crossentropy&amp;quot;, 
                  optimizer = &amp;quot;sgd&amp;quot;, metrics = [&amp;quot;accuracy&amp;quot;])
#Fit model to training data
NN_final.fit(training_prepared, training_labels, epochs = 5)
#Extract predicted probabilities on the test set
y_pred = NN_final.predict(test_prepared)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 595212 samples
Epoch 1/5
595212/595212 [==============================] - 91s 154us/sample - loss: 0.1541 - accuracy: 0.9636
Epoch 2/5
595212/595212 [==============================] - 89s 150us/sample - loss: 0.1537 - accuracy: 0.9636
Epoch 3/5
595212/595212 [==============================] - 93s 156us/sample - loss: 0.1536 - accuracy: 0.9636
Epoch 4/5
595212/595212 [==============================] - 90s 152us/sample - loss: 0.1535 - accuracy: 0.9636
Epoch 5/5
595212/595212 [==============================] - 90s 151us/sample - loss: 0.1534 - accuracy: 0.9636
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now calculate the AUC score for the final model, however this time we train each fold on 5 epochs to ensure convergence to the optimal weights at each layer. We then average the AUC score for each fold to estimate the overall AUC score.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def Neural_Net_Final_CrossVal(Neural_Net, k = 3, random_state = 160001695, epochs = 5):
    #empty array to store predictions
    NN_pred = np.zeros(len(training_labels))
    for train_index,test_index in KFold(k, random_state = 160001695).split(training_prepared):
        #split data into training and test set
        x_train, x_test=training_prepared[train_index], training_prepared[test_index]
        #split corresponding test and training labels
        y_train, y_test=training_labels[train_index], training_labels[test_index]
        #Fit model on training set
        Neural_Net.fit(x_train, y_train,epochs= epochs)
        #Make predictions on test test
        NN_pred = Neural_Net.predict(x_test)
        #Calculate AUC score for the kth fold
        print(&amp;quot;AUC cross-validation score: &amp;quot;, round(roc_auc_score(y_test, NN_pred), 5))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Neural_Net_Final_CrossVal(NN_final, k = 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637
Epoch 2/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1530 - accuracy: 0.9637
Epoch 3/5
396808/396808 [==============================] - 59s 149us/sample - loss: 0.1529 - accuracy: 0.9637
Epoch 4/5
396808/396808 [==============================] - 58s 146us/sample - loss: 0.1529 - accuracy: 0.9637
Epoch 5/5
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1529 - accuracy: 0.9637
AUC cross-validation score:  0.62041
Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 60s 150us/sample - loss: 0.1538 - accuracy: 0.9634
Epoch 2/5
396808/396808 [==============================] - 58s 147us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 3/5
396808/396808 [==============================] - 60s 152us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 4/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634
Epoch 5/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1537 - accuracy: 0.9634
AUC cross-validation score:  0.6219
Train on 396808 samples
Epoch 1/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1531 - accuracy: 0.9636
Epoch 2/5
396808/396808 [==============================] - 62s 156us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 3/5
396808/396808 [==============================] - 61s 153us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 4/5
396808/396808 [==============================] - 63s 159us/sample - loss: 0.1530 - accuracy: 0.9636
Epoch 5/5
396808/396808 [==============================] - 60s 151us/sample - loss: 0.1530 - accuracy: 0.9636
AUC cross-validation score:  0.62102
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Average AUC score&amp;quot;, round(((0.62041 + 0.62102 + 0.6219) / 3), 4))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Average AUC score 0.6211
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whilst the AUC score provides a formal metric to assess the performance of each model, we may also consider plotting the ROC curve for the fitted Neural Network model and compare it to the ROC curves under Logistic Regression and the Random Forest classifier. Unfortunately, there is no easy way to extract all predictions from the Cross-Validation carried out, so we instead split the training data randomly into a training and validation set purely for the purpose of plotting a ROC curve to roughly illustrate the differences in performance between the different models. To do this, we randomly shuffle the indices of the data and split the test and training set into 80% training set and 20% validation set. We then approximate the true ROC curve using predictions solely on the validation set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
#Set random seed for reproducibility
random.seed(160001695)
#shuffle indices of full training set
shuffled_indices = np.random.permutation(len(training_prepared))
#validation set is 20% of full training data
validation_set_size = int(len(training_prepared) * 0.2)
#extract the validation indices
validation_indices = shuffled_indices[:validation_set_size]
#extract the training indices
train_indices = shuffled_indices[validation_set_size:]
#create training set
train_set = training_prepared[train_indices]
#create validation set
validation_set = training_prepared[validation_indices]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Train on the training data and predict for the validation data
NN_fit_ROC = NN_final.fit(train_set, training_labels[train_indices], epochs = 5)
NN_pred_ROC = NN_final.predict(validation_set)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Train on 476170 samples
Epoch 1/5
476170/476170 [==============================] - 83s 175us/sample - loss: 0.1533 - accuracy: 0.9635
Epoch 2/5
476170/476170 [==============================] - 80s 167us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 3/5
476170/476170 [==============================] - 70s 146us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 4/5
476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635
Epoch 5/5
476170/476170 [==============================] - 71s 149us/sample - loss: 0.1532 - accuracy: 0.9635
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Extract the false positive rate, true positive rate and thresholds for the validation set predictions
fpr_NN, tpr_NN, thresholds_NN = roc_curve(training_labels[validation_indices], NN_pred_ROC[:, 0])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Plot the ROC curve for all three different models assessed
plt.plot(fpr, tpr, &amp;quot;b:&amp;quot;, label = &amp;quot;Logistic Regression&amp;quot;)
plot_roc_curve(fpr_rf, tpr_rf, &amp;quot;Random Forest&amp;quot;)
plot_roc_curve(fpr_NN, tpr_NN, &amp;quot;Tuned Neural Network&amp;quot;)
plt.legend(loc = &amp;quot;lower right&amp;quot;)
plt.savefig(&amp;quot;roc_curve.png&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_447a0ddd472adf9489cd47152c4a00f6.png 400w,
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_7cb1bcaa309cbe6951b469d350e2c649.png 760w,
               /project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://domscruton.github.io/project/carinsurance/output_147_0_hu3c9220a08119193daaf7d8894893dd90_26959_447a0ddd472adf9489cd47152c4a00f6.png&#34;
               width=&#34;393&#34;
               height=&#34;286&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The above ROC curve illustrates that the tuned Neural Network model tends to outperform the other fitted models for the majority of threshold values, when estimating its probabilities on the randomly sampled validation set. Despite having a slightly lower (but almost identical) AUC score of 0.6211 to that of the Logistic Regression model, the ROC curve seems to slightly favour the Tuned Neural Network model and hence we use this model as the final model to make predictions on the test data for upload to Kaggle (Also increasing the number of epochs to 20 for example would have likely raised the AUC Score for this model).&lt;/p&gt;
&lt;p&gt;[Note- both the AUC score and ROC Curves are dependent on how the data is split for each fold in the cross-validation process and this splitting is done randomly. Therefore, it is not contradictory that the ROC curve for the Tuned Neural Network is generally higher than that of the Logistic Regression model, despite the fact that the Logistic Regression model had a larger AUC score. This is because the AUC was calculated on a different set of sampled cross-validations. Essentially we have two seemingly equally performing models and in this case we choose the Neural Network. It may or may not generalize as well to the actual test data]&lt;/p&gt;
&lt;p&gt;We now use the following code to upload the best model (Neural network with 2 hidden layers, 10 nodes per hidden layer and a learning rate of 1e-2) to kaggle:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#use fitted model to predict for test data
target_NN = NN_final.predict(test_prepared)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#extract probalities for &#39;claim&#39;
target_NN = target_NN[:, 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Extract identity column
identity = test_data[&amp;quot;id&amp;quot;]
#Prepare the data into 2 columns for submission
submission = pd.DataFrame({&#39;id&#39;: identity, &#39;target&#39;: target_NN})
filename = &#39;Porto Seguro Predictions.csv&#39;
#Convert file to csv for Kaggle submission
submission.to_csv(filename,index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;9-conclusion&#34;&gt;9) Conclusion&lt;/h2&gt;
&lt;p&gt;In this project, we have assessed the performance of three models that could be used for binary classification for the Porto Seguro automotive insurance data. The three assessed models are: Logistic Regression, Random Forest Classifier and a Multi-Layer Perceptron Neural Network. The ROC curve and AUC score were used as measures of performance, as these take into account the false negative rate that could be more important for economic reasons to this insurance company. Cross-validation was then used to assess the performance of the different models, allowing an unbiased calculation of the AUC score by training a model and validating on a previously unseen kth fold of the data. The model that was chosen in the end was a Neural Network Multi-Layer Perceptron with two hidden layers, ten neurons per layer and a learning rate of 1e-2. Its AUC score was 0.6211 using three folds and five training epochs.&lt;/p&gt;
&lt;p&gt;There are further methods that could also be considered beyond the analysis presented here to improve the performance of these models. Primarily, the number of features could be reduced and this may improve the tuning of neural networks as they may suffer less from overfitting, hence more layers could be added to create a model that acts on the reduced set of features which contain the most information. Principal component analysis could also be used to condense some of the numerical and categorical variables into new features containing most of the information (variation) from the original features, which would again slow the onset of overfitting and lead to potentially more powerful neural networks.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Geron, A. (2019). &amp;lsquo;Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; Tensorflow&amp;rsquo;, O&amp;rsquo;Reilly Media.&lt;/p&gt;
&lt;p&gt;Severance, C. (2016). &amp;lsquo;Python for Everybody: Exploring Data in Python 3&amp;rsquo;, CreateSpace Independent Publishing Platform.&lt;/p&gt;
&lt;p&gt;Python Software Foundation (2016). Python Language Reference, version 2.3. URL: &lt;a href=&#34;http://www.python.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.python.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA. URL: &lt;a href=&#34;http://www.rstudio.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.rstudio.com/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kaggle.com (2017). Porto Seguro&amp;rsquo;s Safe Driver Prediction Data. URL: &lt;a href=&#34;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaggle.com (2017). Porto Seguro&amp;rsquo;s Safe Driver Prediction- Welcome. URL: &lt;a href=&#34;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spotify PCA &amp; Cluster Analysis</title>
      <link>https://domscruton.github.io/project/spotify/spotify/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://domscruton.github.io/project/spotify/spotify/</guid>
      <description>



&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This report utilizes Principal Component and Cluster Analysis to assess differences between the performance of 5872 songs from the 2000&#39;s on the basis of 13 numeric variables. Firstly, the data contains a large amount of independent information, with the first three principal components consisting of just 54.7% of the total variation in the data. The &#39;loudness&#39; of a song had the highest absolute loading on the first principal component axis, suggesting it was the most important variable in explaining the variance between observations. Cluster analysis was then used to group the songs, and eight clusters were selected using K-Means. In particular, the performance of a song greatly varied between clusters. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. The best-performing clusters had higher average danceability scores, while the worst performing clusters had very high levels of instrumentalness. This suggests that songs that are more suitable for dancing and contain higher levels of vocalness (less instrumental) are more likely to be &#39;hits&#39;, on average. From the conclusions of the analysis, several hypotheses were generated regarding differences in the performance and composition of songs contained within different clusters of the data and further methods for analysis, such as a Principal Component Logistic Regression were discussed.&lt;/p&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;strong&gt;1 Introduction&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;This report considers Principal Component and Cluster analyses to identify which variables provide the largest amount of independent information and to understand the composition of songs that are more successful. The data consists of 5872 songs from the 2000&#39;s and 16 variables, 12 of which are continuous, three are categorical and one is binary. Seven of the continuous variables are &#39;engineered&#39; metrics that relate to the constitution of each song. For example, &#39;energy&#39; &amp;quot;represents a perceptual measure of intensity and activity&amp;quot; and &#39;danceability&#39; &amp;quot;describes how suitable a track is for dancing&amp;quot; (Ansari, 2020). The binary variable, called &#39;target&#39;, indicates whether the song was a &#39;hit&#39; or a &#39;flop&#39;. A &#39;hit&#39; is defined as a song that has featured in the weekly list (issued by Billboards) of Hot-100 tracks in that decade at least once. Exactly half of the songs are labeled as &#39;hits&#39; with the other half labeled as &#39;flops&#39;. This analysis would enable artists to test whether their song is more likely to be a ‘hit’ or a ‘flop&#39; based on the characteristics of the song and alter its chances of success depending on which cluster or partition of the data the song is located.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;strong&gt;2 Exploratory Data Analysis&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Exploring the Data before carrying out a more rigorous analysis is important as it will enable us to assess the distributional properties of the variables and identify some of the individual patterns and relationships (Everitt, 2005). Furthermore, it can be used to consider the most appropriate methods of analysis for complex datasets that contain a range of variables whose inter-relationships are currently based on little theoretical background. We restrict the analysis of the data to the twelve continuous features that will be used for Principal Component and Cluster analyses.&lt;/p&gt;
&lt;div id=&#34;assessing-the-distribution-of-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 Assessing the Distribution of Variables&lt;/h2&gt;
&lt;p&gt;Histograms are used to assess the general distribution of the variables and identify any outlying observations (&lt;strong&gt;Appendix A&lt;/strong&gt;). In particular, we identify that some of the variables are expressed on widely differing scales: song duration is measured in milliseconds and ranges from 15920 to 4170227, whilst the engineered variables, such as &#39;danceability&#39; range from just 0 to 1. In order for Principal Component and Cluster Analyses to work, the data therefore need to be scaled. This standardization may be adversely affected by strongly outlying observations. The histograms in &lt;strong&gt;Appendix A&lt;/strong&gt; identify that the variables &#39;duration_ms&#39; and &#39;sections&#39;, corresponding to the duration and number of sections of each song have a few strongly outlying observations. The &#39;Plot of Outliers&#39; (&lt;strong&gt;Appendix A&lt;/strong&gt;) identifies three clearly outlying observations and it is hard to fathom that they have come from the same underlying distribution as the rest of the data and may have been incorrectly imputed. Therefore, these observations are dropped from the analysis.&lt;/p&gt;
&lt;p&gt;Some of the univariate distributions for the variables are quite skewed and this may affect the quality of the Principal Component Analysis. Scaling the data, by subtracting the mean and dividing by the standard deviation will partly alleviate this issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-relationships-between-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 Exploring the Relationships Between Variables&lt;/h2&gt;
&lt;p&gt;Principal Component Analysis (PCA) assumes that the covariance or correlation matrix adequately describes the relationship amongst the variables, that is the relationships are linear (Kutner et al., 2013). The matrix of scatterplots in &lt;strong&gt;Appendix A&lt;/strong&gt; indicates that most variables appear to have roughly linear relationships, although there is often a large amount of scatter and &#39;noise&#39;. Therefore, using the correlation matrix to describe the strength of relationships between the variables is justified and hence the conclusions of the PCA will be valid.&lt;/p&gt;
&lt;p&gt;However, whilst PCA nor Cluster analysis require multivariate normal data, they tend to work better on data that is multivariate normal (and spherical) (Everitt, 2005). In this case there appears to be some deviation from normality, especially at the tails of the distribution (&lt;strong&gt;Appendix A&lt;/strong&gt;). Whilst this should not affect qualitative conclusions, it may impact on the exact specification of the principal components.&lt;/p&gt;
&lt;p&gt;The complexity of the Spotify data can be illustrated by plotting a scatterplot of &#39;danceability&#39; against &#39;acousticness&#39;, but stratified by &#39;energy&#39;. For low energy, the data tend to have high levels of acousticness and lower danceability scores, whilst for high energy, the levels of acousticness fall but danceability rises. Therefore, the relationship between &#39;danceability&#39; and &#39;acousticness&#39; depends on the energy levels of a song. Untangling the relationships and information within the data therefore justifies the use of more rigorous multivariate analytic techniques, such as PCA and Cluster Analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-among-the-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.3 Correlation Among the Variables&lt;/h2&gt;
&lt;p&gt;For PCA to be worthwhile, there needs to be some correlation between variables in the dataset (Everitt, 2005). The correlation plot below demonstrates that some variables are highly correlated, such as &#39;section&#39; and &#39;duration&#39;, whilst others show no association. &lt;strong&gt;Table 1&lt;/strong&gt; shows the variables that have the highest correlation with song performance (target). This indicates that songs that are more danceable and louder tend to perform better, however, they need not be the variables that explain the most variation in the data as assessed by Principal Component Analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 1- Correlation between the Target (&#39;Hit&#39; or &#39;Flop&#39;) for each explanatory Variable&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Correlation with Target&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Danceability&lt;/td&gt;
&lt;td&gt;0.4585&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Instrumentalness&lt;/td&gt;
&lt;td&gt;-0.4713&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Loudness&lt;/td&gt;
&lt;td&gt;0.3473&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;methodology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3 Methodology&lt;/h1&gt;
&lt;div id=&#34;principal-component-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1 Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;The Spotify dataset we have chosen has great potential for dimensionality reduction via Principal Component Analysis (PCA) as there is clear collinearity and correlation between some variables (see the correlation plot), suggesting that they share similar information (Zelterman, 2015). PCA was developed for continuous data, therefore the categorical variables (&#39;key&#39;, &#39;mode&#39;, &#39;target&#39; and &#39;time_signature&#39;) are dropped.&lt;/p&gt;
&lt;div id=&#34;the-theory-of-principal-component-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.1.1 The Theory of Principal Component Analysis&lt;/h3&gt;
&lt;p&gt;Consider a multivariate random vector, &lt;span class=&#34;math inline&#34;&gt;\(\vec{x} = (x_1, ..., x_p)\)&lt;/span&gt; with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;. Consider p different linear combinations of the the random vector &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i = \vec{\alpha_i}^T \vec{x}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,p\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of variables in the dataset and also corresponds to the total number of principal component axes. We would like to place each random vector of the data onto a new axis, where each axis explains as much variation as possible. &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; are the projections of the random variable, &lt;span class=&#34;math inline&#34;&gt;\(\vec{x}\)&lt;/span&gt; onto each of these axes. The &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&#39;s are the loadings and they explain how much each variable, &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, contributes to each of the principal component axes (&lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the value of the random vector for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th variable). For each &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,p\)&lt;/span&gt;, the variances of &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; can be expressed as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Var(y_i) = \vec{\alpha}_i^T \Sigma \vec{\alpha}_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In particular, we want to find the loadings (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&#39;s) that maximize the variance along each axis (Everitt, 2005). However, one could make these variances arbitrarily large by multiply the loadings by an arbitrarily large scalar, so we restrict the loadings to have length 1. The second restriction is that the new axes, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; should be mutually uncorrelated:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Cov(y_i, y_j) = \vec{\alpha}_i^T \Sigma \vec{\alpha}_j = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The solution to this constrained maximization problem can be solved via the use of a Lagrange Multiplier. The principal components are then given by the eigenvectors of the correlation (or covariance) matrix and the eigenvalues are the variances of each principal component. The principal components are mutually orthogonal (uncorrelated) and decrease in variance.&lt;/p&gt;
&lt;p&gt;However, the scaling of the variables may have a significant impact on the results of a Principal Component Analysis. If the variables are measured on different scales, we should use the correlation matrix in the PCA, which is equivalent to scaling the data by dividing each data point by the standard deviation of that variable. In this case, the &#39;engineered&#39; variables, such as &#39;liveness&#39; and &#39;speechiness&#39; lie on a normalized scale between 0 and 1, meanwhile tempo is measured in Beats Per Minute (BPM) and loudness is measured in decibels. Furthermore, if we carry out the analysis without scaling the variables, nearly 100% of the total variation is explained by just one principal component, which consists almost entirely of the &#39;duration&#39; variable (&lt;strong&gt;Appendix B&lt;/strong&gt;). This is because song duration is recorded in milliseconds, with a range of 5,920 to 4,170,227 milliseconds. Clearly scaling is required for any meaningful interpretation to take place.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-an-appropriate-number-of-principal-components-to-retain&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.1.2 Choosing an Appropriate Number of Principal Components to Retain&lt;/h3&gt;
&lt;p&gt;In order to reduce the dimensionality of the problem, we need to reduce the number of principal components, whilst ensuring information loss is minimized. These reduced components could then be used in a logistic regression in order to make predictions regarding the popularity of a song based on its characteristics. The two methods considered are Kaiser&#39;s criteria and the Scree plot. Under Kaiser&#39;s Criterion, we keep only those Principal components that have eigenvalues (variances) larger than the average. Since the PCA is calculated on the correlation matrix, this average will always be one. However, this cut-off point tends to retain too few components when the number of variables is small (Everitt, 2005). Another scheme for analyzing how many components to keep is the Scree Plot. This plots the variance of each principal component and the idea is to stop retaining components after the largest significant drop in the variance, thus removing the &#39;scree&#39; from the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysing-the-principal-components-and-interpreting-the-loadings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.1.2 Analysing the Principal Components and Interpreting the Loadings&lt;/h3&gt;
&lt;p&gt;The principal component loadings represent the relative importance of each of the original variables in determining the direction of the new principal component axes (Zelterman, 2015). The scores of the principal components are the orthogonal projections of each data point onto the principal component axes and represent the new space created under PCA. Mardia&#39;s Criterion can be used to select those variables that have significant influence on the positions of each principal component. Under this &amp;quot;rule of thumb&amp;quot;, a variable is said to have a high influence if the value of its loading on the principal component axis is larger than &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sqrt{p}}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of variables used in the PCA. The variables with high loadings on the most important axes are the dominant variables that contribute most to the variation in the dataset (Zelterman, 2015).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2 Cluster Analysis&lt;/h2&gt;
&lt;p&gt;Clustering the data will allow homogeneous subgroups of songs that display similar attributes to be identified. This information can then be used to generate hypotheses about differences in the observations between groups, such as whether those songs in groups that performed better, on average, exhibit certain attributes.&lt;/p&gt;
&lt;div id=&#34;performing-a-cluster-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.2.1 Performing a Cluster Analysis&lt;/h3&gt;
&lt;p&gt;The two general methods in which to perform a cluster analysis are partitioning and hierarchical methods. The K-Means algorithm first assigns the data into K clusters. The means or centroids of the clusters are then calculated. Observations are then iteratively reassigned to the nearest cluster (according to Euclidean distances) and for each iteration the centroid of each cluster will change as different observations become part of the grouping. This process continues until no more observations are reallocated. The data will need to be standardized and outlying observations removed in order to effectively carry out the K-means algorithm appropriately (Everitt, 2005). To deal with the sensitivity of the clusters under different starting location, the K-means approach can be first applied to a subset of the data to generate sensible initial centroids and then those centroids used as the starting point for the algorithm to be applied on the whole dataset.&lt;/p&gt;
&lt;p&gt;Hierarchical methods assume that groupings in the data cloud have a hierarchical structure, where smaller groups are nested in larger groups. However, such methods are difficult to justify for real datasets. For complex structures, partitioning the data using K-Means is a better approach (Zelterman, 2015). Non-hierarchical methods, such as K-means are also more appropriate if the data consist of mainly continuous or ordered variables. In this case, all of the variables used in the analysis are continuous. Therefore, K-means is used to partition the data cloud into songs with similar characteristics, as opposed to identifying natural clusters under a hierarchical method. In this case, applying Ward&#39;s method with 8 clusters provides similar qualitative results regarding cluster location (&lt;strong&gt;Appendix C&lt;/strong&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-the-number-of-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.2.2 Selecting the Number of Clusters&lt;/h3&gt;
&lt;p&gt;The &#39;elbow&#39; method can be used to select an appropriate number of clusters for the K-means algorithm (Geron, 2019). The within group sum of squares is plotted against the number of clusters. Naturally, as the number of clusters increases, the within group sum of squares will decline, as observations in the cluster become &#39;closer&#39;, on average. In a similar manner to the scree plot, the number of clusters is chosen at the &#39;elbow&#39;, where the last significant drop in the within group sum of squares occurs (Everitt, 2005).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4 Results&lt;/h1&gt;
&lt;div id=&#34;principal-component-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.1 Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;The results of the Principal Component analysis demonstrate that variables in the dataset contain a fairly large amount of independent information. In particular, the first three principal components contain only 54.7% of the total variation, whilst to retain 90% of the total variation in the data, we would need to keep the first eight principal components.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 2- Standard Deviation and Cumulative Variance of the Principal Components&lt;/strong&gt;&lt;/p&gt;
&lt;table style=&#34;width:100%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;col width=&#34;6%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Principal Component:&lt;/th&gt;
&lt;th&gt;PC1&lt;/th&gt;
&lt;th&gt;PC2&lt;/th&gt;
&lt;th&gt;PC3&lt;/th&gt;
&lt;th&gt;PC4&lt;/th&gt;
&lt;th&gt;PC5&lt;/th&gt;
&lt;th&gt;PC6&lt;/th&gt;
&lt;th&gt;PC7&lt;/th&gt;
&lt;th&gt;PC8&lt;/th&gt;
&lt;th&gt;PC9&lt;/th&gt;
&lt;th&gt;PC10&lt;/th&gt;
&lt;th&gt;PC11&lt;/th&gt;
&lt;th&gt;PC12&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Standard Deviation&lt;/td&gt;
&lt;td&gt;1.801&lt;/td&gt;
&lt;td&gt;1.314&lt;/td&gt;
&lt;td&gt;1.263&lt;/td&gt;
&lt;td&gt;1.055&lt;/td&gt;
&lt;td&gt;1.015&lt;/td&gt;
&lt;td&gt;0.924&lt;/td&gt;
&lt;td&gt;0.885&lt;/td&gt;
&lt;td&gt;0.841&lt;/td&gt;
&lt;td&gt;0.646&lt;/td&gt;
&lt;td&gt;0.530&lt;/td&gt;
&lt;td&gt;0.380&lt;/td&gt;
&lt;td&gt;0.323&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Cumulative Proportion&lt;/td&gt;
&lt;td&gt;0.270&lt;/td&gt;
&lt;td&gt;0.414&lt;/td&gt;
&lt;td&gt;0.547&lt;/td&gt;
&lt;td&gt;0.640&lt;/td&gt;
&lt;td&gt;0.726&lt;/td&gt;
&lt;td&gt;0.797&lt;/td&gt;
&lt;td&gt;0.862&lt;/td&gt;
&lt;td&gt;0.921&lt;/td&gt;
&lt;td&gt;0.956&lt;/td&gt;
&lt;td&gt;0.979&lt;/td&gt;
&lt;td&gt;0.991&lt;/td&gt;
&lt;td&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The Scree plot suggests the use of just two principal components, however, there is a large amount of scree (remaining variation) left over. Under Kaiser&#39;s Criterion, the first five components have standard deviations greater than one and hence we would choose to retain these. Yet, the appropriate number of components to keep is context-specific. If the principal components were to be used as variables in a PC logistic regression, we may want to retain more components, to reduce information loss. However, the purpose of this report is to visualize and explain the most important components and variables, hence only the first two or three principal components are required for further analysis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Large Loadings for a Principal Component (PC) axis indicate that a variable is important in explaining variation along that axis. The main loadings on the first two PC axes as assessed by Mardia&#39;s Criterion are shown in &lt;strong&gt;Table 3&lt;/strong&gt;. The loudness of a song influences the direction of the first axis the most. Songs that are louder will on average have lower scores on this first axis. Similarly, energy and acousticness also have high influence. Therefore the first PC identifies a trend in the characteristics of songs; songs that have high levels of acousticness tend to have low energy and are less loud. Furthermore, given that the first PC contains the highest information of all of the components, this suggests that &#39;loudness&#39;, &#39;acousticness&#39; and &#39;energy&#39; provide more information regarding variation between songs than the other variables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 3- Loadings of each Variable for the Two Main Principal Component Axes&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Loading on First PC Axis&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Loudness&lt;/td&gt;
&lt;td&gt;-0.4761&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Energy&lt;/td&gt;
&lt;td&gt;-0.4423&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Acousticness&lt;/td&gt;
&lt;td&gt;0.4194&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Loading on Second PC Axis&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sections&lt;/td&gt;
&lt;td&gt;0.6664&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Duration / ms&lt;/td&gt;
&lt;td&gt;0.6541&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It appears that the first principal component is partly splitting the data between songs that were &#39;hits&#39; and those that were &#39;flops&#39;. As discussed, high values of this principle component are associated with high acousticness and low energy/loudness. Therefore it appears that songs with high acousticness perform worse, in general. These poorly performing songs also generally have larger number of sections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.2 Cluster Analysis&lt;/h2&gt;
&lt;p&gt;The K-means clustering algorithm is applied to the standardized data, in order to partition the data cloud by songs with similar characteristics. The plot of the within sum of squares for different numbers of clusters suggests that eight clusters should suffice in the analysis (at the &#39;elbow&#39; of the curve).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As this dataset is relatively large, a subsample of the songs is initially clustered and then the estimated cluster centroids are used as seeds for analysis on the full dataset. Further, the clusters can be depicted on the principal component axes. These plots show that the first and second PC axes partly split the clusters. For example, clusters 6 and 7 are well separated from most other clusters, with large values on the first principal component. Since the first principal component has a large positive loading for acousticness and negative loadings for energy and loudness, this suggests that songs in these clusters are similarly characterized by higher levels of acousticness and lower levels of energy and loudness. This is consistent with the results in &lt;strong&gt;Table 4&lt;/strong&gt;, where clusters 6 and 7 have large (scaled) acousticness values of 2.26 and 1.76, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 4- Centroids of Each Cluster&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Cluster&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Danceability&lt;/td&gt;
&lt;td&gt;-0.564&lt;/td&gt;
&lt;td&gt;-0.415&lt;/td&gt;
&lt;td&gt;0.805&lt;/td&gt;
&lt;td&gt;0.748&lt;/td&gt;
&lt;td&gt;-0.511&lt;/td&gt;
&lt;td&gt;-1.229&lt;/td&gt;
&lt;td&gt;-0.874&lt;/td&gt;
&lt;td&gt;0.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Energy&lt;/td&gt;
&lt;td&gt;0.608&lt;/td&gt;
&lt;td&gt;0.629&lt;/td&gt;
&lt;td&gt;0.077&lt;/td&gt;
&lt;td&gt;0.283&lt;/td&gt;
&lt;td&gt;0.412&lt;/td&gt;
&lt;td&gt;-2.230&lt;/td&gt;
&lt;td&gt;-1.312&lt;/td&gt;
&lt;td&gt;-0.972&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Loudness&lt;/td&gt;
&lt;td&gt;0.456&lt;/td&gt;
&lt;td&gt;0.378&lt;/td&gt;
&lt;td&gt;0.229&lt;/td&gt;
&lt;td&gt;0.361&lt;/td&gt;
&lt;td&gt;-0.024&lt;/td&gt;
&lt;td&gt;-2.737&lt;/td&gt;
&lt;td&gt;-1.505&lt;/td&gt;
&lt;td&gt;-0.283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Speechiness&lt;/td&gt;
&lt;td&gt;-0.162&lt;/td&gt;
&lt;td&gt;0.124&lt;/td&gt;
&lt;td&gt;2.308&lt;/td&gt;
&lt;td&gt;-0.298&lt;/td&gt;
&lt;td&gt;-0.193&lt;/td&gt;
&lt;td&gt;-0.442&lt;/td&gt;
&lt;td&gt;0.344&lt;/td&gt;
&lt;td&gt;-0.512&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Acousticness&lt;/td&gt;
&lt;td&gt;-0.547&lt;/td&gt;
&lt;td&gt;-0.375&lt;/td&gt;
&lt;td&gt;-0.062&lt;/td&gt;
&lt;td&gt;-0.337&lt;/td&gt;
&lt;td&gt;-0.449&lt;/td&gt;
&lt;td&gt;2.263&lt;/td&gt;
&lt;td&gt;1.762&lt;/td&gt;
&lt;td&gt;0.817&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Instrumentalness&lt;/td&gt;
&lt;td&gt;-0.412&lt;/td&gt;
&lt;td&gt;-0.276&lt;/td&gt;
&lt;td&gt;-0.470&lt;/td&gt;
&lt;td&gt;-0.431&lt;/td&gt;
&lt;td&gt;2.008&lt;/td&gt;
&lt;td&gt;1.644&lt;/td&gt;
&lt;td&gt;0.898&lt;/td&gt;
&lt;td&gt;-0.388&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Liveness&lt;/td&gt;
&lt;td&gt;-0.163&lt;/td&gt;
&lt;td&gt;2.437&lt;/td&gt;
&lt;td&gt;0.041&lt;/td&gt;
&lt;td&gt;-0.236&lt;/td&gt;
&lt;td&gt;-0.109&lt;/td&gt;
&lt;td&gt;-0.348&lt;/td&gt;
&lt;td&gt;-0.065&lt;/td&gt;
&lt;td&gt;-0.336&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Valence&lt;/td&gt;
&lt;td&gt;0.273&lt;/td&gt;
&lt;td&gt;-0.224&lt;/td&gt;
&lt;td&gt;0.642&lt;/td&gt;
&lt;td&gt;0.794&lt;/td&gt;
&lt;td&gt;-0.559&lt;/td&gt;
&lt;td&gt;-1.206&lt;/td&gt;
&lt;td&gt;-0.619&lt;/td&gt;
&lt;td&gt;-0.468&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Tempo&lt;/td&gt;
&lt;td&gt;0.903&lt;/td&gt;
&lt;td&gt;-0.066&lt;/td&gt;
&lt;td&gt;-0.296&lt;/td&gt;
&lt;td&gt;-0.363&lt;/td&gt;
&lt;td&gt;0.146&lt;/td&gt;
&lt;td&gt;-0.568&lt;/td&gt;
&lt;td&gt;-0.273&lt;/td&gt;
&lt;td&gt;-0.198&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;duration_ms&lt;/td&gt;
&lt;td&gt;-0.145&lt;/td&gt;
&lt;td&gt;-0.107&lt;/td&gt;
&lt;td&gt;-0.105&lt;/td&gt;
&lt;td&gt;-0.199&lt;/td&gt;
&lt;td&gt;0.302&lt;/td&gt;
&lt;td&gt;-0.028&lt;/td&gt;
&lt;td&gt;6.044&lt;/td&gt;
&lt;td&gt;0.031&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;chorus_hit&lt;/td&gt;
&lt;td&gt;-0.155&lt;/td&gt;
&lt;td&gt;0.410&lt;/td&gt;
&lt;td&gt;-0.082&lt;/td&gt;
&lt;td&gt;-0.084&lt;/td&gt;
&lt;td&gt;0.261&lt;/td&gt;
&lt;td&gt;0.196&lt;/td&gt;
&lt;td&gt;0.224&lt;/td&gt;
&lt;td&gt;-0.061&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;sections&lt;/td&gt;
&lt;td&gt;-0.101&lt;/td&gt;
&lt;td&gt;-0.232&lt;/td&gt;
&lt;td&gt;-0.034&lt;/td&gt;
&lt;td&gt;-0.172&lt;/td&gt;
&lt;td&gt;0.121&lt;/td&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;td&gt;5.755&lt;/td&gt;
&lt;td&gt;0.062&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In order to assess which variables play the largest role in separating out the clusters, a separate one-way ANOVA can be performed to test for a difference in the mean of each variable for each cluster. By ranking the F-values, we can identify which variables are most effective at separating the groups. &lt;strong&gt;Table 5&lt;/strong&gt; shows the three variables with the largest F-values. This suggests energy, acousticness and then loudness are the variables that are most responsible for the differences between the clusters. This is consistent with the conclusion of the Principal Component Analysis, where these three variables had the highest loadings on the first principal component.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 5- F-Values for Variables across the Clusters&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;F-Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Energy&lt;/td&gt;
&lt;td&gt;3345.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Acousticness&lt;/td&gt;
&lt;td&gt;2371.59&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Loudness&lt;/td&gt;
&lt;td&gt;1346.10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The number of songs in each cluster reveals that the seventh cluster contains relatively few songs. In particular, this cluster is characterized by long song duration and a large number of sections in each song (&lt;strong&gt;Table 4&lt;/strong&gt;) and indicates songs with quite different characteristics to those in the rest of the dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some of the clusters of songs have far higher proportions of &#39;hits&#39; than other clusters. Songs in clusters 3 and 4 display a 77% and 73% chance of being hits, respectively (&lt;strong&gt;Table 6&lt;/strong&gt;). These clusters have higher average danceability scores than the other clusters (&lt;strong&gt;Table 4&lt;/strong&gt;). Hence, songs that are better for dancing are more likely to be popular. This is consistent with the results from &lt;strong&gt;Table 1&lt;/strong&gt;, where it was shown that danceability was positively correlated with song popularity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 6- Percentage of Songs in each Cluster that are Hits&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Cluster&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Percentage of Hits (%)&lt;/td&gt;
&lt;td&gt;47.74&lt;/td&gt;
&lt;td&gt;37.76&lt;/td&gt;
&lt;td&gt;77.02&lt;/td&gt;
&lt;td&gt;73.88&lt;/td&gt;
&lt;td&gt;2.48&lt;/td&gt;
&lt;td&gt;0.73&lt;/td&gt;
&lt;td&gt;1.41&lt;/td&gt;
&lt;td&gt;56.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5 Discussion&lt;/h1&gt;
&lt;p&gt;Prior research has concluded that successful songs tend to be &amp;quot;‘happier’, more ‘party-like’, less ‘relaxed’ and more ‘female’ than most&amp;quot; (Interiano et al., 2018, p.1). The evidence presented here is fairly consistent with these results, as songs with higher danceability (more &#39;party-like&#39;) and more energy (less &#39;relaxed&#39;) tend to perform better. On a more general level, this information could be used to inform music artists about how to increase the likelihood that a song will be a &#39;hit&#39; by altering the characteristics of that song to be similar to songs in clusters with a larger proportion of &#39;hits&#39;.&lt;/p&gt;
&lt;p&gt;One can also see that between clusters there is a great deal of heterogeneity. The third cluster is the only cluster to have a large value for &#39;speechiness&#39;, in particular containing songs with a large amount of spoken words, possibly of the rap genre of music (&lt;strong&gt;Table 4&lt;/strong&gt;). Further, the results of the Exploratory, Principal Component and Cluster Analyses are consistent. Energy, instrumentalness and loudness are the three variables that play the largest role in separating out the clusters (&lt;strong&gt;Table 3&lt;/strong&gt;). These three variables also have the highest loadings on the first principal component, suggesting that they explain more of the variation in the data than the other variables. Furthermore, clusters of songs with high danceability and low instrumentalness were associated with greater popularity. This is consistent with the results in &lt;strong&gt;Table 1&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Cluster analysis is a useful tool for generating hypotheses. For example, consider clusters 2 and 5. The centroids of these clusters are fairly similar, with roughly equal levels of danceability. The distinguishing difference between songs in each cluster are the levels of instrumentalness and liveness (&lt;strong&gt;Table 4&lt;/strong&gt;). One could then formulate and test the hypothesis that the difference in the performance of songs between these two clusters is driven by differences in instrumentalness. If this is true, this suggests that an artist could increase the popularity of their song if it has similar characteristics to songs in cluster 5 by reducing the level of instrumentalness (i.e. by adding more vocal elements to their song).&lt;/p&gt;
&lt;p&gt;We may also consider different hypotheses about the performance of songs contained within a given cluster. For example, are there differences in the characteristics of songs in cluster 1 that are &#39;hits&#39; and &#39;flops&#39; and which variables are the most important in explaining the within cluster popularity? This would enable an artist within a given genre (songs that have certain characteristics) to alter their music to increase the likelihood that the songs would be successful. The characteristics that increase song success may also differ between clusters.&lt;/p&gt;
&lt;p&gt;In order to make predictions regarding the popularity of a new song based on its attributes, a Principal Component Logistic Regression model which utilizes the principal components as regressors could be fitted. This is useful in overcoming issues of collinearity between variables, by excluding some of the low variance principal components from the model. Hence, a new song based on the discussed variables that has maximal probability of being a hit could be created.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6 Conclusion&lt;/h1&gt;
&lt;p&gt;This report has uncovered several insights through the use of Principal Component and Cluster Analyses, some of which were not obvious when first exploring the data. In the top-performing cluster, over 77% of songs were hits, in contrast to the worst performing cluster, where just 0.73% of songs were hits. Clusters of songs with higher rates of success were associated with higher levels of danceability and lower instrumentalness (i.e. more vocal elements). Energy, acousticness and loudness were the variables that had the greatest influence on separating out the clusters and these variables also had the highest absolute loadings on the first principal component axes. From these conclusions, several hypotheses concerning differences in the characteristics and performance of songs across different clusters and within the same cluster were generated. These hypotheses included &amp;quot;Is the difference in the performance between songs in clusters 2 and 5 driven by differences in instrumentalness?&amp;quot; or &amp;quot;Do songs in cluster 1 that are &#39;hits&#39; exhibit different characteristics to those songs that are &#39;flops&#39;?&amp;quot; and suggest areas for future research.&lt;/p&gt;

&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Brian S. Everitt. 2005. &amp;quot;An R and S-Plus Companion to Multivariate Analysis&amp;quot;. Springer.&lt;/p&gt;
&lt;p&gt;Daniel Zelterman. 2015. &amp;quot;Applied Multivariate Statistics with R&amp;quot;. Springer.&lt;/p&gt;
&lt;p&gt;Aurelien Geron. 2019. &amp;quot;Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow&amp;quot;. O&#39;Reilly Media. pp 213-274.&lt;/p&gt;
&lt;p&gt;Michael H. Kutner, Christopher J. Nachtsheim, John Neter, William Li. 2013. &amp;quot;Applied Linear Statistical Models.&amp;quot; McGraw Hill Education.&lt;/p&gt;
&lt;p&gt;Garrett Grolemund, Hadley Wickham. 2017. &amp;quot;R for Data Science.&amp;quot; O&#39;Reilly Media. doi: &lt;a href=&#34;https://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;https://r4ds.had.co.nz/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Myra Interiano, Kamyar Kazemi, Lijia Wang, Jienian Yang, Zhaoxia Yu and Natalia L. Komarova. 2018. &amp;quot;Musical Trends and Predictability of Success in Contemporary Songs in and out of the Top Charts.&amp;quot; Royal Society Open Science. doi: &lt;a href=&#34;https://royalsocietypublishing.org/doi/10.1098/rsos.171274&#34; class=&#34;uri&#34;&gt;https://royalsocietypublishing.org/doi/10.1098/rsos.171274&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Eric A. Strobl, Clive Tucker. 2000. &amp;quot;The Dynamics of Chart Success in the U.K. Pre-Recorded Popular Music Industry.&amp;quot; Journal of Cultural Economics 24. doi: &lt;a href=&#34;https://doi.org/10.1023/A:1007601402245&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1023/A:1007601402245&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Farooq Ansari. 2020. Spotify Hit Predictor Dataset. kaggle.com doi: &lt;a href=&#34;https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?fbclid=IwAR1kE9neO0sdKb3pv6g-Z-SOvPXii9Ubqx0PTRIDkZYdqaBGEhtLGTrFkLA&#34; class=&#34;uri&#34;&gt;https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?fbclid=IwAR1kE9neO0sdKb3pv6g-Z-SOvPXii9Ubqx0PTRIDkZYdqaBGEhtLGTrFkLA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;RStudio Team (2016). RStudio: Integrated Development for R. RStudio, Inc., Boston, MA URL &lt;a href=&#34;http://www.rstudio.com/&#34; class=&#34;uri&#34;&gt;http://www.rstudio.com/&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;strong&gt;Appendix&lt;/strong&gt;&lt;/h1&gt;
&lt;div id=&#34;a-exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A) Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Histograms&lt;/strong&gt; Histograms for each continuous variable in the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#histograms for each variable to assess the distribution and look for outliers.
par(mfrow = c(1, 1))
for (i in numeric_variables) {
  hist(spotify_data[,i], main = c(&amp;quot;Histogram of &amp;quot;, i), xlab = i, breaks = 20)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-6.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-7.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-8.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-9.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-10.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-11.png&#34; width=&#34;336&#34; /&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-17-12.png&#34; width=&#34;336&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(&amp;#39;Range of Song Duration / milliseconds:&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Range of Song Duration / milliseconds:&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(c(range(spotify_data$duration_ms)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   15920 4170227&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Plot of Outliers&lt;/strong&gt; Scatterplot of Song Duration against the number of Sections in a song, to identify outlying observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Analysis of outliers in the dataset
#List of variables with clear outliers
outliers &amp;lt;- c(&amp;quot;duration_ms&amp;quot;, &amp;quot;sections&amp;quot;)
plot(spotify_data[, outliers], main = &amp;quot;Plot of Outliers&amp;quot;, xlab = &amp;quot;Song Duration&amp;quot;, ylab = &amp;quot;Sections&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matrix of Scatterplots&lt;/strong&gt; Pairwise scatterplots for each continuous variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs(spotify_no_outliers[numeric_variables][1:200,])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Testing for Multivariate Normality&lt;/strong&gt; A simple test of multivariate normality is conducted by plotting the (ordered) squared Mahalanobis Distance against the corresponding quantiles of a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;-distribution with degrees of freedom equal to the number of variables in the dataset. If the transformed data lie along a straight line then the data are multivariate normally distributed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-principal-component-analysis-without-scaling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;B) Principal Component Analysis Without Scaling&lt;/h2&gt;
&lt;p&gt;This appendix contains the results of a Principal Component Analysis on the unscaled numerical data. In particular, the first principal contains almost all of the variation in the data and this is provided by the &#39;duration&#39; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#PCA Without Scaling
pca.spotify.unscaled &amp;lt;- prcomp(spotify_numeric)
summary(pca.spotify.unscaled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Importance of components:
##                           PC1   PC2   PC3   PC4   PC5    PC6    PC7    PC8
## Standard deviation     112174 30.23 20.11 4.966 2.136 0.2979 0.2365 0.2245
## Proportion of Variance      1  0.00  0.00 0.000 0.000 0.0000 0.0000 0.0000
## Cumulative Proportion       1  1.00  1.00 1.000 1.000 1.0000 1.0000 1.0000
##                           PC9   PC10   PC11    PC12
## Standard deviation     0.1688 0.1381 0.1034 0.08701
## Proportion of Variance 0.0000 0.0000 0.0000 0.00000
## Cumulative Proportion  1.0000 1.0000 1.0000 1.00000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.spotify.unscaled$rotation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                            PC1           PC2           PC3           PC4
## danceability     -1.302208e-07 -8.650312e-04 -8.958769e-04 -1.181289e-02
## energy           -3.102277e-07  1.729092e-03  2.404089e-04 -3.530510e-02
## loudness         -7.337131e-06  2.773744e-02 -8.162803e-03 -9.964386e-01
## speechiness       1.318074e-08 -5.034456e-05 -7.637436e-05 -2.022660e-03
## acousticness      3.824303e-07 -1.529234e-03 -2.182361e-04  3.912435e-02
## instrumentalness  4.572867e-07 -3.172999e-04  1.134257e-03  2.521589e-02
## liveness         -3.717663e-08  2.124559e-04  2.317251e-04 -3.759318e-03
## valence          -3.518179e-07  1.845762e-04 -8.102301e-04 -1.605659e-02
## tempo            -8.319169e-06  9.965844e-01  7.776454e-02  2.675845e-02
## duration_ms       1.000000e+00  9.430128e-06 -1.429315e-05 -8.819774e-06
## chorus_hit        1.652100e-05 -7.717809e-02  9.959979e-01 -8.118795e-03
## sections          3.641884e-05  9.347518e-03 -4.325897e-02  4.999516e-02
##                            PC5           PC6           PC7           PC8
## danceability     -4.964583e-03  3.733586e-01  3.111789e-01 -1.021434e-01
## energy            3.988151e-03 -1.320574e-01  2.489982e-01  3.197516e-01
## loudness         -4.925853e-02 -2.149171e-02 -2.318400e-02 -5.028992e-02
## speechiness      -1.256685e-03  4.660923e-02  3.200967e-02  2.167349e-02
## acousticness     -4.364288e-03  1.514271e-01 -3.634591e-01 -8.138958e-01
## instrumentalness  1.295951e-02 -7.139996e-01  5.418293e-01 -4.239904e-01
## liveness          1.471401e-03 -5.298768e-02 -4.544922e-02  1.110710e-01
## valence          -1.037914e-02  5.519963e-01  6.417573e-01 -1.725608e-01
## tempo             7.282709e-03  1.279704e-03 -3.406733e-05 -6.739756e-04
## duration_ms       3.675594e-05  9.349871e-07 -2.608640e-08  7.858695e-09
## chorus_hit       -4.433113e-02  6.574808e-04 -2.330242e-05 -2.036780e-04
## sections         -9.976053e-01 -1.716226e-02  2.436889e-03  4.258439e-03
##                            PC9          PC10          PC11          PC12
## danceability     -3.101105e-01 -6.427783e-01  4.055585e-01 -2.817126e-01
## energy            3.170263e-01  4.086354e-01  6.385141e-01 -3.766503e-01
## loudness         -6.092071e-03 -6.119759e-03 -1.309125e-02  8.556876e-03
## speechiness       1.120431e-01 -8.222999e-02  4.896812e-01  8.586177e-01
## acousticness      2.436457e-01  1.597009e-01  2.732911e-01 -1.464008e-01
## instrumentalness -3.280515e-02 -1.097029e-01 -3.381879e-02  4.240195e-02
## liveness          8.186357e-01 -5.330207e-01 -1.545167e-01 -6.798993e-02
## valence           2.452160e-01  3.023282e-01 -2.969051e-01  1.166974e-01
## tempo            -5.212203e-04 -8.387501e-04  1.209447e-04  6.572693e-06
## duration_ms       4.495584e-08  1.004793e-07 -1.165984e-07  3.722348e-08
## chorus_hit       -2.501794e-04 -1.733233e-04  1.018916e-04 -7.168368e-05
## sections          1.417663e-04 -8.156361e-04  1.786454e-03 -1.727792e-03&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;c-clustering-with-hierarchical-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;C) Clustering with Hierarchical Methods&lt;/h2&gt;
&lt;p&gt;Hierarchical clustering using Ward&#39;s method performs similarly to K-means on this dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Euclidean Distance matrix
DEuclidean &amp;lt;- dist(spotify_scaled)
#cluster using Ward&amp;#39;s method
HClusters &amp;lt;- hclust(DEuclidean, &amp;quot;ward.D2&amp;quot;)
#Dendrogram
plot(HClusters, xlab = &amp;quot;Observation Number&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Plotting the clusters on the first two principal component axes shows that cluster locations are generally similar to that of the K-means method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Cut observations into 8 clusters
MyClusters &amp;lt;- cutree(HClusters, 8)
#copy dataset
spotify_trial &amp;lt;- spotify_no_outliers
#extract clusters
spotify_trial$cluster &amp;lt;- MyClusters
spotify_trial$cluster &amp;lt;- as.factor(spotify_trial$cluster)
#plot clusters on PC axes
ggplot(spotify_trial, aes(x = scores_1, y = scores_2, colour = cluster)) + geom_point(shape=1) +
  ggtitle(&amp;quot;Hierarchical Clusters on the Principal Component Axes&amp;quot;) + labs(y=&amp;quot;PC2&amp;quot;, x = &amp;quot;PC1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://domscruton.github.io/project/spotify/spotify_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d-code-used-in-the-main-report&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;D) Code used in the Main Report&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Import Data and Relevant Libraries
#Import data
spotify_data &amp;lt;- read.csv(&amp;quot;C:/Users/User/Documents/St Andrews/Multivariate Analysis/Data/spotify dataset.csv&amp;quot;)
#Remove the track, artist and url for each song (irrelevant to the purposes of this general analysis)
spotify_data &amp;lt;- spotify_data[, 4:19]
#Load relevant libraries:
#three-dimensional data cloud plots
library(scatterplot3d)
#multivariate visualization
library(lattice)
#visualization of correlation matrix
library(corrplot)
#Dendrograms
library(ape)
#Choose appropriate number of clusters
library(NbClust)
#Analysis of the Composition of different clusters
library(tidyverse)
library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#2. Dropping of Outliers from the Data and Extraction of Variables used in the PCA
#Extract continuous variables
variables &amp;lt;- sapply(spotify_data, is.numeric)
numeric_variables &amp;lt;- vars[vars == TRUE]
#Drop outlying observations
outliers &amp;lt;- which(spotify_data$duration_ms &amp;gt; 3e6)
spotify_no_outliers &amp;lt;- spotify_data[-c(outliers), ]
#Extract numeric variables for later PCA and Cluster Analysis
spotify_numeric &amp;lt;- spotify_no_outliers[numeric_variables]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#3. CoPlot to illustrate complex relationships in the data cloud
#CoPlot for danceability and acousticness, stratified by Energy
xyplot(spotify_no_outliers$danceability ~ spotify_no_outliers$acousticness |
         cut(spotify_no_outliers$energy, 3), 
         main = &amp;quot;Coplot for Danceability and Acousticness, Cut by Energy&amp;quot;, 
         xlab = &amp;quot;Acousticness&amp;quot;, ylab = &amp;quot;Danceability&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#4. Correlation Between Variables
#correlation matrix
corr_matrix &amp;lt;- cor(spotify_no_outliers)
sort(corr_matrix[, 16], decreasing = TRUE)
#correlation plot
corrplot(corr_matrix, method=&amp;quot;ellipse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#5. Principal Component Analysis
#Principal Component Analysis with scaling
pca.spotify &amp;lt;- prcomp(spotify_numeric, scale. = TRUE)
summary(pca.spotify)
#Scree Plot
plot(pca.spotify$sdev^2, xlab = &amp;quot;Principal Component&amp;quot;, ylab = &amp;quot;variance&amp;quot;, 
     main = &amp;quot;Scree Plot for Spotify Principal Component Analysis (PCA)&amp;quot;)
lines(pca.spotify$sdev^2)
#Principal Component loadings (rotations)
pca.spotify$rotation
#Mardia&amp;#39;s Criterion to select variables with high loadings on each PC axis
for(i in 1:8){
  which.pass&amp;lt;-abs(pca.spotify$rotation[,i])&amp;gt;(0.7*max(abs(pca.spotify$rotation[,i])))
  cat(&amp;quot;\nPC&amp;quot;,i,&amp;quot;\n&amp;quot;,sep=&amp;quot;&amp;quot;)
  print(pca.spotify$rotation[which.pass,i])
}
#use sample of data to visualize scores
scores1 &amp;lt;- pca.spotify$x[1:500,1]
scores2 &amp;lt;- pca.spotify$x[1:500,2]
#plot scores
par(mfrow = c(1,2))
plot(scores1,scores2,ylim=range(scores2),xlab=&amp;quot;PC1&amp;quot;,ylab=&amp;quot;PC2&amp;quot;, type=&amp;quot;n&amp;quot;,lwd=2, 
     main = &amp;quot;Hit (1) or Flop (0)&amp;quot;)
#Add target variable in reduced spatial plot
text(scores1,scores2,labels=spotify_no_outliers$target[1:500],cex=0.7,lwd=2)
#add scroes in reduced spatial plot
plot(scores1,scores2,ylim=range(scores2),xlab=&amp;quot;PC1&amp;quot;,ylab=&amp;quot;PC2&amp;quot;, 
     cex = 0.1 * spotify_no_outliers$sections, main = &amp;quot;Sections&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#6. Cluster Analysis
#Scale data
spotify_scaled &amp;lt;- scale(spotify_numeric)
n&amp;lt;-length(spotify_scaled[,1])

#find within group sum of squares (WSS) for different numbers of clusters
#WSS for first cluster
wss1 &amp;lt;- (n - 1) * sum(apply(spotify_scaled, 2, var))
wss &amp;lt;- numeric(0)
#calculate WSS for 2 to 20 group partitions given by k-means clustering
set.seed(160001695)
for (i in 2: 20) {
  W &amp;lt;- sum(kmeans(spotify_scaled, i)$withinss)
  wss &amp;lt;-c (wss,W)
}
wss&amp;lt;-c(wss1, wss)
#Plot WSS against each cluster to select number of clusters for K-Means
plot(1:20, wss, type = &amp;quot;l&amp;quot;, xlab = &amp;quot;Number of groups&amp;quot;, ylab = &amp;quot;within groups sum of squares&amp;quot;, 
     lwd = 2, main = &amp;quot;Selecting the Number of Clusters for K-Means&amp;quot;)

set.seed(160001695)
#perform initial cluster analysis on sub-sample of data
k8_initial &amp;lt;- kmeans(spotify_scaled[1:500, ], 8)
set.seed(160001695)
#use initial clusters to start the full clustering
k8 &amp;lt;- kmeans(spotify_scaled, centers = k8_initial$centers)
scores_1 &amp;lt;- pca.spotify$x[,1]
scores_2 &amp;lt;- pca.spotify$x[,2]
#Add clusters as new variable, so we can test differences and generate hypotheses 
#between songs in different (or the same) cluster. 
spotify_no_outliers$cluster &amp;lt;- k8$cluster
scores_3 &amp;lt;- pca.spotify$x[, 3]
par(mfrow = c(1,1))
#Plot clusters on PC axes
spotify_no_outliers$cluster &amp;lt;- as.factor(spotify_no_outliers$cluster)
ggplot(spotify_no_outliers, aes(x = scores_1, y = scores_2, colour = cluster)) +
  geom_point(shape=1) + ggtitle(&amp;quot;Clusters on the Principal Component Axes&amp;quot;) + 
  labs(y=&amp;quot;PC2&amp;quot;, x = &amp;quot;PC1&amp;quot;)
#Centres of each of the five clusters, by variable
round(k8$centers, 3)

#matrix to store F Values
F.Results &amp;lt;- matrix(NA, nrow = length(variables), ncol = 2)
F.Results &amp;lt;- as.data.frame(F.Results)

F.Results$Variables &amp;lt;- variables
#aov does not enable lists to be used as variables, so we must calculate all of the F-statistics
#individually
#F-statistic for assessing difference in mean danceability across clusters- anova of each variable
#for each cluster
i &amp;lt;- 0
for(num_var in c(numeric_variables)){
  i &amp;lt;- i + 1
  F.Results[i, 2] &amp;lt;- summary(aov(spotify_no_outliers[[num_var]] ~
                                 spotify_no_outliers$cluster))[[1]][[&amp;quot;F value&amp;quot;]][1]
}

#Count number of songs in each cluster
Cluster &amp;lt;- table(spotify_no_outliers$cluster)
Counts &amp;lt;- data.frame(
  No_Clusters = factor(c(seq(1, 8)), levels=c(seq(1, 8))),
  Songs = Cluster
)
#Plot number of songs in each cluster
ggplot(data = Counts, aes(x = No_Clusters, y = Songs.Freq, fill = No_Clusters)) +
  geom_bar(colour=&amp;quot;black&amp;quot;, stat=&amp;quot;identity&amp;quot;) + 
  ggtitle(&amp;quot;Number of Songs Contained Within Each Cluster&amp;quot;) + labs(y=&amp;quot;Song Frequency&amp;quot;, 
                                                                  x = &amp;quot;Cluster Number&amp;quot;)

#Percentage of songs that are &amp;#39;hits&amp;#39; in each cluster
#Number of Clusters
N_clusters &amp;lt;- c(seq(1, 8))
#Empty vector of Percentages
Percentage_target &amp;lt;- c(rep(NA, 8))
for (i in N_clusters) {
  x &amp;lt;- filter(spotify_no_outliers, spotify_no_outliers$cluster == i)
  Percentage_target[i] &amp;lt;- (dim(filter(x, x$target == 1))[1] / dim(x)[1]) * 100
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
